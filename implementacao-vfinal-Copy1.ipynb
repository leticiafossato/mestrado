{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba26060",
   "metadata": {},
   "source": [
    "# Código Mestrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110edfc",
   "metadata": {},
   "source": [
    "https://stackabuse.com/python-for-nlp-working-with-facebook-fasttext-library/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb111096",
   "metadata": {},
   "source": [
    "https://github.com/PacktPublishing/fastText-Quick-Start-Guide/blob/master/chapter5/python%20fastText%20unsupervised%20learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a37394",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cdd3979",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:00.396065Z",
     "start_time": "2023-03-03T01:47:57.533199Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import numpy\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import itertools\n",
    "import string\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "from fasttext import load_model\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f83164c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:00.412180Z",
     "start_time": "2023-03-03T01:48:00.396577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1d871ec4a00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word2Vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174bd82a",
   "metadata": {},
   "source": [
    "## Leitura de Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2acaae9",
   "metadata": {},
   "source": [
    "### DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b6337f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:00.482924Z",
     "start_time": "2023-03-03T01:48:00.414179Z"
    }
   },
   "outputs": [],
   "source": [
    "papers = joblib.load('papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262368c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:00.498668Z",
     "start_time": "2023-03-03T01:48:00.485538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15fc8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T20:19:19.109050Z",
     "start_time": "2023-02-27T20:19:19.099194Z"
    }
   },
   "source": [
    "papers_treino = dict(itertools.islice(papers.items(),495))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab49749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:00.514423Z",
     "start_time": "2023-03-03T01:48:00.498668Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_treino = dict(list(papers.items())[0:495])\n",
    "papers_teste = dict(list(papers.items())[495:708])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348d7f7",
   "metadata": {},
   "source": [
    "### Padrão Ouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367471ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:00.545904Z",
     "start_time": "2023-03-03T01:48:00.514423Z"
    }
   },
   "outputs": [],
   "source": [
    "padrao_ouro = joblib.load('selecao_padrao_ouro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "587562e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:00.561809Z",
     "start_time": "2023-03-03T01:48:00.547815Z"
    }
   },
   "outputs": [],
   "source": [
    "padrao_ouro_treino = dict(list(padrao_ouro.items())[0:495])\n",
    "padrao_ouro_teste  = dict(list(padrao_ouro.items())[495:708])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c60f9eae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T03:05:03.248930Z",
     "start_time": "2023-03-03T03:05:03.243930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7c65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c591f50",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3df1d12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:00.577913Z",
     "start_time": "2023-03-03T01:48:00.564136Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessamento(corpus: str):\n",
    "    \"\"\" Pré-processamento inicial para remover pontuações, padronizar a caixa e pegar o conteúdo de cada paper\"\"\"\n",
    "    regIter = re.finditer('(<PAPER>(.*)<\\/PAPER>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    #lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        #txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(\"\\\\t\",\"\",txt)\n",
    "        #txt = re.sub('\\/','', txt)\n",
    "        txt = re.sub(\"\\-\",\"\",txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        #txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        txt = re.sub('&amp;quot;seed&amp;quot;', '', txt)\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;\\?@\\[\\]^_`{|}\\~\\%\\.]', '', txt)\n",
    "        txt = re.sub('=', ' ', txt)\n",
    "        txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "        txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "        #txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "        txt = re.sub('i\\.e.', '', txt)\n",
    "        txt = re.sub('acc\\.', '', txt)\n",
    "        txt = re.sub('e\\.g\\.', '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9862046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:00.593667Z",
     "start_time": "2023-03-03T01:48:00.580550Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_processamento_ouro(txt):\n",
    "    #txt = re.sub('\\\\n',' ', txt)\n",
    "    txt = txt.lower()\n",
    "    #txt = re.sub(r'<.*?>','', txt)\n",
    "    txt = re.sub(\"\\\\t\",\"\",txt)\n",
    "    #txt = re.sub('\\/','', txt)\n",
    "    txt = re.sub(\"\\-\",\"\",txt)\n",
    "    txt = txt.strip()\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "    txt = re.sub('&amp;quot;', '', txt)\n",
    "    txt = re.sub('&amp;quot;seed&amp;quot;', '', txt)\n",
    "    txt = re.sub('\\.+', '', txt)\n",
    "    txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;=\\?@\\[\\]^_`{|}\\~\\%\\.]', '', txt)\n",
    "    txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "    txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "    #txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "    txt = re.sub('i\\.e.', '', txt)\n",
    "    txt = re.sub('acc\\.', '', txt)\n",
    "    txt = re.sub('e\\.g\\.', '', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2c73fc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:03.445784Z",
     "start_time": "2023-03-03T01:48:00.597518Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_treino2 = {k: preprocessamento(v) for k, v in papers_treino.items()}\n",
    "padrao_ouro_treino2 = {k: pre_processamento_ouro(v) for k, v in padrao_ouro_treino.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f9a445e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:04.739110Z",
     "start_time": "2023-03-03T01:48:03.445784Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_teste2 = {k: preprocessamento(v) for k, v in papers_teste.items()}\n",
    "padrao_ouro_teste2 = {k: pre_processamento_ouro(v) for k, v in padrao_ouro_teste.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a15a0be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:04.760739Z",
     "start_time": "2023-03-03T01:48:04.741736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s sid >a practical partofspeech tagger</s> <abstract> <s sid  ssid >we present an implementation of a partofspeech tagger based on a hidden markov model</s> <s sid  ssid >the methodology enables robust and accurate tagging with few resource requirements</s> <s sid  ssid >only a lexicon and some unlabeled training text are required</s> <s sid  ssid >accuracy exceeds </s> <s sid  ssid >we describe implementation strategies and optimizations which result in highspeed operation</s> <s sid  ssid >three applications for tagging are described phrase recognition word sense disambiguation and grammatical function assignment</s> <s sid  ssid > desiderata many words are ambiguous in their part of speech</s> <s sid  ssid >for example tag can be a noun or a verb</s> <s sid  ssid >however when a word appears in the context of other words the ambiguity is often reduced in a tag is a partofspeech label the tag can only be a noun</s> <s sid  ssid >a tagger is a system that uses context to assign parts of speech to words</s> <s sid  ssid >automatic text tagging is an important first step in discovering the linguistic structure of large text corpora</s> <s sid  ssid >partofspeech information facilitates higherlevel analysis such as recognizing noun phrases and other patterns in text</s> <s sid  ssid >for a tagger to function as a practical component in a language processing system we believe that a tagger must be corpora contain ungrammatical constructions isolated phrases such as titles and nonlinguistic data such as tables</s> <s sid  ssid >corpora are also likely to contain words that are unknown to the tagger</s> <s sid  ssid >it is desirable that a tagger deal gracefully with these situations a tagger is to be used to analyze arbitrarily large corpora it must be efficientperforming in time linear in the number of words tagged</s> <s sid  ssid >any training required should also be fast enabling rapid turnaround with new corpora and new text genres</s> <s sid  ssid >a should attempt to assign the correct partofspeech tag to every word encountered</s> <s sid  ssid >a should be able to take advantage of linguistic insights</s> <s sid  ssid >one should be able to correct errors by supplying appropriate priori hints it should be possible to give different hints for different corpora effort required to retarget a tagger to new corpora new tagsets and new languages should be minimal</s> <s sid  ssid > methodology  background several different approaches have been used for building text taggers</s> <s sid  ssid >greene and rubin used a rulebased approach in the taggit program greene and rubin  which was an aid in tagging the brown corpus francis and kueera </s> <s sid  ssid >taggit disambiguated  of the corpus the rest was done manually over a period of several years</s> <s sid  ssid >more recently koskenniemi also used a rulebased approach implemented with finitestate machines koskenniemi </s> <s sid  ssid >statistical methods have also been used eg derose garside al these provide the capability of resolving ambiguity on the basis of most likely interpretation</s> <s sid  ssid >a form of markov model has been widely used that assumes that a word depends probabilistically on just its partofspeech category which in turn depends solely on the categories of the preceding two words</s> <s sid  ssid >two types of training ie parameter estimation have been used with this model</s> <s sid  ssid >the first makes use of a tagged training corpus</s> <s sid  ssid >derouault and merialdo use a bootstrap method for training derouault and merialdo </s> <s sid  ssid >at first a relatively small amount of text is manually tagged and used to train a partially accurate model</s> <s sid  ssid >the model is then used to tag more text and the tags are manually corrected and then used to retrain the model</s> <s sid  ssid >church uses the tagged brown corpus for training church </s> <s sid  ssid >these models involve probabilities for each word in the lexicon so large tagged corpora are required for reliable estimation</s> <s sid  ssid >the second method of training does not require a tagged training corpus</s> <s sid  ssid >in this situation the baumwelch algorithm also known as the forwardbackward algorithm can be used baum </s> <s sid  ssid >under this regime the model is a markov model as state transitions ie partofspeech categories are assumed to be unobservable</s> <s sid  ssid >jelinek has used this method for training a text tagger jelinek </s> <s sid  ssid >parameter smoothing can be conachieved using the method of interpolawhich weighted estimates are taken from secondand firstorder models and a uniform probability distribution jelinek and mercer </s> <s sid  ssid >kupiec used word equivclasses referred to here as classes on parts of speech to pool data from individual words kupiec b</s> <s sid  ssid >the most common words are still represented individually as sufficient data exist for robust estimation</s> <s sid  ssid > however all other words are represented according to the set of possible categories they can assume</s> <s sid  ssid >in this manner the vocabulary of  words in the brown corpus can be reduced to approximately  distinct ambiguity classes kupiec </s> <s sid  ssid >to further reduce the number of parameters a firstorder model can be employed this assumes that a words category depends only on the immediately preceding words category</s> <s sid  ssid >in kupiec a networks are used to selectively augment the context in a basic firstorder model rather than using uniformly secondorder dependencies</s> <s sid  ssid > our approach we next describe how our choice of techniques satisfies the listed in section </s> <s sid  ssid >the use of an complete flexibility in the choice of training corpora</s> <s sid  ssid >text from any desired domain can be used and a tagger can be tailored for use with a particular text database by training on a portion of that database</s> <s sid  ssid >lexicons containing alternative tag sets can be easily accommodated without any need for relabeling the training corpus affording further flexibility in the use of specialized tags</s> <s sid  ssid >as the resources required are simply a lexicon and a suitably large sample of ordinary text taggers can be built with minimal effort even for other languages such as french eg kupiec </s> <s sid  ssid >the use of ambiguity classes and a firstorder model reduces the number of parameters to be estimated without significant reduction in accuracy discussed in section </s> <s sid  ssid >this also enables a tagger to be reliably trained using only moderate amounts of text</s> <s sid  ssid >we have produced reasonable results training on as few as  sentences</s> <s sid  ssid >fewer parameters also reduce the time required for training</s> <s sid  ssid >relatively few ambiguity classes are sufficient for wide coverage so it is unlikely that adding new words to the lexicon requires retraining as their ambiguity classes are already accommodated</s> <s sid  ssid >vocabulary independence is achieved by predicting categories for words not in the lexicon using both context and suffix information</s> <s sid  ssid >probabilities corresponding to category sequences that never occurred in the training data are assigned small nonzero values ensuring that the model will accept any sequence of tokens while still providing the most likely tagging</s> <s sid  ssid >by using the fact that words are typically associated with only a few partofspeech categories and carefully ordering the computation the algorithms have linear complexity section </s> <s sid  ssid > hidden markov modeling the hidden markov modeling component of our tagger is implemented as an independent module following the specgiven in levinson et with special attention to space and time efficiency issues</s> <s sid  ssid >only firstorder modeling is addressed and will be presumed for the remainder of this discussion</s> <s sid  ssid > formalism brief an a doubly stochastic process that generates sequence of symbols  sls   ltilt where w is some finite set of possible symbols by composing an underlying markov process with a statedependent symbol generator ie a markov process with noise</s> <s sid  ssid >th markov process captures the notion of sequence depen and is described by a set of a matrix c probabilities a    lt a the probability of moving from state i to state of initial probabilities h     lt i lt is the probability of starting in state i</s> <s sid  ssid >the symbol ger erator is a statedependent measure on v described by of symbol probabilities  lt j lt lt lt m   iw i is the probability  symbol given that the markov process is i in partofspeech tagging we will model word order di pendency through an underlying markov process that  erates in terms of lexical tags yet we will only be ab to observe the sets of tags or ambiguity classes that ai possible for individual words</s> <s sid  ssid >the ambiguity class of eac word is the set of its permitted parts of speech only or of which is correct in context</s> <s sid  ssid >given the parameters  markov modeling allows us to compute ti most probable sequence of state transitions and hence a mostly likely sequence of lexical tags corresponding to of ambiguity classes</s> <s sid  ssid >in the following identified with the number of possible tags and w wil the set of all ambiguity classes</s> <s sid  ssid >applying an hmm consists of two tasks estimating ti parameters a a training set ar computing the most likely sequence of underlying sta transitions given new observations</s> <s sid  ssid >maximum likeliho estimates that is estimates that maximize the probabili of the training set can be found through application of z ternating expectation in a procedure known as the baur welch or forwardbackward algorithm baum  proceeds by recursively defining two sets of probabiliti the forward probabilities eft</s> <s sid  ssid >i   bisti lt t lt t      for all the backward prob bilities ii     lt t lt     for all forward probabili the joint probability of the sequence up to tir t s  the event that the markov pr is in state i at time the backwa is the probability of seeing the sequen  st that the markov process is state i at time t it follows that the probability of t entire sequence is n n  </s> </abstract> <section title  desiderata number > <s sid  ssid >many words are ambiguous in their part of speech</s> <s sid  ssid >for example tag can be a noun or a verb</s> <s sid  ssid >however when a word appears in the context of other words the ambiguity is often reduced in a tag is a partofspeech label the word tag can only be a noun</s> <s sid  ssid >a partofspeech tagger is a system that uses context to assign parts of speech to words</s> <s sid  ssid >automatic text tagging is an important first step in discovering the linguistic structure of large text corpora</s> <s sid  ssid >partofspeech information facilitates higherlevel analysis such as recognizing noun phrases and other patterns in text</s> <s sid  ssid >for a tagger to function as a practical component in a language processing system we believe that a tagger must be robust text corpora contain ungrammatical constructions isolated phrases such as titles and nonlinguistic data such as tables</s> <s sid  ssid >corpora are also likely to contain words that are unknown to the tagger</s> <s sid  ssid >it is desirable that a tagger deal gracefully with these situations</s> <s sid  ssid >efficient if a tagger is to be used to analyze arbitrarily large corpora it must be efficientperforming in time linear in the number of words tagged</s> <s sid  ssid >any training required should also be fast enabling rapid turnaround with new corpora and new text genres</s> <s sid  ssid >accurate a tagger should attempt to assign the correct partofspeech tag to every word encountered</s> <s sid  ssid >tunable a tagger should be able to take advantage of linguistic insights</s> <s sid  ssid >one should be able to correct systematic errors by supplying appropriate a priori hints it should be possible to give different hints for different corpora</s> <s sid  ssid >reusable the effort required to retarget a tagger to new corpora new tagsets and new languages should be minimal</s> </section> <section title  methodology number > <s sid  ssid >several different approaches have been used for building text taggers</s> <s sid  ssid >greene and rubin used a rulebased approach in the taggit program greene and rubin  which was an aid in tagging the brown corpus francis and kueera </s> <s sid  ssid >taggit disambiguated  of the corpus the rest was done manually over a period of several years</s> <s sid  ssid >more recently koskenniemi also used a rulebased approach implemented with finitestate machines koskenniemi </s> <s sid  ssid >statistical methods have also been used eg derose  garside et al </s> <s sid  ssid >these provide the capability of resolving ambiguity on the basis of most likely interpretation</s> <s sid  ssid >a form of markov model has been widely used that assumes that a word depends probabilistically on just its partofspeech category which in turn depends solely on the categories of the preceding two words</s> <s sid  ssid >two types of training ie parameter estimation have been used with this model</s> <s sid  ssid >the first makes use of a tagged training corpus</s> <s sid  ssid >derouault and merialdo use a bootstrap method for training derouault and merialdo </s> <s sid  ssid >at first a relatively small amount of text is manually tagged and used to train a partially accurate model</s> <s sid  ssid >the model is then used to tag more text and the tags are manually corrected and then used to retrain the model</s> <s sid  ssid >church uses the tagged brown corpus for training church </s> <s sid  ssid >these models involve probabilities for each word in the lexicon so large tagged corpora are required for reliable estimation</s> <s sid  ssid >the second method of training does not require a tagged training corpus</s> <s sid  ssid >in this situation the baumwelch algorithm also known as the forwardbackward algorithm can be used baum </s> <s sid  ssid >under this regime the model is called a hidden markov model hmm as state transitions ie partofspeech categories are assumed to be unobservable</s> <s sid  ssid >jelinek has used this method for training a text tagger jelinek </s> <s sid  ssid >parameter smoothing can be conveniently achieved using the method of deleted interpolation in which weighted estimates are taken from secondand firstorder models and a uniform probability distribution jelinek and mercer </s> <s sid  ssid >kupiec used word equivalence classes referred to here as ambiguity classes based on parts of speech to pool data from individual words kupiec b</s> <s sid  ssid >the most common words are still represented individually as sufficient data exist for robust estimation</s> <s sid  ssid >however all other words are represented according to the set of possible categories they can assume</s> <s sid  ssid >in this manner the vocabulary of  words in the brown corpus can be reduced to approximately  distinct ambiguity classes kupiec </s> <s sid  ssid >to further reduce the number of parameters a firstorder model can be employed this assumes that a words category depends only on the immediately preceding words category</s> <s sid  ssid >in kupiec a networks are used to selectively augment the context in a basic firstorder model rather than using uniformly secondorder dependencies</s> <s sid  ssid >we next describe how our choice of techniques satisfies the criteria listed in section </s> <s sid  ssid >the use of an hmm permits complete flexibility in the choice of training corpora</s> <s sid  ssid >text from any desired domain can be used and a tagger can be tailored for use with a particular text database by training on a portion of that database</s> <s sid  ssid >lexicons containing alternative tag sets can be easily accommodated without any need for relabeling the training corpus affording further flexibility in the use of specialized tags</s> <s sid  ssid >as the resources required are simply a lexicon and a suitably large sample of ordinary text taggers can be built with minimal effort even for other languages such as french eg kupiec </s> <s sid  ssid >the use of ambiguity classes and a firstorder model reduces the number of parameters to be estimated without significant reduction in accuracy discussed in section </s> <s sid  ssid >this also enables a tagger to be reliably trained using only moderate amounts of text</s> <s sid  ssid >we have produced reasonable results training on as few as  sentences</s> <s sid  ssid >fewer parameters also reduce the time required for training</s> <s sid  ssid >relatively few ambiguity classes are sufficient for wide coverage so it is unlikely that adding new words to the lexicon requires retraining as their ambiguity classes are already accommodated</s> <s sid  ssid >vocabulary independence is achieved by predicting categories for words not in the lexicon using both context and suffix information</s> <s sid  ssid >probabilities corresponding to category sequences that never occurred in the training data are assigned small nonzero values ensuring that the model will accept any sequence of tokens while still providing the most likely tagging</s> <s sid  ssid >by using the fact that words are typically associated with only a few partofspeech categories and carefully ordering the computation the algorithms have linear complexity section </s> </section> <section title  hidden markov modeling number > <s sid  ssid >the hidden markov modeling component of our tagger is implemented as an independent module following the specification given in levinson et al  with special attention to space and time efficiency issues</s> <s sid  ssid >only firstorder modeling is addressed and will be presumed for the remainder of this discussion</s> <s sid  ssid >in brief an hmm is a doubly stochastic process that generates sequence of symbols s  sls  st sew  ltilt t where w is some finite set of possible symbols by composing an underlying markov process with a statedependent symbol generator ie a markov process with noise</s> <s sid  ssid >th markov process captures the notion of sequence depen dency and is described by a set of n states a matrix c transition probabilities a   la  lt i j lt n where a is the probability of moving from state i to state j and vector of initial probabilities h     lt i lt n where  is the probability of starting in state i</s> <s sid  ssid >the symbol ger erator is a statedependent measure on v described by matrix of symbol probabilities b kik  lt j lt n an  lt k lt m where m   iw i and kik is the probability  generating symbol sk given that the markov process is i state j in partofspeech tagging we will model word order di pendency through an underlying markov process that  erates in terms of lexical tags yet we will only be ab to observe the sets of tags or ambiguity classes that ai possible for individual words</s> <s sid  ssid >the ambiguity class of eac word is the set of its permitted parts of speech only or of which is correct in context</s> <s sid  ssid >given the parameters  and ii hidden markov modeling allows us to compute ti most probable sequence of state transitions and hence a mostly likely sequence of lexical tags corresponding to sequence of ambiguity classes</s> <s sid  ssid >in the following n can identified with the number of possible tags and w wil the set of all ambiguity classes</s> <s sid  ssid >applying an hmm consists of two tasks estimating ti model parameters a b and h from a training set ar computing the most likely sequence of underlying sta transitions given new observations</s> <s sid  ssid >maximum likeliho estimates that is estimates that maximize the probabili of the training set can be found through application of z ternating expectation in a procedure known as the baur welch or forwardbackward algorithm baum  proceeds by recursively defining two sets of probabiliti the forward probabilities where oti   ribi for all i and the backward prob bilities where otj    for all j</s> <s sid  ssid >the forward probabili oeti is the joint probability of the sequence up to tir t s s  st and the event that the markov pr cess is in state i at time t similarly the backwa probability otj is the probability of seeing the sequen sti st  st given that the markov process is state i at time t it follows that the probability of t entire sequence is for any tin the range  ltt lt t   for an introduction to hidden markov modeling see f biner and juang </s> <s sid  ssid >given an initial choice for the parameters a b and h the expected number of transitions  from state i to state j conditioned on the observation sequence s may be computed as follows rescale</s> <s sid  ssid >one approach premultiplies the a and  probabilities with an accumulating product depending on t levinson et al </s> <s sid  ssid >let eii   aii and define and  </s> <s sid  ssid >  y</s> <s sid  ssid > in summary to find maximum likelihood estimates for a b and h via the baumwelch algorithm one chooses some starting values applies equations  to compute new values and then iterates until convergence</s> <s sid  ssid >it can be shown that this algorithm will converge although possibly to a nonglobal maximum baum </s> <s sid  ssid >once a model has been estimated selecting the most likely underlying sequence of state transitions corresponding to an observation s can be thought of as a maximization over all sequences that might generate s an efficient dynamic programming procedure known as the viterbi algorithm viterbi  arranges for this computation to proceed in time proportional to t suppose v   vt  lt t lt t is a state sequence that generates s then the probability that v generates s is to find the most probable such sequence we start by defining i   rabisi for  lt i lt n and then perform the recursion for  lt t lt t and  lt j lt n the crucial observation is that for each time t and each state i one need only consider the most probable sequence arriving at state i at time t the probability of the most probable sequence is maxiltltnoti while the sequence itself can be reconstructed by defining vt   maxltn oti and vt    ikeqt for t gt t gt </s> <s sid  ssid >the baumwelch algorithm equations  and the viterbi algorithm equation  involve operations on products of numbers constrained to be between  and </s> <s sid  ssid >since these products can easily underflow measures must be taken to now define ampti   ciciti and use a in place of a in equation  to define amp for the next iteration note that eini eeti    for  lt t lt t similarly let oti   oti and define ti   ctti for t gt t gt  where the scaled backward and forward probabilities amp and  can be exchanged for the unscaled probabilities in equations  without affecting the value of the ratios</s> <s sid  ssid >to see this note that ampti   cati and ti where now in terms of the scaled probabilities equation  for example can be seen to be unchanged a slight difficulty occurs in equation  that can be cured by the addition of a new term cti in each product of the upper sum numerical instability in the viterbi algorithm can be ameliorated by operating on a logarithmic scale levinson et al </s> <s sid  ssid >that is one maximizes the log probability of each sequence of state transitions care must be taken with zero probabilities</s> <s sid  ssid >however this can be elegantly handled through the use of ieee negative infinity p </s> <s sid  ssid >as can be seen from equations  the time cost of training is tn</s> <s sid  ssid >similarly as given in equation  the viterbi algorithm is also tn</s> <s sid  ssid >however in partofspeech tagging the problem structure dictates that the matrix of symbol probabilities b is sparsely populated</s> <s sid  ssid >that is  if the ambiguity class corresponding to symbol j includes the partofspeech tag associated with state i</s> <s sid  ssid >in practice the degree of overlap between ambiguity classes is relatively low some tokens are assigned unique tags and hence have only one nonzero symbol probability</s> <s sid  ssid >the sparseness of b leads one to consider restructuring equations  so a check for zero symbol probability can obviate the need for further computation</s> <s sid  ssid >equation  is already conveniently factored so that the dependence on bjsti  is outside the inner sum</s> <s sid  ssid >hence if k is the average number of nonzero entries in each row of b the cost of computing equation  can be reduced to ktn</s> <s sid  ssid >equations  can be similarly reduced by switching the order of iteration</s> <s sid  ssid >for example in equation  rather than for a given t computing oti for each i one at a time one can accumulate terms for all i in parallel</s> <s sid  ssid >the net effect of this rewriting is to place a bsti    check outside the innermost iteration</s> <s sid  ssid >equations  and  submit to a similar approach</s> <s sid  ssid >equation  is already only n</s> <s sid  ssid >hence the overall cost of training can be reduced to ktn which in our experience amounts to an order of magnitude speedup</s> <s sid  ssid > the time complexity of the viterbi algorithm can also be reduced to ktn by noting that b s can be factored out of the maximization of equation </s> <s sid  ssid >adding up the sizes of the probability matrices a b and h it is easy to see that the storage cost for directly representing one model is proportional to nn m </s> <s sid  ssid >running the baumwelch algorithm requires storage for the sequence of observations the a and  probabilities the vector c and copies of the a and b matrices since the originals cannot be overwritten until the end of each iteration</s> <s sid  ssid >hence the grand total of space required for training is proportional to t nt n m  </s> <s sid  ssid >since n and m are fixed by the model the only parameter that can be varied to reduce storage costs is t now adequate training requires processing from tens of thousands to hundreds of thousands of tokens kupiec a</s> <s sid  ssid >the training set can be considered one long sequence it which case t is very large indeed or it can be broken up into a number of smaller sequences at convenient boundaries</s> <s sid  ssid >in firstorder hidden markov modeling the stochastic process effectively restarts at unambiguous tokens such as sentence and paragraph markers hence these tokens are convenient points at which to break the training set</s> <s sid  ssid >if the baumwelch algorithm is run separately from the same starting point on each piece the resulting trained models must be recombined in some way</s> <s sid  ssid >one obvious approach is simply to average</s> <s sid  ssid >however this fails if any two an equivalent approach maintains a mapping from states i to nonzero symbol probabilities and simply avoids in the inner iteration computing products which must be zero kupiec  states are indistinguishable in the sense that they had the same transition probabilities and the same symbol probabilities at start because states are then not matched across trained models</s> <s sid  ssid >it is therefore important that each state have a distinguished role which is relatively easy to achieve in partofspeech tagging</s> <s sid  ssid >our implementation of the baumwelch algorithm breaks up the input into fixedsized pieces of training text</s> <s sid  ssid >the baumwelch algorithm is then run separately on each piece and the results are averaged together</s> <s sid  ssid >running the viterbi algorithm requires storage for the sequence of observations a vector of current maxes a scratch array of the same size and a matrix of ib indices for a total proportional to t  n t and a grand total including the model of t nn  m t </s> <s sid  ssid >again n and m are fixed</s> <s sid  ssid >however t need not be longer than a single sentence since as was observed above the hmm and hence the viterbi algorithm restarts at sentence boundaries</s> <s sid  ssid >an hmm for partofspeech tagging can be tuned in a variety of ways</s> <s sid  ssid >first the choice of tagset and lexicon determines the initial model</s> <s sid  ssid >second empirical and a priori information can influence the choice of starting values for the baumwelch algorithm</s> <s sid  ssid >for example counting instances of ambiguity classes in running text allows one to assign nonuniform starting probabilities in a for a particular tags realization as a particular ambiguity class</s> <s sid  ssid >alternatively one can state a priori that a particular ambiguity class is most likely to be the reflection of some subset of its component tags</s> <s sid  ssid >for example if an ambiguity class consisting of the open class tags is used for unknown words one may encode the fact that most unknown words are nouns or proper nouns by biasing the initial probabilities in b</s> <s sid  ssid >another biasing of starting values can arises from noting that some tags are unlikely to be followed by others</s> <s sid  ssid >for example the lexical item to maps to an ambiguity class containing two tags infinitivemarker and toaspreposition neither of which occurs in any other ambiguity class</s> <s sid  ssid >if nothing more were stated the hmm would have two states which were indistinguishable</s> <s sid  ssid >this can be remedied by setting the initial transition probabilities from infinitivemarker to strongly favor transitions to such states as verbuninflected and adverb</s> <s sid  ssid >our implementation allows for two sorts of biasing of starting values ambiguity classes can be annotated with favored tags and states can be annotated with favored transitions</s> <s sid  ssid >these biases may be specified either as sets or as set complements</s> <s sid  ssid >biases are implemented by replacing the disfavored probabilities with a small constant machine epsilon and redistributing mass to the other possibilities</s> <s sid  ssid >this has the effect of disfavoring the indicated outcomes without disallowing them sufficient converse data can rehabilitate these values</s> </section> <section title  architecture number > <s sid  ssid >in support of this and other work we have developed a system architecture for text access cutting et al </s> <s sid  ssid >this architecture defines five components for such systems corpus which provides text in a generic manner analysis which extracts terms from the text index which stores term occurrence statistics and search which utilizes these statistics to resolve queries</s> <s sid  ssid >the partofspeech tagger described here is implemented as an analysis module</s> <s sid  ssid >figure  illustrates the overall architecture showing the tagger analysis implementation in detail</s> <s sid  ssid >the tagger itself has a modular architecture isolating behind standard protocols those elements which may vary enabling easy substitution of alternate implementations</s> <s sid  ssid >also illustrated here are the data types which flow between tagger components</s> <s sid  ssid >as an analysis implementation the tagger must generate terms from text</s> <s sid  ssid >in this context a term is a word stem annotated with part of speech</s> <s sid  ssid >text enters the analysis subsystem where the first processing module it encounters is the tokenizer whose duty is to convert text a sequence of characters into a sequence of tokens</s> <s sid  ssid >sentence boundaries are also identified by the tokenizer and are passed as reserved tokens</s> <s sid  ssid >the tokenizer subsequently passes tokens to the lexicon</s> <s sid  ssid >here tokens are converted into a set of stems each annotated with a partofspeech tag</s> <s sid  ssid >the set of tags identifies an ambiguity class</s> <s sid  ssid >the identification of these classes is also the responsibility of the lexicon</s> <s sid  ssid >thus the lexicon delivers a set of stems paired with tags and an ambiguity class</s> <s sid  ssid >the training module takes long sequences of ambiguity classes as input</s> <s sid  ssid >it uses the baumwelch algorithm to produce a trained hmm an input to the tagging module</s> <s sid  ssid >training is typically performed on a sample of the corpus at hand with the trained hmm being saved for subsequent use on the corpus at large</s> <s sid  ssid >the tagging module buffers sequences of ambiguity classes between sentence boundaries</s> <s sid  ssid >these sequences are disambiguated by computing the maximal path through the hmm with the viterbi algorithm</s> <s sid  ssid >operating at sentence granularity provides fast throughput without loss of accuracy as sentence boundaries are unambiguous</s> <s sid  ssid >the resulting sequence of tags is used to select the appropriate stems</s> <s sid  ssid >pairs of stems and tags are subsequently emitted</s> <s sid  ssid >the tagger may function as a complete analysis component providing tagged text to search and indexing components or as a subsystem of a more elaborate analysis such as phrase recognition</s> <s sid  ssid >the problem of tokenization has been well addressed by much work in compilation of programming languages</s> <s sid  ssid >the accepted approach is to specify token classes with regular expressions</s> <s sid  ssid >these may be compiled into a single deterministic finite state automaton which partitions character streams into labeled tokens aho et al  lesk </s> <s sid  ssid >in the context of tagging we require at least two token classes sentence boundary and word</s> <s sid  ssid >other classes may include numbers paragraph boundaries and various sorts of punctuation eg braces of various types commas</s> <s sid  ssid >however for simplicity we will henceforth assume only words and sentence boundaries are extracted</s> <s sid  ssid >just as with programming languages with text it is not always possible to unambiguously specify the required token classes with regular expressions</s> <s sid  ssid >however the addition of a simple lookahead mechanism which allows specification of right context ameliorates this aho et al  lesk </s> <s sid  ssid >for example a sentence boundary in english text might be identified by a period followed bywhitespace followed by an uppercase letter</s> <s sid  ssid >however the uppercase letter must not be consumed as it is the first component of the next token</s> <s sid  ssid >a lookahead mechanism allows us to specify in the sentenceboundary regular expression that the final character matched should not be considered a part of the token</s> <s sid  ssid >this method meets our stated goals for the overall system</s> <s sid  ssid >it is efficient requiring that each character be examined only once modulo lookahead</s> <s sid  ssid >it is easily parameterizable providing the expressive power to concisely define accurate and robust token classes</s> <s sid  ssid >the lexicon module is responsible for enumerating parts of speech and their associated stems for each word it is given</s> <s sid  ssid >for the english word does the lexicon might return do verb and doe pluralnoun it is also responsible for identifying ambiguity classes based upon sets of tags</s> <s sid  ssid >we have employed a threestage implementation first we consult a manuallyconstructed lexicon to find stems and parts of speech</s> <s sid  ssid >exhaustive lexicons of this sort are expensive if not impossible to produce</s> <s sid  ssid >fortunately a small set of words accounts for the vast majority of word occurences</s> <s sid  ssid >thus high coverage can be obtained without prohibitive effort</s> <s sid  ssid >words not found in the manually constructed lexicon are generally both open class and regularly inflected</s> <s sid  ssid >as a second stage a languagespecific method can be employed to guess ambiguity classes for unknown words</s> <s sid  ssid >for many languages eg english and french word suffixes provide strong cues to words possible categories</s> <s sid  ssid >probabalistic predictions of a words category can be made by analyzing suffixes in untagged text kupiec  meteer et al </s> <s sid  ssid >as a final stage if a word is not in the manually constructed lexicon and its suffix is not recognized a default ambiguity class is used</s> <s sid  ssid >this class typically contains all the open class categories in the language</s> <s sid  ssid >dictionaries and suffix tables are both efficiently implementable as letter trees or tries knuth  which require that each character of a word be examined only once during a lookup</s> </section> <section title  performance number > <s sid  ssid >in this section we detail how our tagger meets the desiderata that we outlined in section </s> <s sid  ssid >the system is implemented in common lisp steele </s> <s sid  ssid >all timings reported are for a sun sparcstation</s> <s sid  ssid >the english lexicon used contains  tags m    and  ambiguity classes n   </s> <s sid  ssid >training was performed on  words in articles selected randomly from groliers encyclopedia</s> <s sid  ssid >five iterations of training were performed in a total time of  cpu seconds</s> <s sid  ssid >following is a time breakdown by component training average pseconds per token tokenizer lexicon  iteration  iterations total      tagging was performed on  words in a collection of articles by the journalist dave barry</s> <s sid  ssid >this required a total of of  cpu seconds</s> <s sid  ssid >the time breakdown for this was as follows tagging average pseconds per token tokenizer lexicon viterbi total     it can be seen from these figures that training on a new corpus may be accomplished in a matter of minutes and that tens of megabytes of text may then be tagged per hour</s> <s sid  ssid >when using a lexicon and tagset built from the tagged text of the brown corpus francis and kueera  training on one half of the corpus about  words and tagging the other  of word instances were assigned the correct tag</s> <s sid  ssid >eight iterations of training were used</s> <s sid  ssid >this level of accuracy is comparable to the best achieved by other taggers church  merialdo </s> <s sid  ssid >the brown corpus contains fragments and ungrammaticalities thus providing a good demonstration of robustness</s> <s sid  ssid >a tagger should be tunable so that systematic tagging errors and anomalies can be addressed</s> <s sid  ssid >similarly it is important that it be fast and easy to target the tagger to new genres and languages and to experiment with different tagsets reflecting different insights into the linguistic phenomena found in text</s> <s sid  ssid >in section  we describe how the hmm implementation itself supports tuning</s> <s sid  ssid >in addition our implementation supports a number of explicit parameters to facilitate tuning and reuse including specification of lexicon and training corpus</s> <s sid  ssid >there is also support for a flexible tagset</s> <s sid  ssid >for example if we want to collapse distinctions in the lexicon such as those between positive comparative and superlative adjectives we only have to make a small change in the mapping from lexicon to tagset</s> <s sid  ssid >similarly if we wish to make finer grain distinctions than those available in the lexicon such as case marking on pronouns there is a simple way to note such exceptions</s> </section> <section title  applications number > <s sid  ssid >we have used the tagger in a number of applications</s> <s sid  ssid >we describe three applications here phrase recognition word sense disambiguation and grammatical function assignment</s> <s sid  ssid >these projects are part of a research effort to use shallow analysis techniques to extract content from unrestricted text</s> <s sid  ssid >we have constructed a system that recognizes simple phrases when given as input the sequence of tags for a sentence</s> <s sid  ssid >there are recognizers for noun phrases verb groups adverbial phrases and prepositional phrases</s> <s sid  ssid >each of these phrases comprises a contiguous sequence of tags that sat is fies a simple grammar</s> <s sid  ssid >for example a noun phrase can be a unary sequence containing a pronoun tag or an arbitrar ily long sequence of noun and adjective tags possibly pre ceded by a determiner tag and possibly with an embeddec possessive marker</s> <s sid  ssid >the longest possible sequence is founc eg the program committee but not the program conjunctions are not recognized as part of any phrase for example in the fragment the cats and dogs the cats and dogs will be recognized as two noun phrases</s> <s sid  ssid >prepositional phrase attachment is not performed at this stage of processing</s> <s sid  ssid >this approach to phrase recognition in some cases captures only parts of some phrases however our approach minimizes false positives so that we can rely on the recognizers results</s> <s sid  ssid >partofspeech tagging in and of itself is a useful tool in lexical disambiguation for example knowing that dig is being used as a noun rather than as a verb indicates the words appropriate meaning</s> <s sid  ssid >but many words have multiple meanings even while occupying the same part of speech</s> <s sid  ssid >to this end the tagger has been used in the implementation of an experimental noun homograph disambiguation algorithm hearst </s> <s sid  ssid >the algorithm known as catchword performs supervised training over a large text corpus gathering lexical orthographic and simple syntactic evidence for each sense of the ambiguous noun</s> <s sid  ssid >after a period of training catch word classifies new instances of the noun by checking its context against that of previously observed instances and choosing the sense for which the most evidence is found</s> <s sid  ssid >because the sense distinctions made are coarse the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms</s> <s sid  ssid >initial tests resulted in accuracies of around  for nouns with strongly distinct senses</s> <s sid  ssid >this algorithm uses the tagger in two ways i to determine the part of speech of the target word filtering out the nonnoun usages and ii as a step in the phrase recognition analysis of the context surrounding the noun</s> <s sid  ssid >the phrase recognizers also provide input to a system sopa sibun  which recognizes nominal arguments of verbs specifically subject object and predicative arguments</s> <s sid  ssid >sopa does not rely on information such as arity or voice specific to the particular verbs involved</s> <s sid  ssid >the first step in assigning grammatical functions is to partition the tag sequence of each sentence into phrases</s> <s sid  ssid >the phrase types include those mentioned in section  additional types to account for conjunctions complementizers and indicators of sentence boundaries and an unknown type</s> <s sid  ssid >after a sentence has been partitioned each simple noun phrase is examined in the context of the phrase to its left and the phrase to its right</s> <s sid  ssid >on the basis of this local context and a set of rules the noun phrase is marked as a syntactic subject object predicative or is not marked at all</s> <s sid  ssid >a label of predicative is assigned only if it can be determined that the governing verb group is a form of a predicating verb eg a form of be</s> <s sid  ssid >because this cannot always be determined some predicatives are labeled objects</s> <s sid  ssid >if a noun phrase is labeled it is also annotated as to whether the governing verb is the closest verb group to the right or to the left</s> <s sid  ssid >the algorithm has an accuracy of approximately  in assigning grammatical functions</s> </section> <section title acknowledgments number > <s sid  ssid >we would like to thank marti hearst for her contributions to this paper lauri karttunen and annie zaenen for their work on lexicons and kris halvorsen for supporting this project</s> </section>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_treino2[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b970b8",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c914b",
   "metadata": {},
   "source": [
    "## Pré-Processamento para Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25b81d5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:04.775957Z",
     "start_time": "2023-03-03T01:48:04.763281Z"
    }
   },
   "outputs": [],
   "source": [
    "def separa_frases(txt: str):\n",
    "    \"\"\"Separa as frases dos papers\"\"\"\n",
    "    txt = txt.split('</s>')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac376f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:04.791763Z",
     "start_time": "2023-03-03T01:48:04.778343Z"
    }
   },
   "outputs": [],
   "source": [
    "def separa_frases_ouro(txt: str):\n",
    "    \"\"\"Separa as frases dos papers\"\"\"\n",
    "    txt = txt.split('\\n')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07b1a9d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:04.807244Z",
     "start_time": "2023-03-03T01:48:04.794219Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessamento_secundario(txt: str):\n",
    "    \"\"\"Remoção das demais tags após quebrar no </s>\"\"\"\n",
    "    txt = re.sub(r'<.*?>','', txt)\n",
    "    txt = re.sub('\\/','', txt)\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "    txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49eea4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:04.828919Z",
     "start_time": "2023-03-03T01:48:04.809226Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessamento_secundario_ouro(txt: str):\n",
    "    \"\"\"Remoção das demais tags após quebrar no \\n\"\"\"\n",
    "    txt = re.sub(r'<.*?>','', txt)\n",
    "    txt = re.sub('\\\\n',' ', txt)\n",
    "    txt = re.sub('\\/','', txt)\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "    txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3254c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:04.844360Z",
     "start_time": "2023-03-03T01:48:04.831342Z"
    }
   },
   "outputs": [],
   "source": [
    "def segundo_preprocessamento(dicionario):\n",
    "    \"\"\"input: dicionário\n",
    "        output: frases pré-processadas\"\"\"\n",
    "    frases_papers_treino={}\n",
    "    for i,v in dicionario.items():\n",
    "        x=[]\n",
    "        for j in v:\n",
    "            j=preprocessamento_secundario(j)\n",
    "            if j!='': #remove itens nulos da lista\n",
    "                x.append(j)\n",
    "        frases_papers_treino[i]= x\n",
    "    return frases_papers_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08555b39",
   "metadata": {},
   "source": [
    "frases_papers_treino={}\n",
    "for i,v in papers_treino3.items():\n",
    "    x=[]\n",
    "    for j in v:\n",
    "        j=preprocessamento_secundario(j)\n",
    "        if j!='': #remove itens nulos da lista\n",
    "            x.append(j)\n",
    "    frases_papers_treino[i]= x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c2ba8b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:08.539865Z",
     "start_time": "2023-03-03T01:48:04.846839Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_treino3 = {k: separa_frases(v) for k, v in papers_treino2.items()}\n",
    "frases_papers_treino = segundo_preprocessamento(papers_treino3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f66fef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:09.744000Z",
     "start_time": "2023-03-03T01:48:08.539865Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_teste3 = {k: separa_frases(v) for k, v in papers_teste2.items()}\n",
    "frases_papers_teste = segundo_preprocessamento(papers_teste3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf755be",
   "metadata": {},
   "source": [
    "### Input Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee892260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:10.162718Z",
     "start_time": "2023-03-03T01:48:09.746257Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_tokens=[]\n",
    "for i,v in frases_papers_treino.items():\n",
    "    for j in v:\n",
    "        lista_tokens.append(j.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dacef5",
   "metadata": {},
   "source": [
    "### Input FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faa997cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:48:10.294862Z",
     "start_time": "2023-03-03T01:48:10.165086Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('preprocessado_treino_fasttext.txt', 'w', encoding='utf-8') as f:\n",
    "    for lista in frases_papers_treino.values():\n",
    "        for line in lista:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a846088",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fdc3a2",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a583df14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:51:37.287177Z",
     "start_time": "2023-03-03T01:48:10.294862Z"
    }
   },
   "outputs": [],
   "source": [
    "model_w2v_cbow = Word2Vec(lista_tokens, min_count=1, epochs=100, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e36ffed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T01:51:37.302552Z",
     "start_time": "2023-03-03T01:51:37.289155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46323"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_w2v_cbow.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f8589",
   "metadata": {},
   "source": [
    "### SKIP-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08a3d9e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:01:02.542028Z",
     "start_time": "2023-03-03T01:51:37.305548Z"
    }
   },
   "outputs": [],
   "source": [
    "model_w2v_sg = Word2Vec(lista_tokens, min_count=1, epochs=100, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a6d3bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:01:02.557673Z",
     "start_time": "2023-03-03T02:01:02.542028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46323"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_w2v_sg.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8154c",
   "metadata": {},
   "source": [
    "## Fast Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbbcd2d",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34f6eebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:15:19.457317Z",
     "start_time": "2023-03-03T02:01:02.558375Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ft_cbow = fasttext.train_unsupervised('preprocessado_treino_fasttext.txt', model='cbow', minCount=1, epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a9099bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:15:22.905862Z",
     "start_time": "2023-03-03T02:15:19.457317Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fasttext.FastText._FastText at 0x1d80a617460>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft_cbow.save_model('model_ft_cbow.bin')\n",
    "load_model('model_ft_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47bf6eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:15:22.952525Z",
     "start_time": "2023-03-03T02:15:22.928260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46324"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_ft_cbow.get_words( on_unicode_error='ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4263e3f",
   "metadata": {},
   "source": [
    "### SKIP-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dfe1f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:31.979145Z",
     "start_time": "2023-03-03T02:15:22.952525Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ft_sg = fasttext.train_unsupervised('preprocessado_treino_fasttext.txt', model='skipgram', minCount=1, epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecb7acbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:35.354899Z",
     "start_time": "2023-03-03T02:36:31.979145Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fasttext.FastText._FastText at 0x1d80a1c9c70>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft_sg.save_model('model_ft_sg.bin')\n",
    "load_model('model_ft_sg.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71a1d640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:35.386497Z",
     "start_time": "2023-03-03T02:36:35.354899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46324"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_ft_sg.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed881bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:30:21.498788Z",
     "start_time": "2023-02-28T02:30:21.498788Z"
    }
   },
   "source": [
    "len(model_ft_sg.get_words( on_unicode_error='ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16362643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:15:49.857224Z",
     "start_time": "2023-02-28T02:15:49.830541Z"
    }
   },
   "source": [
    "print(model_ft_sg.get_subwords('tnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eacf0c06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:35.481067Z",
     "start_time": "2023-03-03T02:36:35.386497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comparando os tokens dos 2 embeddings\n",
    "lista_w2v=[]\n",
    "for i in model_w2v_sg.wv.key_to_index:\n",
    "    lista_w2v.append(i)\n",
    "    \n",
    "lista_ft=[]\n",
    "for i in model_ft_sg.get_words( on_unicode_error='ignore'):\n",
    "    lista_ft.append(i)\n",
    "    \n",
    "list(set(lista_ft)-set(lista_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ce62d",
   "metadata": {},
   "source": [
    "# Medida de Similaridade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd82c6",
   "metadata": {},
   "source": [
    "## Detecção das Seções \n",
    "Abstract, Introdução e Conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f359dd3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:35.496966Z",
     "start_time": "2023-03-03T02:36:35.483102Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_abstract(corpus: str):\n",
    "    regIter = re.finditer(\"(<[Aa][Bb][sS][tT][Rr][aA][cC][tT]>)(.+?)(</[aA][bB][sS][tT][rR][aA][cC][tT]>)\", corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    return textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a6bc61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:35.513077Z",
     "start_time": "2023-03-03T02:36:35.499951Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_intro(corpus: str):\n",
    "    regIter = re.finditer('(<section.*[Ii][nN][tT][rR][oO][dD][Uu][Cc][Tt][Ii][Oo][Nn].*?>(.*)<\\/section>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    return textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "654d7de9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:35.528963Z",
     "start_time": "2023-03-03T02:36:35.515099Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_conclusion(corpus: str):\n",
    "    regIter1 = re.finditer('(<section.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/section>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos1 = [ t.group(2) for t in regIter1]\n",
    "    regIter2 = re.finditer('(<section.*[Aa][Cc][Kk][Nn][Oo][Ww][Ll][Ee][Dd][Gg][Mm][Ee][Nn][Tt][Ss].*?>(.*)<\\/section>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos2 = [ t.group(2) for t in regIter2]\n",
    "    regIter3 = re.finditer('(<section.*[Aa][Cc][Kk][Nn][Oo][Ww][Ll][Ee][Dd][Gg][Ee][Mm][Ee][Nn][Tt][Ss].*?>(.*)<\\/section>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos3 = [ t.group(2) for t in regIter3]\n",
    "    textos = textos1\n",
    "    for i in textos2:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    for i in textos3:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    return textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd26ab09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:36.517177Z",
     "start_time": "2023-03-03T02:36:35.530323Z"
    }
   },
   "outputs": [],
   "source": [
    "abstract        = {k: preprocessa_abstract(v) for k, v in papers_treino2.items()}\n",
    "abstract_frases = {k: separa_frases(j) for k, v in abstract.items() for j in v}\n",
    "abstract_frases = segundo_preprocessamento(abstract_frases)\n",
    "abstract_palavras = {k:[word_tokenize(f) for f in v] for k,v in abstract_frases.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdc3ba95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:54.148419Z",
     "start_time": "2023-03-03T02:36:36.517177Z"
    }
   },
   "outputs": [],
   "source": [
    "introducao = {k: preprocessa_intro(v) for k, v in papers_treino2.items()}\n",
    "introducao_frases = {k: separa_frases(j) for k, v in introducao.items() for j in v}\n",
    "introducao_frases = segundo_preprocessamento(introducao_frases)\n",
    "introducao_palavras = {k:[word_tokenize(f) for f in v] for k,v in introducao_frases.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea3998ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:56.304132Z",
     "start_time": "2023-03-03T02:36:54.148419Z"
    }
   },
   "outputs": [],
   "source": [
    "conclusao = {k: preprocessa_conclusion(v) for k, v in papers_treino2.items()}\n",
    "conclusao_frases = {k: separa_frases(j) for k, v in conclusao.items() for j in v}\n",
    "conclusao_frases = segundo_preprocessamento(conclusao_frases)\n",
    "conclusao_palavras = {k:[word_tokenize(f) for f in v] for k,v in conclusao_frases.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963dd1c",
   "metadata": {},
   "source": [
    "Validei que tudo mede 495"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e82e3",
   "metadata": {},
   "source": [
    "## Aplicação do Word Embedding\n",
    "\n",
    "`model_w2v_cbow`\n",
    "\n",
    "`model_w2v_sg`\n",
    "\n",
    "`model_ft_cbow`\n",
    "\n",
    "`model_ft_sg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5f6f414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:56.319650Z",
     "start_time": "2023-03-03T02:36:56.304132Z"
    }
   },
   "outputs": [],
   "source": [
    "def vetor_frases_w2v(sessao_palavras, modelo_embedding):\n",
    "    \"\"\"input: dicionário de sessão das palavras\n",
    "        output: dicionário de vetores\"\"\"\n",
    "    dicio={}\n",
    "    for k,v in sessao_palavras.items():\n",
    "        x=[]\n",
    "        for lista_frase in v:\n",
    "            soma=0\n",
    "            for palavra in lista_frase:\n",
    "                try:\n",
    "                    soma = soma + modelo_embedding.wv[palavra]\n",
    "                except:\n",
    "                    soma = soma\n",
    "            x.append(soma)\n",
    "        dicio[k] = x\n",
    "    return dicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9320de69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:36:56.335317Z",
     "start_time": "2023-03-03T02:36:56.320461Z"
    }
   },
   "outputs": [],
   "source": [
    "def vetor_frases_ft(sessao_palavras, modelo_embedding):\n",
    "    \"\"\"input: dicionário de sessão das palavras\n",
    "        output: dicionário de vetores\"\"\"\n",
    "    dicio={}\n",
    "    for k,v in sessao_palavras.items():\n",
    "        x=[]\n",
    "        for lista_frase in v:\n",
    "            soma=0\n",
    "            for palavra in lista_frase:\n",
    "                try:\n",
    "                    soma = soma + modelo_embedding.get_word_vector(palavra)\n",
    "                except:\n",
    "                    soma=soma\n",
    "            x.append(soma)\n",
    "        dicio[k] = x\n",
    "    return dicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07852e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:37:02.830646Z",
     "start_time": "2023-03-03T02:36:56.336071Z"
    }
   },
   "outputs": [],
   "source": [
    "v_w2v_cbow_abstract   = vetor_frases_w2v(abstract_palavras,model_w2v_cbow)\n",
    "v_w2v_cbow_introducao = vetor_frases_w2v(introducao_palavras,model_w2v_cbow)\n",
    "v_w2v_cbow_conclusao  = vetor_frases_w2v(conclusao_palavras,model_w2v_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c38f65d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:37:09.408915Z",
     "start_time": "2023-03-03T02:37:02.830646Z"
    }
   },
   "outputs": [],
   "source": [
    "v_w2v_sg_abstract   = vetor_frases_w2v(abstract_palavras,model_w2v_sg)\n",
    "v_w2v_sg_introducao = vetor_frases_w2v(introducao_palavras,model_w2v_sg)\n",
    "v_w2v_sg_conclusao  = vetor_frases_w2v(conclusao_palavras,model_w2v_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9979a318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:37:45.251195Z",
     "start_time": "2023-03-03T02:37:09.408915Z"
    }
   },
   "outputs": [],
   "source": [
    "v_ft_cbow_abstract   = vetor_frases_ft(abstract_palavras,model_ft_cbow)\n",
    "v_ft_cbow_introducao = vetor_frases_ft(introducao_palavras,model_ft_cbow)\n",
    "v_ft_cbow_conclusao  = vetor_frases_ft(conclusao_palavras,model_ft_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b0990e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:38:21.338495Z",
     "start_time": "2023-03-03T02:37:45.251195Z"
    }
   },
   "outputs": [],
   "source": [
    "v_ft_sg_abstract   = vetor_frases_ft(abstract_palavras,model_ft_sg)\n",
    "v_ft_sg_introducao = vetor_frases_ft(introducao_palavras,model_ft_sg)\n",
    "v_ft_sg_conclusao  = vetor_frases_ft(conclusao_palavras,model_ft_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1d99156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:38:21.354366Z",
     "start_time": "2023-03-03T02:38:21.338495Z"
    }
   },
   "outputs": [],
   "source": [
    "def funcao_sim_coseno(a,b):\n",
    "    if ((numpy.linalg.norm(a)==0) |(numpy.linalg.norm(b)==0)):\n",
    "        return 0\n",
    "    else:\n",
    "        valor_coseno = (numpy.dot(a, b))/(numpy.linalg.norm(a)* numpy.linalg.norm(b))\n",
    "        return valor_coseno\n",
    "    \n",
    "def dicionario_similaridade(vetor_a,vetor_b):\n",
    "    \"\"\"Input: vetor correspondente às frases das seções a e b\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    dicionario={}\n",
    "    for i in range(len(vetor_a)):\n",
    "        for j in range(len(vetor_b)):\n",
    "            dicionario[i,j]= funcao_sim_coseno(vetor_a[i],vetor_b[j])\n",
    "    return dicionario\n",
    "\n",
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbd028eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:38:21.370214Z",
     "start_time": "2023-03-03T02:38:21.354366Z"
    }
   },
   "outputs": [],
   "source": [
    "def aplicando_dicionario_similaridade(v1,v2):\n",
    "    \"\"\"cálculo da similaridade, pegando os 3 vetores mais similares\"\"\"\n",
    "    x = []\n",
    "    for i,j in zip(range(len(v1)),range(len(v2))):\n",
    "        x.append(dicionario_similaridade(v1[i], v2[j]))\n",
    "        \n",
    "    top_3_indices = []\n",
    "    for i in range(len(x)):\n",
    "        top_3_indices.append(sorted(x[i], key=x[i].get, reverse=True)[:3])\n",
    "        \n",
    "    lista_1=[]\n",
    "    for i in top_3_indices:\n",
    "        top3=[]\n",
    "        for j in i:\n",
    "            top3.append(j[0])\n",
    "        lista_1.append(list(set(top3)))\n",
    "        \n",
    "    return x,top_3_indices,lista_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b465f52e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:38:21.385908Z",
     "start_time": "2023-03-03T02:38:21.370214Z"
    }
   },
   "outputs": [],
   "source": [
    "def frases_interseccao(lista_1,lista_2):\n",
    "    \"\"\"comparando se os índices similares pertencem às duas listas (abstract e conclusão) e (abstract e intro) \n",
    "    e retorna os índices em comum\"\"\"\n",
    "    indices_objetivo=[]\n",
    "    for i,j in zip(lista_1,lista_2):\n",
    "        indices_objetivo.append(list(set.intersection(*map(set,[i,j]))))\n",
    "    return indices_objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb0d31b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:38:21.401498Z",
     "start_time": "2023-03-03T02:38:21.385908Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frases_objetivo(indices_objetivo, idx_paper):\n",
    "    \"\"\"retorna a frase objetivo [do abstract] a partir dos indices\"\"\"\n",
    "    frase_objetivo={}\n",
    "    for idx,idx_f,idx_ind in zip(idx_paper,range(len(list(abstract_frases.values()))),indices_objetivo):\n",
    "        lista_one=[]\n",
    "        for j in idx_ind:\n",
    "            lista_one.append(list(abstract_frases.values())[idx_f][j])\n",
    "            #print(idx_f)\n",
    "            #print(j)\n",
    "            #print(frases_abstract[idx_f][j])\n",
    "        frase_objetivo[idx] = lista_one\n",
    "    return frase_objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0066937",
   "metadata": {},
   "source": [
    "### Similaridade w2v cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2cdea0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:38:38.750345Z",
     "start_time": "2023-03-03T02:38:21.402397Z"
    }
   },
   "outputs": [],
   "source": [
    "#Abtract e Introdução\n",
    "sim_ab_intro_cbow,indices_ab_intro_cbow,lista_idx_ab_intro_cbow =aplicando_dicionario_similaridade(list(v_w2v_cbow_abstract.values()),list(v_w2v_cbow_introducao.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_cbow,indices_ab_conc_cbow,lista_idx_ab_c_cbow = aplicando_dicionario_similaridade(list(v_w2v_cbow_abstract.values()),list(v_w2v_cbow_conclusao.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_cbow = frases_interseccao(lista_idx_ab_intro_cbow,lista_idx_ab_c_cbow)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_cbow_w2v = get_frases_objetivo(indices_objetivo_cbow, papers_treino.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e5bdcd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:38:38.782193Z",
     "start_time": "2023-03-03T02:38:38.750934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frase_objetivo_w2v_cbow']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(frase_objetivo_cbow_w2v,'frase_objetivo_w2v_cbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64677996",
   "metadata": {},
   "source": [
    "### Similaridade w2v skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "382216cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:38:55.223625Z",
     "start_time": "2023-03-03T02:38:38.783379Z"
    }
   },
   "outputs": [],
   "source": [
    "#Abtract e Introdução\n",
    "sim_ab_intro_sg,indices_ab_intro_sg,lista_idx_ab_intro_sg =aplicando_dicionario_similaridade(list(v_w2v_sg_abstract.values()),list(v_w2v_sg_introducao.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_sg,indices_ab_conc_sg,lista_idx_ab_c_sg = aplicando_dicionario_similaridade(list(v_w2v_sg_abstract.values()),list(v_w2v_sg_conclusao.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_sg = frases_interseccao(lista_idx_ab_intro_sg,lista_idx_ab_c_sg)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_sg_w2v = get_frases_objetivo(indices_objetivo_sg, papers_treino.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8a9d222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:38:55.254994Z",
     "start_time": "2023-03-03T02:38:55.223625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frase_objetivo_w2v_sg']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(frase_objetivo_sg_w2v,'frase_objetivo_w2v_sg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a82f3e",
   "metadata": {},
   "source": [
    "vai até 4 a 14, pois o abstract tem 5 frases e a conclusao tem 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31854dd3",
   "metadata": {},
   "source": [
    "### Similaridade fasttext cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ddeb858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:11.183991Z",
     "start_time": "2023-03-03T02:38:55.255918Z"
    }
   },
   "outputs": [],
   "source": [
    "#Abtract e Introdução\n",
    "sim_ab_intro_cbow_ft,indices_ab_intro_cbow_ft,lista_idx_ab_intro_cbow_ft =aplicando_dicionario_similaridade(list(v_ft_cbow_abstract.values()),list(v_ft_cbow_introducao.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_cbow_ft,indices_ab_conc_cbow_ft,lista_idx_ab_c_cbow_ft = aplicando_dicionario_similaridade(list(v_ft_cbow_abstract.values()),list(v_ft_cbow_conclusao.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_cbow_ft = frases_interseccao(lista_idx_ab_intro_cbow_ft,lista_idx_ab_c_cbow_ft)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_cbow_ft  = get_frases_objetivo(indices_objetivo_cbow_ft, papers_treino.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36f686e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:11.199750Z",
     "start_time": "2023-03-03T02:39:11.183991Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frase_objetivo_cbow_ft']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(frase_objetivo_cbow_ft,'frase_objetivo_cbow_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c6101d3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T13:48:59.690325Z",
     "start_time": "2023-03-03T13:48:59.476279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['contrary to claims found elsewhere in the literature we argue that a tagger based on markov models performs at least as well as other current approaches including the maximum entropy framework',\n",
       "  'a recent comparison has even shown that tnt performs significantly better for the tested corpora'],\n",
       " 2: ['intersentence similarity is replaced by rank in the local context'],\n",
       " 3: ['this paper presents a corpusbased approach to word sense disambiguation that builds an ensemble of naive bayesian classifiers each of which is based on lexical features that represent cooccurring words in varying sized windows of context'],\n",
       " 4: ['we present a new parser for parsing down to penn treebank style parse trees that achieves average precisionrecall for sentences of and less and for of length and less when trained and tested on the previously established standard sections of the wall street journal treebank',\n",
       "  'this represents a decrease in error rate over the best singleparser results on this corpus'],\n",
       " 5: ['the system was developed and tested using essaylength responses to prompts on the test of english as a foreign language toefl'],\n",
       " 6: [],\n",
       " 7: ['the first two systems called nlg and nlg require a corpus marked only with domainspecific semantic attributes while the last system called nlg requires a corpus marked with both semantic attributes and syntactic dependency information'],\n",
       " 8: ['since a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy as measured against the upenn treebank as a gold standard',\n",
       "  'in this paper we report adapting a lexic al ized probabilistic contextfree parser to information extraction and evaluate this new technique on muc template elements and template relations'],\n",
       " 9: ['a maximum entropy approach to natural lanprocessing'],\n",
       " 10: ['preferences for the target slots are compared using a measure of distributional similarity',\n",
       "  'the method is evaluated on the causative and conative alternations but is generally applicable and does not require a priori knowledge specific to the alternation'],\n",
       " 13: [],\n",
       " 14: ['the rulebased tagger has many advantages over these taggers including a vast reduction in stored information required the perspicuity of a small set of meaningful rules ease of finding and implementing improvements to the tagger and better portability from one tag set corpus genre or language to another',\n",
       "  'perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging',\n",
       "  'the fact that a simple rulebased tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rulebased tagging searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below'],\n",
       " 15: [],\n",
       " 16: ['the conclusions are broadly in agreement with those of merialdo but give greater detail about the contributions of different parts of the model'],\n",
       " 18: ['performance is comparable to or better than the performance of similar systems but we emphasize the simplicity of retraining for new domains'],\n",
       " 19: ['we first describe the older constraint grammar parser where many of the ideas come from'],\n",
       " 20: ['the resulting scheme reflects a stratificational notion of language and makes only minimal assumpabout the interrelation of the particu lar representational strata'],\n",
       " 31: ['we next determine the possible translations from among the candidates using one of the two methods that we have developed'],\n",
       " 32: ['therefore we present a method that makes the system substantially fasterthis approach can also be applied to other similar tasks such as chunking and partofspeech tagging'],\n",
       " 33: ['the graph model is built by linking pairs of words which participate in particular syntacticrelationships',\n",
       "  'we focus on the symmetric relationship between pairs of nouns which occur to gether in lists'],\n",
       " 35: ['evaluating cluster quality has always been a difficult task',\n",
       "  'we present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from wordnet the answer key'],\n",
       " 37: ['this paper presents a machine learning approach toquestion classification',\n",
       "  'we learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types and eventually classifies questions into finegrained classes'],\n",
       " 39: ['this paper presents a deterministic dependency parser based on memorybased learning which parses english text in linear time'],\n",
       " 41: ['this paper describes the role of supertagging in a widecoverage ccg parser which uses a loglinear model to select an analysis',\n",
       "  'we show that large increases in speedcan be obtained by tightly integrating the su pertagger with the ccg grammar and parserthis is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammarwe also further reduce the derivation space us ing constraints on category combination'],\n",
       " 43: ['analysis of pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase'],\n",
       " 45: ['in this paper we introduce a new evaluation method orange for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations'],\n",
       " 47: ['we present a new hmm tagger that exploits context on both sides of a word to be tagged and evaluate it in both the unsupervised and supervised case',\n",
       "  'along the way we present the first comprehensive comparison of unsupervised methods for partofspeech tagging noting that published results to date have not been comparable across corpora or lexicons'],\n",
       " 48: [],\n",
       " 50: ['we present an algorithm designed for the terascale for mining isa relations that achieves similar performance to a stateoftheart linguisticallyrich method',\n",
       "  'we focus on the accuracy of these two systems as a func tion of processing time and corpus size'],\n",
       " 51: ['finally we consider theimpact that this has on one application of distributional similarity methods judging the composition ality of collocations'],\n",
       " 53: ['the system combines a machine learning technique with an inference procedurebased on integer linear programming that supports the incorporation of linguistic and struc tural constraints into the decision process'],\n",
       " 54: ['identifying sentiments the affective parts of opinions is a challenging problem',\n",
       "  'the system contains a module for determining word sentiment and another for combining sentiments within a sentence'],\n",
       " 55: ['we present a new corpus that is suitedto our task and a discriminative treeto tree transduction model that can naturallyaccount for structural and lexical mis matches'],\n",
       " 56: ['in this paper we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native l english writing',\n",
       "  'we show that models of use for these parts of speech can be learned with an accuracy of and respectively on l text and present first results in an error detection task for l writing'],\n",
       " 58: [],\n",
       " 59: ['in addition we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation',\n",
       "  'we present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems'],\n",
       " 60: [],\n",
       " 61: ['we are convinced that the hashkernel and the parallelization can be ap plied successful to other nlp applicationsas well such as transition based depen dency parsers phrase structrue parsers and machine translation',\n",
       "  'in addition to a high accuracy short parsing and training times are the most important properties of a parser'],\n",
       " 62: ['in this paper we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target',\n",
       "  'we propose a treebased simplification model tsm which to our knowledge is the first statistical simplification model covering splitting dropping reorderingand substitution integrally',\n",
       "  'the evaluation shows that our model achieves better readability scores than a set of baseline systems'],\n",
       " 63: ['in our experi ments we show that since our features areable to capture a more abstract representation of tweets our solution is more ef fective than previous ones and also more robust regarding biased and noisy data which is the kind of data provided by these sources'],\n",
       " 64: ['we evaluate the contribution of different feature types for sentiment classification and show that our framework successfully identifies sentiment types of untagged sentences'],\n",
       " 74: [],\n",
       " 88: [],\n",
       " 97: ['in this paper we describe a new model for word alignment in statistical trans lation and present experimental results',\n",
       "  'the idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions'],\n",
       " 99: [],\n",
       " 101: ['we show for the first time that incorporatingthe predictions of a word sense disambigua tion system within a typical phrasebased statistical machine translation smt model consistently improves translation qualityacross all three different iwslt chineseenglish test sets as well as producing sta tistically significant improvements on the larger nist chineseenglish mt task',\n",
       "  'instead of directly incor porating a sensevalstyle wsd system weredefine the wsd task to match the ex act same phrasal translation disambiguation task faced by phrasebased smt systemsour results provide the first known empirical evidence that lexical semantics are in deed useful for smt despite claims to the contrary'],\n",
       " 102: [],\n",
       " 103: ['v measure provides an elegant solution tomany problems that affect previously defined cluster evaluation measures includ ing dependence on clustering algorithm or data set the problem of matching where the clustering of only a portion of datapoints are evaluated and accurate evaluation and combination of two desirable aspects of clustering homogeneity and completeness'],\n",
       " 104: ['in our experiments the resultingrelatedness measure is the wordnetbased measure most highly correlated with human similar ity judgments by rank ordering at'],\n",
       " 105: ['we consider the problem of learning toparse sentences to lambdacalculus repre sentations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar ccg',\n",
       "  'we also present a new online algorithm for inducing a weighted ccg',\n",
       "  'results for the approach on atis data show fmeasure in recovering fully correct semantic analyses and fmeasure by a partialmatch criterion a more than improvement over the partialmatch figure reported by he and young'],\n",
       " 106: ['we present a nonparametric bayesian model of tree structures based on the hierarchical dirichlet process hdp',\n",
       "  'in addition to presenting a fully bayesianmodel for the pcfg we also develop an ef ficient variational inference procedure'],\n",
       " 108: ['moreover this paper evaluates the complementary nature between our tree kernel and a stateoftheart linear kernel',\n",
       "  'evaluation on the ace rdc corpora shows that our dynamic contextsensitive tree span is much more suitable for relation extraction than spt and our tree kernel outperforms the stateoftheart collins and duffys convolution tree kernel'],\n",
       " 110: [],\n",
       " 112: [],\n",
       " 113: ['in this paper we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages',\n",
       "  'in addition we characterize the different approaches of the participating systems report the test results and provide a first analysis of these results'],\n",
       " 114: ['we describe a twostage optimization of the maltparser system for the ten languages in the multilingual track of the conll shared task on dependency parsing',\n",
       "  'the first stage consists in tuning a singleparsersystem for each language by optimizing parameters of the parsing algorithm the fea ture model and the learning algorithm'],\n",
       " 115: ['our experiments show that considering higherorder information yields signifi cant improvements in parsing accuracy but comes at a high cost in terms of both timeand memory consumption'],\n",
       " 116: [],\n",
       " 117: ['a major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translationrulesets',\n",
       "  'wedescribe new lookup algorithms for hierar chical phrasebased translation that reduce the empirical computation time by nearly two orders of magnitude making onthefly lookup feasible for source phrases with gaps'],\n",
       " 118: ['we develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpusand learning the domains in which to consider each word'],\n",
       " 119: ['parser actions are determined by a classifier based on features that represent the current state of the parser',\n",
       "  'in the multilingual track we train three lr models for each of the ten languages and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme'],\n",
       " 121: ['the ihmmbased method significantly outperforms the stateoftheart terbased alignment model in our experiments on nist benchmark datasets',\n",
       "  'our combined smt system using the proposed method achieved the best chinesetoenglish translation result in the constrained training track of the'],\n",
       " 122: ['specifically we attempt to leverage on the resources available for english and by employing machine translation generate resources for subjectivity analysis in other languages',\n",
       "  'through comparative evaluations on two different languages romanian and spanish we show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language'],\n",
       " 123: [],\n",
       " 124: ['this is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text',\n",
       "  'we also establish that readability predictors behave differently depending on the task predicting text readability or ranking the readability'],\n",
       " 125: ['we improve the quality of paraphrases extracted from parallel corpora by requiring that phrases and their paraphrases be the same syntactic type'],\n",
       " 126: ['when combined with our previous work on forestbased decoding it achieves a bleu points improvement over the baseline and even outperforms the hierarchical system of hiero by points'],\n",
       " 127: ['we first show that by parallel processing and exploiting more of the parse forest we can obtain results using mira that match or surpass mert in terms of both translation quality and computational cost',\n",
       "  'we then test the method on two classes of features that address deficiencies in the hiero hierarchical phrasebased model first we simultaneously train a large number of marton and resniks soft syntactic constraints and second we introduce a novel structural distortion model'],\n",
       " 128: ['we investigate five tasks affect recognition word similarity recognizing textual entailment event temporal ordering and word sense disambiguation',\n",
       "  'we propose a technique for bias correction that significantly improves annotation quality on two tasks'],\n",
       " 129: ['this paper describes a rather simple pairwise classification model for coreference resolution developed with a welldesigned set of features'],\n",
       " 130: ['we show that lexical cohesion can be placed in a bayesian context by modeling the words in each topic segment as draws from a multinomial language model associated with the segment maximizing the observation likelihood in such a model yields a lexicallycohesive segmentation'],\n",
       " 131: ['recent papers have given contradictory results when comparing bayesian estimators to expectation maximization em for unsupervised hmm pos tagging and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the hmm',\n",
       "  'in terms of times of convergence we find that variational bayes was the fastest of all the estimators especially on large data sets and that explicit gibbs sampler both pointwise and sentenceblocked were generally faster than their collapsed counterparts on large data sets'],\n",
       " 132: ['by developing a graphbased and a transitionbased dependency parser we show that a beamsearch decoder is a competitive choice for both methods',\n",
       "  'more importantly we propose a beamsearchbased parser that combines both graphbased and transitionbased parsing into a single system for training and decoding showing that it outperforms both the pure graphbased and the pure transitionbased parsers'],\n",
       " 134: ['this is made possible by performing joint inference across mentions in contrast to the pairwise classification typically used in supervised methods and by using markov logic as a representation language which enables us to easily express relations like apposition and predicate nominals'],\n",
       " 136: ['in this paper we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures',\n",
       "  'in experiments we demonstrate that the model when coupled with a discriminative reranking technique achieves stateoftheart performance when tested on two publicly available corpora'],\n",
       " 137: ['in this paper we view such interactions in light of composiand present a novel learningbased approach that incorporates structural inference motivated by compositional semantics into the learning procedure',\n",
       "  'our experiments show that simple heuristics based on compositional semantics can perform better than learningbased methods that do not incorporate compositional semantics accuracy of vs but a method that integrates compositional semantics into learning performs better than all other alternatives'],\n",
       " 138: ['in this paper we present a novel hierarchical phrase reordering model aimed at improvingnonlocal reorderings which seamlessly in tegrates with a standard phrasebased system with little loss of computational efficiency'],\n",
       " 139: ['we show that jointly parsing a bitext can substantially improve parse quality on both sides'],\n",
       " 140: ['we present the first unsupervised approach to the problem of learning a semantic parser using markov logic',\n",
       "  'the map semantic parse of a sentence is obtained by recursively assigning its parts to lambdaform clusters and composing them'],\n",
       " 141: ['under this paradigm we use weights from the expectation semiring eisner to compute firstorder statistics eg the expected hypothesis length or feature counts over packed forests of translations lattices or hypergraphs',\n",
       "  'we then introduce novel semiring which computes secondorder statistics eg the variance of the hypothesis length or the gradient of entropy'],\n",
       " 142: ['a significant portion of the worlds text is tagged by readers on social bookmarkwebsites attribution an inherent problem in these corpora because most pages have multiple tags but the tags do not always apply with equal specificity across the whole document'],\n",
       " 143: ['we find that when combined nonexpert judgments have a highlevel of agreement with the existing goldstandard judgments of machine translation quality and correlate more strongly with expert judgments than bleu does'],\n",
       " 144: ['we describe an extension of semisupervised structured conditional models ssscms to the dependency parsing problem whose framework is originally proposed in suzuki and isozaki'],\n",
       " 146: ['meanwhile massive collections of interlinked documents in dozens of languages such as wikipedia are now widely available calling for tools that can characterize content in many languages',\n",
       "  'we explore the models characteristics using two large corpora each with over ten different languages and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages'],\n",
       " 147: ['we propose a highly scalable implementation based on distributional similarity implemented in the mapreduce framework and deployed over a billion word crawl of the web',\n",
       "  'the pairwise similarity between million terms is computed in hours using quadcore nodes'],\n",
       " 148: [],\n",
       " 149: ['primary contributions include the presentation of a simpletoreproduce highperforming baseline and the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon and perhaps best addressed by noncoreference systems'],\n",
       " 150: [],\n",
       " 151: ['by taking advantage of the observation that a lot of product features are phrases a concept of phrase dependency parsing is introduced which extends traditional dependency parsing to phrase level',\n",
       "  'experimental evaluations show that the mining task can benefit from phrase dependency parsing'],\n",
       " 152: [],\n",
       " 153: ['this extends previous work on discriminative weighting by using a finer granularity focusing on the properties of instances rather than corpus components and using a simpler training procedure'],\n",
       " 154: ['this cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time'],\n",
       " 155: ['our model significantly outperforms the rivals on the task of reconstructing an vectors not seen in training',\n",
       "  'we show moreover that our approach provides two novel ways to represent adjective meanings alternative to its representation via corpusbased cooccurrence vectors both outperforming the latter in an adjective clustering task'],\n",
       " 156: ['we use higherorder unification to define a hypothesis space containing all grammars consistent with the training data and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a loglinear parsing model'],\n",
       " 157: ['during inference of the probabilistic model we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules'],\n",
       " 158: ['in this paper we present a multilevel generative model that reasons jointly about latent topics and geographical regions',\n",
       "  'applied to a new dataset of geotagged microblogs our model recovers coherent topics and their regional variants while identifying geographic areas of linguistic consistency'],\n",
       " 159: ['this paper introduces algorithms for nonparsing based on decomposi we focus on parsing algorithms for nonhead a generalization of headautomata models to nonprojective structures'],\n",
       " 160: ['the projected parsers from our system result in stateoftheart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages'],\n",
       " 161: ['in sentiment prediction tasks these representations outperform other stateoftheart approaches on commonly used datasets such as movie reviews without using any predefined sentiment lexica or polarity shifting rules',\n",
       "  'our algorithm can more accurately predict distributions over such labels compared to several competitive baselines'],\n",
       " 162: ['we explore efficient domain adaptation for the task of statistical machine translation based on extracting sentences from a large generaldomain parallel corpus that are most relevant to the target domain',\n",
       "  'as these sentences are not themselves identical the indomain data we call them these subcorpora the size of the original can then used to train small domainadapted statistical machine translation smt systems which outperform systems trained on the entire corpus'],\n",
       " 163: ['in line with recent works emphasizing the need of largescale annotation efforts for textual entailment our work aims to the scarcity of data available to train evaluate systems and the recourse to crowdsourcing as an effective way to reduce the costs of data collection without sacrificing quality'],\n",
       " 164: ['unlike the popular mert algorithm och our pairwise ranking optimization pro method is not limited to a handful of parameters and can easily handle systems with thousands of features',\n",
       "  'we establish pros scalability and effectiveness by comparing it to mert and mira and demonstrate parity on both phrasebased and syntaxbased systems in a variety of language pairs using large scale data scenarios'],\n",
       " 166: ['our novel doubles compared with the ner system the redundancy inherent in tweets to achieve this performance using labeledlda to exploit freebase dictionaries as a source of distant supervision'],\n",
       " 167: [],\n",
       " 168: ['in this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods'],\n",
       " 169: ['we present a transitionbased system for joint partofspeech tagging and labeled dependency parsing with nonprojective trees',\n",
       "  'experimental evaluation on chinese czech english and german shows consistent improvements in both tagging and parsing accuracy when compared to a pipeline system which lead to improved stateoftheart results for all languages'],\n",
       " 170: ['two apparently opposing dop models exist in the literature one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank',\n",
       "  'together with a pcfgreduction of dop we obtain improved accuracy and efficiency on the wall street journal treebank our results show an relative reduction in error rate over previous models and an average processing time of seconds per wsj sentence'],\n",
       " 171: ['in addition we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material'],\n",
       " 172: ['we show how the use of morphological information can improve the performance on rare words and that this is robust across a wide range of languages'],\n",
       " 173: ['this paper investigates two elements of maximum entropy tagging the use of a correction feature in the generalised iterative scaling gis estimation algorithm and techniques for model smoothing'],\n",
       " 174: ['we introduce methods to learn splitting rules from monolingual and parallel corpora',\n",
       "  'we evaluate them against a gold standard and measure their impact on performance of statistical mt systems'],\n",
       " 175: ['a disambiguation svm kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia',\n",
       "  'the resultingmodel significantly outperforms a less in formed baseline'],\n",
       " 176: ['this paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation mt systems',\n",
       "  'the context of a whole document of translations rather than a single sentence is taken into account to produce the alignment'],\n",
       " 177: ['we show that those extensions can make the mst framework computationally intractable but that the intractability can be circumvented with new approximate parsing algorithms',\n",
       "  'we conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for czech and danish'],\n",
       " 178: ['in this paper we show that tree kernels are very helpful in the processing of natural language as a we provide a simple algorithm to compute tree kernels in linear average running time and b our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods'],\n",
       " 179: [],\n",
       " 180: ['we demonstrate that net overlap score can be used as ameasure of the words degree of member ship in the fuzzy category of sentimentthe core adjectives which had the high est net overlap scores were identifiedmost accurately both by step and by hu man annotators while the words on the periphery of the category had the lowest scores and were associated with low rates of interannotator agreement'],\n",
       " 181: ['our measure can be exactly calculated in quadratic time',\n",
       "  'furthermore we will show how some evaluation measures can be improved'],\n",
       " 182: ['we argue that the machine translation community is overly reliant on the bleu machine translation evaluation metric',\n",
       "  'we show that an improved bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality and give two significant counterexamples to bleus correlation with human judgments of quality'],\n",
       " 183: ['we argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly',\n",
       "  'this differs from current stateoftheart models knight and marcu that treat noisy parse trees for both compressed and uncompressed sentences as gold standard when calculating model parameters'],\n",
       " 184: [],\n",
       " 185: ['in this paper we present trofi trope finder a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised wordsense disambiguation and clustering techniques',\n",
       "  'using the trofi algorithm we also build the trofi example base an extensible resource of annotated literalnonliteral examples which is freely available to the nlp research community'],\n",
       " 186: ['we investigate the lexical and syntactic flexibility of a class of idiomatic expressions',\n",
       "  'we also propose a means for automatically determining which syntactic forms a particular idiom can appear in and hence should be included in its lexical representation'],\n",
       " 187: ['we performed experiments on extracting gene and protein interactions from two different data sets',\n",
       "  'the results show that our approach outperforms most of the previous methods based on syntactic and semantic information'],\n",
       " 188: ['our algorithm uses the full graph of the lkb efficiently performing better than previous approaches in english allwords datasets',\n",
       "  'we also show that the algorithm can be easily ported to other languages with good results with the only requirement of having a wordnet'],\n",
       " 189: ['our work places sense induction in a bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words'],\n",
       " 191: ['the principal intended area of application is the representation of lexical entries for natural language processing and we use examples from this domain throughout',\n",
       "  'the goal of the is the design of a simple language that i has the necessary expressive power to encode the lexical entries presupposed by contemporary work in the unification grammar tradition ii can express all the evident generalizations about such entries iii has an explicit theory of inference iv is computationally tractable and v has an explicit declarative semantics'],\n",
       " 192: ['we sketch and illustrate an approach to machine translation that exploits the potential of simultaneous correspondences between separate levels of linguistic representation as in the of codescriptions'],\n",
       " 193: ['we report on the systems performance with gazetteers of different types and different sizes using test material from the muc competition',\n",
       "  'we show that for the text type and task of this competition it is sufficient to use relatively small gazetteers of wellknown names rather than large gazetteers of lowfrequency names'],\n",
       " 194: [],\n",
       " 198: ['even with only labeled train ing examples and simple features whichincorporate counts from a large unlabeled corpus we achieve aer perfor mance close to ibm model in muchless time',\n",
       "  'including model predic tions as features we achieve a relativeaer reduction of in over inter sected model alignments'],\n",
       " 200: ['this paper presents a maximum entropyword alignment algorithm for arabic english based on supervised training datawe demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of su pervised and unsupervised methods yields superior performance',\n",
       "  'the probabilisticmodel used in the alignment directly models the link decisions'],\n",
       " 202: ['opines novel use ofrelaxation labeling for finding the semantic orientation of words in con text leads to strong performance on the tasks of finding opinion phrases and their polarity'],\n",
       " 203: ['this paper presents a new approach to phraselevel sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions',\n",
       "  'with thisapproach the system is able to automat ically identify the contextual polarity for a large subset of sentiment expressionsachieving results that are significantly bet ter than baseline'],\n",
       " 204: ['we pursue another aspect of opinion analysis identi fying the sources of opinions emotions and sentiments'],\n",
       " 205: ['in this paper we describe the construction of three sense annotated corpora in different domains for a sample of english wordswe apply an existing method for acquiring predominant sense information automatically from raw text and for our sam ple demonstrate that acquiring suchinformation automatically from a mixeddomain corpus is more accurate than de riving it from semcor and acquiringit automatically from text in the same do main as the target domain performs best by a large margin',\n",
       "  'we also show that for an all words wsd task this automatic method is best focussed on words that are salient to the domain and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus'],\n",
       " 206: ['the algorithm can enumerate all possible decomposition structures andfind the highest probability sequence together with the corresponding decomposi tion structure in polynomial time',\n",
       "  'we also present an efficient decoding algorithm based on the easiestfirst strategy which gives comparably good performance tofull bidirectional inference with significantly lower computational cost',\n",
       "  'exper imental results of partofspeech tagging and text chunking show that the proposedbidirectional inference methods consis tently outperform unidirectional inference methods and bidirectional memms give comparable performance to that achievedby stateoftheart learning algorithms in cluding kernel support vector machines'],\n",
       " 207: ['using this representation the parsing algorithmof eisner is sufficient for search ing over all projective trees in on time'],\n",
       " 208: ['the goal is to classify the emotional affinity of sentences in the narra tive domain of childrens fairy tales forsubsequent usage in appropriate expressive rendering of texttospeech synthe sis',\n",
       "  'in addition we present plans for a more cognitively soundsequential model taking into considera tion a larger set of basic emotions'],\n",
       " 209: ['finally we use machine learning to combine these deep semantic analysis techniques with simpleshallow word overlap the resulting hy brid model achieves high accuracy on the rte testset given the state of the art',\n",
       "  'ourresults also show that the different techniques that we employ perform very dif ferently on some of the subsets of the rte corpus and as a result it is useful to use the nature of the dataset as a feature'],\n",
       " 210: ['we present a novel approach to relation extraction based on the observation thatthe information required to assert a rela tionship between two named entities in the same sentence is typically capturedby the shortest path between the two entities in the dependency graph'],\n",
       " 222: ['as the focus of information extraction is shifting from nominal information such as named entity to verbal information such as function and interaction of substances applica tion of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sen tences is in demand',\n",
       "  'interannotator agreement test indicated that the writ ing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation and that annotation can be stably done by linguists without much knowledge of bi ology with appropriate guidelines regarding to linguistic phenomena par ticular to scientific texts'],\n",
       " 223: ['the second international chinese word segmentation bakeoff was held in the summer of to evaluate the current state of the art in word segmentationtwenty three groups submitted result sets over two tracks and four different corpora',\n",
       "  'we found that the technol ogy has improved over the intervening two years though the outofvocabularyproblem is still or paramount impor tance'],\n",
       " 225: ['our segmenter was built using a condi tional random field sequence model that provides a framework to use a large number of linguistic features such as character identity morphological and character reduplication features',\n",
       "  'our final system achieved a fscore of as hk pk and msr'],\n",
       " 226: ['we use a decision tree approach inspired by contextual spelling systems for detection and correction suggestions and a large language model trained on the gigaword corpus to provide additional information to filter out spurious suggestions'],\n",
       " 228: ['my goal here is only to demonstrate the value of some previously unused kinds of information that are always available for translation modeling and to show how these information sources can be integrated with others'],\n",
       " 229: ['youre a so youre a senior now'],\n",
       " 231: ['the design of the system is based on the results of a corpus analysis previously reported which highlighted the prevalence of discoursenew descriptions in newspaper corpora'],\n",
       " 232: ['the paper discusses a number of problems with these annotations and concludes that rethinking of the coreference task is needed before the task is expanded',\n",
       "  'in particular it suggests a division of labor whereby annotation of the coreference relation proper is separated from other tasks such as annotation of bound anaphora and of the relation between a subject and a predicative np'],\n",
       " 234: ['we do this by means of experiments involving the task of morphosyntactic word class tagging on the basis of three different tagged corpora'],\n",
       " 235: ['this paper describes the functioning of a broadcoverage probabilistic topdown parser and its application to the problem of language modeling for speech recognition',\n",
       "  'a lexicalized probabilistic topdown parser is then presented which performs very well in terms of both the accuracy of returned parses and the efficiency with which they are found relative to the best broadcoverage statistical parsers'],\n",
       " 243: ['edinburgh in this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article material for summaries is selected in such a way that summaries can highlight the new contribution of the source article and situate it with respect to earlier work'],\n",
       " 244: [],\n",
       " 245: ['this article describes a new approach to the generation of referring expressions',\n",
       "  'the current approach has four main advantages graph structures have been studied extensively and by moving to a graph perspective we get direct access to the many theories and algorithms for dealing with graphs many existing generation algorithms can be reformulated in terms of graphs and this enhances comparison and integration of the various approaches the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions and the combined use of graphs and cost functions paves the way for an integration of rulebased generation techniques with more recent stochastic approaches'],\n",
       " 246: [],\n",
       " 248: ['in this article we report on our work using the strand system for mining parallel text on the world wide webfirst reviewing the original algorithm and results and then presenting a set of significant enhancements',\n",
       "  'these enhancements include the use of supervised learning based on structural features of documents to improve classification performance a new contentbased measure of translational equivalence and adaptation of the system to take advantage of the internet archive for mining parallel text from the web on a large scale',\n",
       "  'finally the value of these techniques is demonstrated in the construction of a significant parallel corpus for a lowdensity language pair'],\n",
       " 258: [],\n",
       " 263: ['lra extends the vsm approach in three ways the patterns are derived automatically from the corpus the singular value decomposition svd is used to smooth the frequency data and automatically generated synonyms are used to explore variations of the word pairs'],\n",
       " 272: ['it exposes the mathematics and underlying assumptions of agreement coefficients covering krippendorffs alpha as well as scotts pi and cohens kappa discusses the use of coefficients in several annotation tasks and argues that weighted alphalike coefficients traditionally less used than kappalike measures in computational linguistics may be more appropriate for many corpus annotation tasksbut that their use makes the interpretation of the value of the coefficient even harder'],\n",
       " 276: ['we propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others infer goals being sought and cooperate in their achievement'],\n",
       " 277: ['the extended formalism makes it easy to describe left extraposition of constituents an important feature of natural language syntax'],\n",
       " 279: [],\n",
       " 280: ['also a commercial online parser for japanese language is being built by intelligent technology incorporation on the technique developed at',\n",
       "  'in several experiments with different english grammars and sentences timings indicate a fiveto tenfold speed advantage over earleys contextfree parsing algorithm algorithm parses a sentence strictly from left to right that is starts parsing as soon as the user types in the first word of a sentence without waiting for completion of the sentence'],\n",
       " 281: ['as a result many systems for representing the semantics of sentences have ignored scoping or generated scopings with mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow',\n",
       "  'the algorithm is not profligate as are those based on permutation of quantifiers and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy'],\n",
       " 282: [],\n",
       " 283: ['a semantics of temporal categories in language and a theory of their use in defining the temporal relations between events both require a more complex structure on the domain underlying the meaning representations than is commonly assumed'],\n",
       " 284: ['they specify entities in an evolving model of the discourse that the listener is constructing'],\n",
       " 286: ['the enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semanticheaddriven fashion'],\n",
       " 296: [],\n",
       " 307: ['this model is associated with features identifiable from text analysis including orthography and part of speech to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech phrases and phrases that directly signal the structure of a discourse been variously termed words discourse markers discourse connectives particles the computational linguistic and conversational analysis',\n",
       "  'although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse the question of how speakers and hearers accomplish this disambiguation is rarely addressed'],\n",
       " 310: ['this paper presents a set of mathematical and computational tools for manipulating and reasoning about regular languages and regular relations and argues that they provide a solid basis for computational phonology'],\n",
       " 313: [],\n",
       " 315: [],\n",
       " 316: ['the original motivations for centering the basic definitions underlying the centering framework and the original theoretical claims',\n",
       "  'we have also greatly shortened the discussion of criteria for and constraints on a possible semantic theory as a foundation for this work'],\n",
       " 318: [],\n",
       " 323: ['the algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of texts'],\n",
       " 326: ['technology introduce a novel inversion transduction formalism bilingual modeling of sentencepairs and the concept of parsing a variety of parallel corpus analysis applications',\n",
       "  'aside from the bilingual orientation three major features distinguish the formalism from the finitestate transducers more traditionally found in computational linguistics it skips directly to a contextfree rather than finitestate base it permits a minimal extra degree of ordering flexibility and its probabilistic formulation admits an efficient maximumlikelihood bilingual parsing algorithm',\n",
       "  'we discuss a number of examples of how stochastic inversion transduction grammars bring bilingual constraints to bear upon problematic corpus analysis tasks such as segmentation bracketing phrasal alignment and parsing'],\n",
       " 338: ['in this paper we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques'],\n",
       " 339: ['considering empirical evidence from a freewordorder language german we propose a revision of the principles guiding the ordering of discourse entities in the forwardlooking center list within the centering model',\n",
       "  'in the first one we compare success rates for the resolution of pronominal anaphora that result from a grammaticalroledriven centering algorithm and from a functional centering algorithm'],\n",
       " 342: [],\n",
       " 345: ['in this paper we present a novel and realistic method for speeding up the training time of a transformationbased learner without sacrificing performance'],\n",
       " 347: ['this paper presents a corpusbased approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby'],\n",
       " 348: ['we present a simple architecture for parsing transcribed speech in which an editedword detector first removes such words from the sentence string and then a standard statistical parser trained on transcribed speech parses the remaining words',\n",
       "  'the edit detector achieves a misclassification rate on edited words of which marks everything as not edited has an error rate of',\n",
       "  'to evaluate our parsing results we introduce a new evaluation metric the purpose of which is to make evaluation of a parse tree relatively indifferent the exact tree position of by this metric the parser achieves precision and recall'],\n",
       " 350: ['this report considers a definition of cognitive load in terms of the total probability of structural options that have been disconfirmed at point in a sentence the surprisal of word its prefix on a phrasestructural language model'],\n",
       " 351: ['the algorithm takes as input a small corpus sentences annotated with parse trees a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text',\n",
       "  'the algorithm iteratively labels the entire data set with parse trees'],\n",
       " 352: ['we propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input'],\n",
       " 355: ['our apapplies alignment sentences gathered from unannotated comparable corpora it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences'],\n",
       " 356: ['we present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical leftcorner parser',\n",
       "  'the resulting statistical parser achieves performance fmeasure on the penn treebank which is only below the best current parser for this task despite using a smaller vocabulary size and less prior linguistic knowledge',\n",
       "  'crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history and no use of hard independence assumptions'],\n",
       " 357: ['on averagelength penn treebank sentences our most detailed estimate reduces the total number of edges processed to less than of that required by exhaustive parsing and a simpler estimate which requires less than a minute of precomputation reduces the work to less than'],\n",
       " 358: ['we propose a new phrasebased translation model and decoding algorithm that enables us to evaluate and compare several previously proposed phrasebased translation models',\n",
       "  'our empirical results which hold for all examined language pairs suggest that the highest levels of performance can be obtained through relatively simple means heuristic learning of phrase translations from wordbased alignments and lexical weighting of phrase translations'],\n",
       " 359: ['the results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations based on various statistical metrics while direct application of the bleu evaluation procedure does not always give good results'],\n",
       " 362: ['our fsas can also predict the correctness of alternative semantic renderings which may be used to evaluate the quality of translations'],\n",
       " 363: ['we present an application of ambiguity packing and stochastic disambiguation techniques for lexicalfunctional grammars lfg to the domain of sentence condensation',\n",
       "  'overall summarization quality of the proposed system is stateoftheart with guaranteed grammaticality of the system output due to the use of a constraintbased parsergenerator'],\n",
       " 364: ['we show here how to train a conditional random field to achieve performance as good as any reported base nounphrase chunking method on the conll task and better than any reported single model',\n",
       "  'we present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximumentropy models'],\n",
       " 365: ['a set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches nearhuman levels of performance'],\n",
       " 366: ['using these ideas together the resulting tagger gives a accuracy on the penn treebank wsj an error reduction of on the best previous single automatically learned tagging result'],\n",
       " 369: ['in this paper we present a statistical languageindependent framework for identifying and tracking named nominal and pronominal references to entities within unrestricted text documents and chaining them into clusters corresponding to each logical entity present in the text',\n",
       "  'the proposed framework is evaluated with several experiments run in arabic chinese and english texts a system based on the approach described here and submitted to the latest automatic content extraction ace evaluation achieved toptier results in all three evaluation languages'],\n",
       " 370: ['this paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems',\n",
       "  'we measured the accuracy of both systems against a gold standard of the parc dependency bank and also measured their processing times'],\n",
       " 371: ['the theory of tree transducer automata provides a possible framework to draw on as it has been worked out in an extensive literature',\n",
       "  'we motivate the use of tree transducers for natural language and address the training problem for probabilistic treetotree and treetostring transducers'],\n",
       " 373: ['the present paper investigates if these results generalize to tasks covering both syntax and semantics both generation and analysis and a larger of for the majority of tasks we find that simple unsupervised models perform when frequencies are obtained from the web rather than from a large corpus'],\n",
       " 374: ['we argue that it is reliable predictive and diagnostic thus improves considerably over the shortcomings of the human evaluation method currently used in the document understanding conference'],\n",
       " 375: ['feature values were combined in a loglinear model to select the highest scoring candidate from an list'],\n",
       " 376: ['we describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings wordtoword alignments from an mt system and syntactic structure from parsetrees of source and target language sentences'],\n",
       " 377: ['we provide experimental results on the nist chineseenglish large data track evaluation'],\n",
       " 378: [],\n",
       " 379: ['we show perfor mance improvements through a number of newfeatures and measure their ability to general ize to a new test set drawn from the aquaint corpus'],\n",
       " 380: [],\n",
       " 381: ['we use our theory to introduce a linear algorithm that can be used to derive from wordaligned parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data'],\n",
       " 382: ['systems that automatically discover semantic classes have emerged in part to address the limitations of broadcoverage lexical resources such as wordnet and cyc',\n",
       "  'we propose an algorithm labeling semantic and for leveraging them to extract relationships using a topdown approach'],\n",
       " 383: ['the basic theory of crfs is becoming wellunderstood but bestpractices for applying them to realworld data requires additional exploration'],\n",
       " 385: ['it provides six measures of similarity and three measures of relatedness all of which are based on the lexical database wordnet'],\n",
       " 389: ['for a training corpus with sentence pairs we increase the coverage of unique test set unigrams from to with more than half of the newly covered items accurately translated as opposed to none in current approaches'],\n",
       " 390: ['current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text using a locally decomposable matching score',\n",
       "  'we report results on data from the pascal rte challenge which surpass previously reported results for alignmentbased systems'],\n",
       " 391: ['most current approaches employ machine learning techniques and require supervised data',\n",
       "  'the algorithm makes use of a new frequency based metric for time distributions and a resource free discriminative approach to transliteration'],\n",
       " 392: [],\n",
       " 393: ['we show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker improved model achieves an of an absolute improvement error reduction over the previous best result for wall street journal parsing',\n",
       "  'finally we provide some analysis to better understand the phenomenon'],\n",
       " 394: ['in this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources',\n",
       "  'we show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns'],\n",
       " 395: ['we devise a lineartime algorithm for factoring syntactic reorderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a stateoftheart syntaxbased machine translation system'],\n",
       " 397: ['for example we can achieve an english partofspeech tagging accuracy of using only three examples of each tag and no dictionary constraints'],\n",
       " 398: ['we present a novel statistical approach to parsing for constructing a complete formal meaning representation of a sentence',\n",
       "  'a word alignment model is used for lexical acquisition and the parsing model itself can be seen as a syntaxbased translation model show that favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision and shows better robustness to variations in task complexity and word order'],\n",
       " 399: ['we also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation'],\n",
       " 400: ['in this paper we study the effect of different wordlevel preprocessing decisions for arabic on smt quality',\n",
       "  'moreover choosing the appropriate preprocessing produces a significant increase in bleu score if there is a change in genre between training and test data'],\n",
       " 403: ['traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases',\n",
       "  'we outline a set of approximations that make this approach practical and apply our method to the ace coreference dataset achieving a error reduction over a comparable method that only considers features of pairs of noun phrases'],\n",
       " 404: ['we illus trate these methods by estimating a sparse grammar describing the morphology ofthe bantu language sesotho demonstrat ing that with suitable priors bayesian techniques can infer linguistic structure in situations where maximum likelihoodmethods such as the insideoutside algo rithm only produce a trivial grammar'],\n",
       " 405: ['we present a sentence compression system based on synchronous contextfree grammars scfg following the successful noisychannel approach of knight and marcu',\n",
       "  'finally we evaluate different markovized models and find that our selected best model is one that exploits headmodifier bilexicalization to accurately distinguish adjuncts from complements and that produces sentences that were judged more grammatical than those generated by previous work'],\n",
       " 406: ['this paper describes three different approaches to mt system combination',\n",
       "  'the wordlevel combination provides the most robust gains but the best results on the development test sets nist mt and the newsgroup portion of gale dryrun were achieved by combining all three methods'],\n",
       " 407: ['in this paper we propose an integer linear programming ilp formulation for coreference resolution which models anaphoricity and coreference as a joint task such that each local model informs the other for the final assignments joint ilp formulation provides score improvements of over a base coreference classifier on the ace datasets'],\n",
       " 408: [],\n",
       " 409: ['we present a novel technique of training with manytomany alignments'],\n",
       " 410: [],\n",
       " 411: ['however existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering',\n",
       "  'this papresents a collection of methods for automatically learning admissible argument values to which an inference rule be applied which we call and methods for filtering out incorrect inferences'],\n",
       " 413: ['each of our methods independently provide the best results in their class on the rg and wordsim datasets and a supervised combination of them yields the best published results on all datasets'],\n",
       " 414: ['this family extends the partitioned logistic normal distribution enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar providing a new way to encode prior knowledge about an unknown grammar',\n",
       "  'we then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a nonparallel multilingual corpus'],\n",
       " 415: ['unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts',\n",
       "  'in this paper we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing',\n",
       "  'our model produces stateoftheart results on the task of unsupervised grammar induction improving over the best previous work by almost percentage points'],\n",
       " 416: ['we use the margin infused relaxed algorithm of crammer et al to add a large number of new features to two machine translation systems the hiero hierarchical phrasebased translation system and our syntaxbased translation system',\n",
       "  'we analyze the impact of the new features and the performance of the learning algorithm'],\n",
       " 417: ['we introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems',\n",
       "  'for a set of five subjectobjectverb sov order languages we show significant improvements in bleu scores when translating from english compared to other reordering approaches in stateoftheart phrasebased smt systems'],\n",
       " 418: ['one of the reasons nonparametric bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities',\n",
       "  'this paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task',\n",
       "  'with appropriate adaptor grammars and inference procedures we achieve an word token fscore on the standard brent version of the bernstein ratner corpus which is an error reduction of over over the best previously reported results for this corpus'],\n",
       " 419: ['this easily results in inconsistent annotations which are harmful to the performance of the aggregate system'],\n",
       " 420: [],\n",
       " 421: ['in this work we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding'],\n",
       " 422: ['the first pass is performed using a conventional phrasebased smt model',\n",
       "  'evaluation on a wall street journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in bleu score over a strong pure phrasebased smt baseline to our knowledge the first successful application of semantic role labeling to smt'],\n",
       " 423: ['this paper presents a method that uses clustering to produce multiple sensespecific vectors for each word',\n",
       "  'experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vectorspace models'],\n",
       " 424: [],\n",
       " 425: ['we propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain'],\n",
       " 427: ['we present a generative modelbased approach in which each of these factors is modularly encapsulated and learned in a primarily unsupervised manner',\n",
       "  'by sharing lexical statistics at the level of abstract entity types our model is able to substantially reduce semantic compatibility errors resulting in the best results to date on the complete endtoend coreference task'],\n",
       " 428: ['we advance the state of the art in parallel sentence extraction by modeling the document level alignment motivated by the observation that parallel sentence pairs are often found in close proximity'],\n",
       " 430: [],\n",
       " 431: ['we analyze a number of these algorithms in terms of their sentencelevel loss functions which motivates several new approaches including a structured svm',\n",
       "  'among other results we find that a simple and efficient batch version of mira performs at least as well as training online and consistently outperforms other options'],\n",
       " 432: ['second and more interestingly we provide an algorithm for inducing crosslingual clusters and we show that features derived from these clusters significantly improve the accuracy of crosslingual structure prediction',\n",
       "  'specifically we show that by augmenting directtransfer systems with crosslingual cluster features the relative error of delexicalized dependency parsers trained on english treebanks and transferred to foreign languages can be reduced by up to'],\n",
       " 433: ['the core of method which we call is an algorithm for efficiently computing the sequence of phraselevel edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard annotation',\n",
       "  'this optimal edit seis subsequently scored using mea we test our on the helping our own hoo shared task data and show that our method results in more accurate evaluation for grammatical error correction'],\n",
       " 434: ['additionally we contribute the first pos annotation guidelines for such text and release a new dataset of english language tweets annotated using these guidelines',\n",
       "  'we systematically evaluate the use of largescale unsupervised word clustering and new lexical features to improve tagging accuracy'],\n",
       " 435: ['we find that these representations are surprisingly good at capturing syntactic and semantic regularities in language and that each relationship is characterized by a relationspecific vector offset',\n",
       "  'for example the malefemale relationship is automatically learned and with the induced vector representations king man woman results in a vector very close to queen we demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions provided with this paper and are able to correctly answer almost of the questions'],\n",
       " 436: ['we introduce an annotation scheme for temporal expressions and describe a method for resolving temporal expressions in print and broadcast news',\n",
       "  'the system which is based on both handcrafted and machinelearnt rules achieves an accuracy fmeasure against handannotated data'],\n",
       " 437: ['results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than handcrafted rule writing at a comparable level of human labor investment'],\n",
       " 439: ['the noisy channel model has been applied to a wide range of problems including spelling correction',\n",
       "  'these models consist of two components a source model and a channel model'],\n",
       " 440: ['this paper presents results on experiments using this approach in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus'],\n",
       " 442: ['we discuss the advantages of lexicalized treeadjoining grammar as an alternative to lexicalized pcfg for statistical parsing describing the induction of a probabilistic ltag model from the penn treebank and evaluating its parsing performance',\n",
       "  'we find that this induction method is an improvement over the embased method of hwa and that the induced model yields results comparable to lexicalized pcfg'],\n",
       " 443: [],\n",
       " 445: ['in this paper we evaluate the performance of different learning methods on a prototypical natural language disambiguation task confusion set disambiguation when trained on orders of magnitude more labeled data than has previously been used'],\n",
       " 446: ['we present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple english translations of the same source text'],\n",
       " 448: ['we develop a framework for formalizing semantic construction within grammars expressed in typed feature struclogics including the approach provides an alternative to the lambda calculus it maintains much of the desirable flexibility of unificationbased approaches to composition while constraining the allowable operations in order to capture basic generalizations and improve maintainability'],\n",
       " 449: ['in our approach we compare the entire list of candidates sorted according to the particular measures to a reference set of manually identified true positives'],\n",
       " 450: ['in this paper we compare the speed and output quality of a traditional stackbased decoding algorithm with two new decoders a fast greedy decoder and a slow but optimal decoder that treats decoding as an integerprogramming optimization problem'],\n",
       " 451: ['we propose a statistical method that finds the maximumprobability segmentation of a given text',\n",
       "  'this method does not require training data because it estimates probabilities from the given text'],\n",
       " 452: ['our model transforms a sourcelanguage parse tree into a targetlanguage string by applying stochastic operations at each node'],\n",
       " 454: [],\n",
       " 455: ['we present a noun phrase coreference system that extends the work of soon et al and to our knowledge produces the best results to date on the muc and muc coreference resolution data sets fmeasures of and respectively',\n",
       "  'improvements arise from two sources extralinguistic changes to the learning framework and a largescale expansion of the feature set to include more sophisticated linguistic knowledge'],\n",
       " 456: ['we present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts'],\n",
       " 457: ['this paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their coindexed antecedents in phrase structure trees that do not contain this information',\n",
       "  'this paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus'],\n",
       " 458: ['this paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction',\n",
       "  'by modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction'],\n",
       " 460: [],\n",
       " 464: ['we present a framework for statistical machine translation of natural languages based on direct maximum entropy models which contains the widely used sourcechannel approach as a special case',\n",
       "  'this approach allows a baseline machine translation system to be extended easily by adding new feature functions'],\n",
       " 465: ['as the model size becomes huge in a practical setting and the decoder considers multiple syntactic structures for each word alignment several pruning techniques are necessary'],\n",
       " 466: ['we present this method as an automated understudy to skilled human judges which substitutes for them when there is for quick or frequent'],\n",
       " 467: ['the parser differs from most existing widecoverage treebank parsers in capturing the longrange dependencies inherent in constructions such as coordination extraction raising and control as well as the standard local predicateargument dependencies',\n",
       "  'a set of dependency structures used for training and testing the parser is obtained from a treebank of ccg normalform derivations which have been derived semi automatically from the penn treebank'],\n",
       " 470: ['we show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as even when the relations are not explicitly marked by cue phrases'],\n",
       " 472: ['we also compare our results with the results obtained from human translations and a commercial system for the same task'],\n",
       " 473: ['this paper presents a simple unsupervised learning algorithm for classifying reviews up or recdown'],\n",
       " 474: ['through the hmm our system is able to apply and integrate four types of internal and external evidences simple deterministic internal feature of the words such as capitalization and digitalization internal semantic feature of important triggers internal gazetteer feature external macro context feature',\n",
       "  'it shows that the performance is significantly better than reported by any other machinelearning system'],\n",
       " 475: [],\n",
       " 476: ['we present an alternative strategy in which patterns are used to extract highly precise relational information offline creating a data repository that is used to efficiently answer questions'],\n",
       " 477: ['in this paper we present a novel customizable ie paradigm that takes advantage of predicateargument structures',\n",
       "  'we also introduce a new way of automatically identifying predicate argument structures which is central to our ie paradigm',\n",
       "  'it is based on an extended set of features and inductive decision tree learning'],\n",
       " 478: ['we introduce a probabilistic noisychannel model for question answering and we show how it can be exploited in the context of an endtoend qa system',\n",
       "  'we also show that the model we propose is flexible enough to accommodate within one mathematical framework many qaspecific resources and techniques which range from the exploitation of wordnet structured and semistructured databases to reasoning and paraphrasing'],\n",
       " 479: ['in this paper we extend mining to convert a kernelbased classifier into a simple and fast linear classifier'],\n",
       " 480: ['a novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters offering us a good insight into the potential and limitations of semantically classifying'],\n",
       " 481: ['to remove these we propose two measures scores that evaluate the validity of alignments',\n",
       "  'the measure for article alignment uses similarities in sentences aligned by dp matching and that for sentence alignment uses similarities in articles aligned by clir',\n",
       "  'using these measures we have successfully constructed a largescale article and sentence alignment corpus available to the public'],\n",
       " 482: ['we augment a model of translation based on reordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure while keeping computational complexity polynomial in the sentence length'],\n",
       " 483: ['we present a statistical model for computing the probability of an alignment given a sentence pair',\n",
       "  'this model allows easy integration of contextspecific features',\n",
       "  'our experiments show that this model can be an effective tool for improving an existing word alignment'],\n",
       " 484: ['we present a probabilistic parsing model for german trained on the negra treebank',\n",
       "  'this model outperforms the baseline achieving a labeled precision and recall of up to'],\n",
       " 485: ['the experiments will show that the baseline itg constraints are not sufficient on the canadian hansards task'],\n",
       " 486: ['we show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure'],\n",
       " 487: ['we present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features'],\n",
       " 488: ['in this paper we propose a competition learning approach to coreference resolution',\n",
       "  'nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model',\n",
       "  'furthermore our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution'],\n",
       " 489: ['several approaches have been described for the automatic unsupervised acquisition of patterns for information extraction',\n",
       "  'in this paper we compare the prior models and introduce a new model the subtree model based on arbitrary subtrees of dependency trees'],\n",
       " 490: ['our system provides a unified approach to the four fundamental features of wordlevel chinese language processing word segmentation morphological analysis factoid detection and named entity recognition',\n",
       "  'the performance of the system is evaluated on a manually annotated test set and is also compared with several stateoftheart systems taking into account the fact that the definition of chinese words often varies from system to system'],\n",
       " 491: ['one common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns with gradually degrading precision'],\n",
       " 494: ['starred examples are correct in corpus alternates are parse errors string speculator richard denthese structures are typically bracketed flat in the etb underspecifying the semantic relations relative to the ctb'],\n",
       " 495: [],\n",
       " 496: ['in this paper we propose an approach to information ordering that is particularly suited for texttotext generation'],\n",
       " 497: ['this segmentation algorithm uses automatically induced decision rules to combine the different features',\n",
       "  'the embedded textbased algorithm builds on lexical cohesion and has performance comparable to stateoftheart algorithms based on lexical information'],\n",
       " 498: ['this paper describes a method of detecting grammatical and lexical errors made by japanese learners of english and other techniques that improve the accuracy of error detection with a limited amount of training data',\n",
       "  'in this paper we demonstrate to what extent the proposed methods hold promise by conducting experiments using our learner corpus which contains information on learners errors'],\n",
       " 500: ['this paper describes a noisy channel model of speech repairs which can identify and correct repairs in speech transcripts'],\n",
       " 501: ['we present three methods for training a neural network to estimate the probabilities for a statistical parser one generative one discriminative and one where the probability model is generative but the training criteria is discriminative'],\n",
       " 502: ['this paper describes and evaluates loglinear parsing models for combinatory categorial a parallel implementation of algorithm is described which runs on a beowulf cluster allowing the complete penn treebank to be used for estimation',\n",
       "  'we also develop a new efficient parsing for maximises expected recall of dependencies'],\n",
       " 503: ['the perceptron approach was implemented with the same feature set as that of an existing generative model roark a and experimental results show that it gives competitive performance to the generative model on parsing the penn treebank'],\n",
       " 504: ['this paper proposes a new approach for resolution which uses the tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the bell tree to the leaf nodes'],\n",
       " 505: ['this paper presents a new framework that allows direct orthographical mapping dom between two different languages through a joint sourcechannel model also transliteration model tm the tm model we automate the orthographic alignment process to derive the aligned transliteration units from bilingual dictionary',\n",
       "  'the tm under the dom framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other stateoftheart machine learning algorithms'],\n",
       " 506: ['to determine this powe propose a novel machinelearning method that applies textcategorization techniques to just the subjective portions of the document',\n",
       "  'extracting these portions can be implemented using efficient for finding cuts in this greatly facilitates incorporation of crosssentence contextual constraints'],\n",
       " 507: ['whilst there are a few handtagged corpora available for some languages one would expect the frequency distribution of the senses of words particularly topical words to depend on the genre and domain of the text under consideration',\n",
       "  'we present work on the use of a thesaurus acquired from raw textual corpora and the wordnet similarity package to find predominant noun senses automatically'],\n",
       " 508: [],\n",
       " 509: ['in this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments',\n",
       "  'support vector machines svms using a combination of such kernels and the flat feature kernel classify prop bank predicate arguments with accuracy higher the current argument classification state additionally experiments on framenet data have shown that svms are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement'],\n",
       " 510: ['the key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities',\n",
       "  'our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision but also that appropriate labels could be automatically provided for the relations'],\n",
       " 511: ['we examine the utility of different features such as wordnet hypernyms parts of speech and entity types and find that the dependency tree kernel achieves a f improvement over a bagofwords kernel'],\n",
       " 512: ['this allows for collective information extraction that exploits the mutual influence between possible extractions',\n",
       "  'experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach'],\n",
       " 513: ['present a generative model for the learning of dependency structures',\n",
       "  'we also demonstrate that the combined model works and is robust crosslinguistically being able to exploit either attachment or distributional regularities that are salient in the data'],\n",
       " 514: ['we demonstrate reduction in alignment error rate of approximately resulting from giving extra weight to the probability of alignment to the null word smoothing probability estimates for rare words and using a simple heuristic estimation method to initialize or replace em training of model parameters'],\n",
       " 515: ['more comprehensively we incorporate all the criteria using two selection strategies both of which result in less labeling cost than singlecriterionbased method',\n",
       "  'the results of the named entity recognition in both muc and genia show that the labeling cost can be reduced by at least without degrading the performance'],\n",
       " 516: ['adequacy placed more emphasis on terms cooccurred in candidate and reference translations as shown in the higher correlations in stem set than case set in table while the reverse was true in the terms of fluency'],\n",
       " 517: ['in an ordinary syntactic parser the input is a string and the grammar ranges over strings'],\n",
       " 518: ['we then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance'],\n",
       " 519: ['we employ maximum entropy models to combine diverse lexical syntactic and semantic features derived from the text',\n",
       "  'here we present our general approach and describe our ace results'],\n",
       " 520: ['the method produces performance higher than the previous best results on conll syntactic chunking and conll named entity chunking english and german'],\n",
       " 523: ['we present an effective training algorithm for linearlyscored dependency parsers that implements online largemargin multiclass training crammer and singer crammer et al on top of efficient parsing techniques for dependency trees eisner'],\n",
       " 524: ['experiments using data from the prague dependency treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy',\n",
       "  'this leads to the best reported performance for robust nonprojective parsing of czech'],\n",
       " 525: ['then we apply a based on a labeling formulation of the problem that alters a givenary classifiers output in an explicit attempt to ensure that similar items receive similar labels'],\n",
       " 526: ['we propose a method for extracting semantic orientations of words desirable or undesirable',\n",
       "  'regarding semantic orientations as spins of electrons we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function',\n",
       "  'given only a small number of seed words the proposed method extracts semantic orientations with high accuracy in the experiments on english lexicon'],\n",
       " 527: ['we view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function'],\n",
       " 528: ['in this paper we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems',\n",
       "  'we propose a set of partitionbased features to learn a ranking model for distinguishing good and bad partitions'],\n",
       " 529: ['this paper describes a simple yet novel method for constructing sets of best parses based on a coarsetofine generative parser charniak'],\n",
       " 530: ['the model is formally a synchronous contextfree grammar but is learned from a bitext without any syntactic information',\n",
       "  'thus it can be seen as shift to the of syntaxtranslation systems without any lin in our experiments using bleu as a metric the hierarchical phrasebased model achieves a relative improvement of over pharaoh a stateoftheart phrasebased system'],\n",
       " 532: ['summarization step sentence knight and marcu knight and marcu kampm present a noisychannel model for sentence compression',\n",
       "  'the main difficulty in using this method is the lack of data knight and marcu use a corpus of training sentences',\n",
       "  'more data is not easily available so in addition to improving the original kampm noisychannel model we create unsupervised and semisupervised models of the task'],\n",
       " 533: ['applied to a sequence labeling problempos tagging given a tagging dictionary and unlabeled textcontrastive estimation outperforms em with the same feature set is more robust to degradations of the dictionary and can largely recover by modeling additional features'],\n",
       " 534: ['most current statistical natural language processing models use only local features so as to permit dynamic programming in inference but this makes them unable to fully account for the long distance structure that is prevalent in language use'],\n",
       " 535: ['the approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant',\n",
       "  'evaluation shows this algorithm performs well when compared with a previously reported documentcentric approach'],\n",
       " 537: ['we also demonstrate how semantic information such as wordnet and name list can be used in featurebased relation extraction to further improve the performance',\n",
       "  'evaluation on the ace corpus shows that effective incorporation of diverse features enables our system outperform previously bestreported systems on the ace relation subtypes and significantly outperforms tree kernelbased systems by over in fmeasure on the ace relation types'],\n",
       " 538: ['we present a framework for word alignment based on loglinear models'],\n",
       " 539: ['we present a version of inversion transduction grammar where rule probabilities are lexicalized throughout the synchronous parse tree along with pruning techniques for efficient training'],\n",
       " 540: ['in this paper we also use support vector machines to combine features from traditional reading level measures statistical language models and other language processing tools to produce a better method of assessing reading level'],\n",
       " 541: ['the first step of the method is to parse the source language string that is being translated',\n",
       "  'the second step is to apply a series of transformations to the parse tree effectively reordering the surface string on the source language side of the translation system',\n",
       "  'the goal of this step is to recover an underlying word order that is closer to the target language wordorder than the original string'],\n",
       " 542: ['in this paper we present a syntaxbased statistical matranslation system based on a probabilistic synchronous dependency insertion grammar',\n",
       "  'we evaluate the outputs of our mt system using the nist and bleu automatic mt evaluation software',\n",
       "  'the result shows that our system outperforms the baseline system based on the ibm models in both translation speed and quality'],\n",
       " 543: ['we present an approach to using a morphological analyzer for tokenizing and morphologically tagging including partofspeech tagging arabic words in one process',\n",
       "  'we learn classifiers for individual morphological features as well as ways of using these classifiers to choose among entries from the output of the analyzer'],\n",
       " 544: ['in this paper we present a stateoftheart baseline semantic role labeling system based on support vector machine classifiers',\n",
       "  'we show improvements on this system by i adding new features including features extracted from dependency parses ii performing feature selection and calibration and iii combining parses obtained from semantic parsers trained using different syntactic views',\n",
       "  'in order to address this problem we combined semantic parses from a minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on charniak parses'],\n",
       " 545: ['this stands in stark contrast to the linguistic observation that a core argument frame is with strong dependencies between arguments'],\n",
       " 546: ['we define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities and show how it can be refined to take contextual information into account'],\n",
       " 547: ['in this paper we explore the power of randomized algorithm to address the challenge of working with very large amounts of data',\n",
       "  'we reduce the running time from quadratic to practically linear in the number of elements to be computed'],\n",
       " 548: ['this paper demonstrates that match with respect to domain and time is also important and presents preliminary experiments with training data labeled with emoticons which has the potential of being independent of domain topic and time'],\n",
       " 549: [],\n",
       " 550: ['we consider the task of unsupervised lecture segmentation',\n",
       "  'our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors'],\n",
       " 551: ['through a simple bootstrapping procedure we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities'],\n",
       " 552: ['the crf is conditioned on both the source and target texts and thus allows for the use of arbitrary and overlapping features over these data',\n",
       "  'we show how a large number of highly predictive features can be easily incorporated into the crf and demonstrate that even with only a few hundred wordaligned training sentences our model improves over the current stateoftheart with alignment error rates of and for the two tasks respectively'],\n",
       " 553: [],\n",
       " 554: ['by analyzing potentially similar sentence pairs using a signal processinginspired approach we detect which segments of the source sentence are translated into segments in the target sentence and which are not'],\n",
       " 555: ['in this paper we present a method for reducing the granularity of the wordnet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies namely the oxford dictionary of english'],\n",
       " 556: ['this paper we present a weaklysupervised generalpurpose and accurate algorithm for harvesting semantic relations',\n",
       "  'the main contributions are i a method for exploiting generic patterns by filtering incorrect instances using the web and ii a principled measure of pattern and instance reliability enabling the filtering algorithm',\n",
       "  'we present an empirical comof various state of the art systems on different size and genre corpora on extracting various general and specific relations'],\n",
       " 557: ['this paper presents a pilot study of the use of phrasal statistical machine translation smt techniques to identify and correct writing errors made by learners of english as a second language esl',\n",
       "  'using examples of mass noun errors in the learner error cor clec guide creation of an engineered training set we show that application of the smt paradigm can capture errors not well addressed by widelyused proofing tools designed for native speakers',\n",
       "  'our system was able to correct of mistakes in a set of naturallyoccurring examples of mass noun errors found on the world wide web suggesting that efforts to collect alignable corpora of preand postediting esl writing samples offer can enable the development of smtbased writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of esl learners'],\n",
       " 559: ['in particular we show that the reranking parser described in charniak and johnson improves performance of the parser on brown to'],\n",
       " 560: ['we present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank'],\n",
       " 561: ['the model provides contentdependent hierarchical phrasal reordering with generalization based on features automatically learned from a realworld bitext',\n",
       "  'we present an algorithm to extract all reordering events of neighbor blocks from bilingual data'],\n",
       " 562: ['we propose a new distortion model that can be used with existing phrasebased smt decoders to address those ngram language model limitations',\n",
       "  'we also propose a novel metric to measure word order similarity or difference between any pair of languages based on word alignments'],\n",
       " 563: ['we relate this approach to contrastive estimation smith and eisner a apply the latter to grammar induction in six languages and show that our new approach improves accuracy by absolute over ce and over em achieving to our knowledge the best results on this to date'],\n",
       " 564: ['the model is linguistically syntaxbased because tats are extracted automatically from wordaligned source side parsed parallel texts'],\n",
       " 565: ['this paper deals with morphological disambiguation of the hebrew language which combines morphemes into a word in both agglutinative and fusional ways',\n",
       "  'we present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules which are quite restricted in hebrew helps in the disambiguation'],\n",
       " 566: ['we propose two new bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively',\n",
       "  'the bigram model greatly outperforms the unigram model and previous probabilistic models demonstrating the importance of such dependencies for word segmentation'],\n",
       " 568: ['to address data sparseness we used temporal reasoning as an oversampling method to dramatically expand the amount of training data resulting in predictive accuracy on link labeling as high as using a maximum entropy classifier on human annotated data',\n",
       "  'this method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions'],\n",
       " 569: ['we introduce a semisupervised approach to training for statistical machine translation that alternates the traditional expectation maximization step that is applied on a large training corpus with a discriminative step aimed at increasing wordalignment quality on a small manually wordaligned subcorpus',\n",
       "  'we show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality'],\n",
       " 570: ['we apply our algorithm on the problem of sensedisambiguated noun hyponym acquisition where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantaxonomy wordnet'],\n",
       " 571: [],\n",
       " 572: ['our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering and can also easily scale to include more features',\n",
       "  'evaluation on the ace corpus shows that our method outperforms the previous bestreported methods and significantly outperforms previous two dependency tree kernels for relation extraction'],\n",
       " 573: ['unsupervised dop models assign all possible binary trees to a set of sentences and next use a large random subset of all subtrees from these binary trees to compute the most probable parse trees',\n",
       "  'to the best of our knowledge this is the first paper which tests a maximum likelihood estimator for dop on the wall street journal leading the surprising result that unsupervised parsing model beats a widely used supervised model a treebank pcfg'],\n",
       " 574: ['work on the semantics of questions has argued that the relation between a question and its answers can be cast in terms of logical entailment',\n",
       "  'in this paper we demonstrate how computational systems to recognize entailment can be used to enhance the accuracy of current opendomain automatic question answering qa systems'],\n",
       " 575: [],\n",
       " 576: ['syn tactic approaches seek to remedy these problemsin this paper we take the framework for acquiring multilevel syntactic translation rules of gal ley et al from aligned treestring pairs and present two main extensions of their approach first instead of merely computing a single derivation that minimally explains a sentence pair we constructa large number of derivations that include contextually richer rules and account for multiple interpretations of unaligned words',\n",
       "  'we contrast differentapproaches on real examples show that our esti mates based on multiple derivations favor phrasal reorderings that are linguistically better motivated and establish that our larger rules provide a bleu point increase over minimal rules'],\n",
       " 577: ['the paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations even in relatively simple real bitexts in syntactically similar languages with rigid word order'],\n",
       " 579: ['this paper explores their interaction and brings empirical evidence in support of the hypotheses that subjectivity is a property that can be associated with word senses and word sense disambiguation can directly benefit from subjectivity annotations'],\n",
       " 581: [],\n",
       " 582: ['we use a publicly available structured output svm to create a maxmargin syntactic aligner with a soft cohesion constraint',\n",
       "  'the resulting aligner is the first to our knowledge to use a discriminative learning method to train an itg bitext parser'],\n",
       " 583: ['in this paper we review and compare the different constraints theoretically and provide an experimental evaluation using data from two treebanks investigating how large a proportion of the structures found in the treebanks are permitted under different constraints'],\n",
       " 584: ['given a users query the system will automatically create patterns to extract salient relations in the text of the topic and build tables from the extracted information using paraphrase discovery technology'],\n",
       " 586: ['unlike in current stateoftheart approaches the kind and number of different tags is generated by the method itself',\n",
       "  'the approach is evaluated on three different languages by measuring agreement with existing taggers'],\n",
       " 587: ['we describe the new release of the rasp robust accurate statistical parsing system designed for syntactic annotation of free text',\n",
       "  'the new version includes a revised and more semanticallymotivated output representation an enhanced grammar and partofspeech tagger lexicon and a more flexible and semisupervised training method for the structural parse ranking model'],\n",
       " 588: ['we propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure while retaining the robustness and efficiency of the hmm alignment model'],\n",
       " 590: ['recent research presents conflicting evidence on whether word sense disambiguation wsd systems can help to improve the performance of statistical machine translation mt systems',\n",
       "  'we show for the first time that integrating a wsd system improves the performance of a stateoftheart statistical mt system on an actual translation task',\n",
       "  'furthermore the improvement is statistically significant'],\n",
       " 591: ['in this paper we first show that an active learning approach can be successfully used to perform domain adaptation of wsd systems'],\n",
       " 592: ['in both cases our methods achieve significant speed improvements often by more than a factor of ten over the conventional beamsearch method at the same levels of search error and translation accuracy'],\n",
       " 593: [],\n",
       " 594: ['we evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good precision'],\n",
       " 596: ['evaluating a parser on the same resource used to create it can lead to noncomparable accuracy scores and an overoptimistic view of parser performance this paper we evaluate a on depbank and demonstrate the difficulties in converting the parser output into dep bank grammatical relations'],\n",
       " 597: ['in this paper we study the domain adaptation problem from the instance weighting perspective',\n",
       "  'we formally analyze and characterize the domain adaptation problem from a distributional view and show that there are two distinct needs for adaptation corresponding to the different distributions of instances and classification functions in the source and the target domains'],\n",
       " 598: ['in this paper we suggest a method for incorporating domain knowledge in semisupervised learning algorithms',\n",
       "  'our novel framework unifies can exploit several kinds of specific the experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks'],\n",
       " 599: ['in addition to supertagging we also explore the utility of a surface global grammaticality measure based on combinatory operators'],\n",
       " 600: [],\n",
       " 601: [],\n",
       " 602: ['in this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity',\n",
       "  'the primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another'],\n",
       " 603: ['this measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains'],\n",
       " 604: ['we present an approach to query expansion in answer retrieval that uses statistical machine translation smt techniques to bridge the lexical gap between questions and answers'],\n",
       " 605: ['here we explore the use of bfs for language modelling in statistical machine translation show how a bf containing can enable us to use much larger corpora and higherorder models complementing a con lm within an smt system',\n",
       "  'our solutions in both cases retain the onesided error guarantees of the bf while taking advantage of the zipflike distribution of word frequencies to reduce the space requirements'],\n",
       " 606: ['given a few pairs of named entities known to exhibit or not exhibit a particular relation bags of sentences containing the pairs are extracted from the web',\n",
       "  'we extend an existing relation extraction method to handle this weaker form of supervision and present experimental results demonstrating that our approach can reliably extract relations from web documents'],\n",
       " 607: ['chl dozhangmicrosoftcom mhliinsunhiteducn muli mingzhoumicrosoftcom guanyiinsunhiteducn abstract inspired by previous preprocessing approaches to smt this paper proposes a novel probabilistic approach to reordering which combines the merits of syntax and phrasebased smt',\n",
       "  'experiments show that for the nist mt task of chineseto english translation the proposal leads to bleu improvement of'],\n",
       " 608: ['this paper presents a method which alleviates this problem by exploiting multiple translations of the same source central to our approach is triangulathe process of translating from a source to a target language via an intermediate third language',\n",
       "  'this allows the use of a much wider range of parallel corpora for training and can be combined with a standard phrasetable using conventional smoothing methods'],\n",
       " 609: ['this difference ensures that the learned structure will have high probability over a range of possible parameters and permits the use of priors favoring the sparse distributions that are typical of natural language'],\n",
       " 610: ['in this paper we propose guided learning a new learning framework for bidirectional sequence classification',\n",
       "  'the tasks of learning the order of inference and training the local classifier are dynamically incorporated into a single perceptron like learning algorithm',\n",
       "  'we apply this novel learning algorithm to pos tagging'],\n",
       " 611: ['we study the impact of syntactic and shallow semantic information in automatic classification of questions and answers and answer reranking',\n",
       "  'we define a new tree structures based on shallow semantics encoded in predicate argument structures pass and b new kernel functions to exploit the representational power of such structures with support vector machines',\n",
       "  'our experiments suggest that syntactic information helps tasks such as questionanswer classification and that shallow semantics gives remarkable contribution when a reliable set of pass can be extracted eg from answers'],\n",
       " 612: ['standard approaches to chinese word segmentation treat the problem as a tagging task assigning labels to the characters in the sequence indicating whether the character marks a word boundary',\n",
       "  'closed tests on the first and show that our system is competitive with the best in the literature achieving the highest reported fscores for a number of corpora'],\n",
       " 613: ['we present an unsupervised nonparametric bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document',\n",
       "  'while most existing coreference work is driven by pairwise decisions our model is fully generative producing each mention from a combination of global entity properties and local attentional state'],\n",
       " 614: ['using statistical machine translation techniques a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms',\n",
       "  'the resulting parser is shown to be the bestperforming system so far in a database query domain'],\n",
       " 615: ['this paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in english',\n",
       "  'given a bridge between english and the selected target language eg a bilingual dictionary or a parallel corpus the methods can be used to rapidly create tools for subjectivity analysis in the new language'],\n",
       " 616: ['our contributions include a precise description of the task with annotation guidelines analysis and discussion a probabilistic weakly supervised learning model and experimental evaluation of the methods presented'],\n",
       " 619: ['we combine the strengths of bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs',\n",
       "  'incorporating a sparse prior using variational bayes biases the models toward generalizable parsimonious parameter sets leading to significant improvements in word alignment'],\n",
       " 620: ['among syntaxbased translation models the which takes as input a parse tree of the source sentence is a promising direction being faster and simpler than its stringbased counterpart',\n",
       "  'we a that translates a packed forest of exponentially many parses which encodes many more alternatives standard lists'],\n",
       " 621: ['we present a translation model which models derivations as a latent variable in both training and decoding and is fully discriminative and globally optimised'],\n",
       " 623: ['we apply the hypothesis of one sense per discourse yarowsky to information extraction ie and extend the scope of discourse from one single document to a cluster of topicallyrelated documents',\n",
       "  'combining global evidence from related documents with local decisions we design a simple scheme to conduct crossdocument inference for improving the ace event ex without using any additional labeled data this new approach obtained higher fmeasure in trigger labeling and higher fmeasure in argument labeling over a stateoftheart ie system which extracts events independently for each sentence'],\n",
       " 624: ['we propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings a fundamental problem in aspectbased sentiment summarization hu and liu a'],\n",
       " 625: ['here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity',\n",
       "  'using a treebank grammar a datadriven lexicon and a linguistically motivated unknowntokens handling technique our model outperforms previous pipelined integrated or factorized systems for hebrew morphological and syntactic processing yielding an error reduction of over the best published results so far'],\n",
       " 626: ['compared with previous models it not only captures nonsyntactic phrases and discontinuous phrases with linguistically structured features but also supports multilevel structure reordering of tree typology with larger span',\n",
       "  'experimental results on the nist mt chineseenglish translation task show that our method statistically significantly outperforms the baseline systems'],\n",
       " 627: ['in this paper we propose a novel stringtodependency algorithm for statistical machine translation',\n",
       "  'with this new framework we employ a target dependency language model during decoding to exploit long distance word relations which are unavailable with a traditional ngram language model',\n",
       "  'our experiments show that the stringtodependency decoder achieves point improvement in bleu and point improvement in ter compared to a standard hierarchical stringtostring system on the nist chineseenglish evaluation set'],\n",
       " 628: ['our final result an fscore of outperforms both best and best reranking baselines and is better than any previously reported systems trained on the treebank'],\n",
       " 629: ['for example in the case of english unlabeled secondorder parsing we improve from a baseline accuof in the case of czech unlabeled secondorder parsing we from a baseline accuracy of addition we demonstrate that our method also improves performance when small amounts of training data are available and can roughly halve the amount of supervised data required to reach a desired level of performance'],\n",
       " 630: ['we incorporate up to gwords one billion tokens of unlabeled data which is the largest amount of unlabeled data ever used for these tasks to investigate the performance improvement',\n",
       "  'in addition our results are superior to the best reported results for all of the above test collections'],\n",
       " 631: ['in particular we study the task of morphological segmentation of multiple languages',\n",
       "  'we present a nonparametric bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies crosslingual morpheme pator we apply our model to three semitic languages arabic hebrew aramaic as well as to english'],\n",
       " 632: ['we demonstrate that good results can be obtained using the robust emhmm learner when provided with good initial conditions even with incomplete dictionaries',\n",
       "  'we present a family of algorithms to compute effective estimations we test the method on the task of full morphological disambiguation in hebrew achieving an error reduction of over a strong uniform distribution baseline',\n",
       "  'we also test the same method on the standard wsj unsupervised pos tagging task and obtain results competitive with recent stateoftheart methods while using simple and efficient learning methods'],\n",
       " 633: ['we introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially classbased models and a distributed version of this algorithm to efficiently obtain automatic word classifications large vocabularies million words ussuch large training corpora billion tokens',\n",
       "  'we show that combining them with wordmodels in the loglinear model of a stateoftheart statistical machine translation system leads to improvements in translation quality as indicated by the bleu score'],\n",
       " 634: ['translations are induced using a generative model based on canonical correlation analysis which explains the monolingual lexicons in terms of latent matchings',\n",
       "  'we show that highprecision lexicons can be learned in a variety of language pairs and from a range of corpus types'],\n",
       " 636: ['in this paper propose a joint segmentation and tagging model that does not impose any hard constraints on the interaction between word and fast decoding is achieved by using a novel multiplebeam search algorithm'],\n",
       " 637: ['we propose a cascaded linear model for joint chinese word segmentation and partofspeech tagging',\n",
       "  'with a characterbased perceptron as the core combined with realvalued features such as language models the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly'],\n",
       " 638: ['by letting one model generate features for the other we consistently improve accuracy for both models resulting in a significant improvement of the state of the art when evaluated on data sets from the conllx shared task'],\n",
       " 639: ['while prior featurebased dynamic programming parsers have restricted training and evaluation to artificially short sentences we present the first general featurerich discriminative parser based on a conditional random field model which has been successfully scaled to the full wsj parsing data'],\n",
       " 640: ['in adding syntax to statistical mt there is a tradeoff between taking advantage of linguistic analysis versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data',\n",
       "  'a number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment',\n",
       "  'we present an approach that explores the tradeoff from the other direction starting with a contextfree translation model learned directly from aligned parallel text and then adding soft constituentlevel constraints based on parses of the source language'],\n",
       " 641: ['additionally we resolve a significant complication that nonlinear word lattice inputs introduce in reordering models',\n",
       "  'our experiments evaluating the approach demonstrate substantial gains for chinese english and arabicenglish translation'],\n",
       " 642: ['we present a novel approach to weakly supervised semantic class learning from the web using a single powerful hyponym pattern combined with graph structures which capture two properties associated with patternbased ina candidate is it was discovered many times by other instances in the pattern',\n",
       "  'together these two measures capture not only frequency of occurrence but also crosschecking that the candidate occurs both near the class name and near other class members'],\n",
       " 644: ['a desirable quality of a coreference resolution system is the ability to handle transitivity constraints such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions it will also consider the likelihood of those two mentions being coreferent when making a final assignment',\n",
       "  'we train a coreference classifier over pairs of mentions and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments'],\n",
       " 645: [],\n",
       " 646: ['in this paper we present a reinforcement learning approach for mapping natural language instructions to sequences of executable actions',\n",
       "  'during training the learner repeatedly constructs action sequences for a set of documents executes those actions and observes the resulting reward'],\n",
       " 647: ['a central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state',\n",
       "  'to deal with the high degree of ambiguity present in this setting we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state'],\n",
       " 649: ['this paper presents an unsupervised opinanalysis method for clasie recognizing which stance a person is taking in an online debate',\n",
       "  'in order to handle the complexities of this genre we mine the web to learn associations that are indicative of opinion stances in debates',\n",
       "  'we combine this knowledge with discourse information and formulate the debate side classification task as an integer linear programming problem'],\n",
       " 650: ['this paper focuses on the problem of crosslingual sentiment classification which leverages an available english corpus for chinese sentiment classification by using the english corpus as training data',\n",
       "  'experimental results show the effectiveness of the proposed approach which can outperform the standard inductive classifiers and the transductive classifiers'],\n",
       " 651: ['we evaluate the performance of our parser on data in several natural languages achieving improvements over existing stateoftheart methods'],\n",
       " 652: ['adding the swapping operation changes the time complexity for deterministic parsing from linear to quadratic in the worst case but empirical estimates based on treebank data show that the expected running time is in fact linear for the range of data attested in the corpora'],\n",
       " 653: ['unlike previous approaches our framework does not require full projected parses allowing partial approximate transfer through linear expectation constraints on the space of distributions over trees'],\n",
       " 654: ['we describe a novel method for the task of unsupervised pos tagging with a dictionary one that uses integer programming to explicitly search for the smallest model that explains the data and then uses em to set parameter values'],\n",
       " 655: ['in this paper we present a discriminative wordcharacter hybrid model for joint chinese word segmentation and pos tagging',\n",
       "  'we describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus',\n",
       "  'we describe an efficient framework for training our model based on the margin infused relaxed algorithm mira evaluate our approach on the penn chinese treebank and show that it achieves superior performance compared to the stateoftheart approaches reported in the literature'],\n",
       " 656: ['we describe an unsupervised system for learncoherent sequences or sets events whose arguments are filled with participant semantic roles defined over words jury unlike most previous work in event structure or semantic role learning our system does not use supervised techniques handbuilt knowledge or predefined classes of events or roles',\n",
       "  'our unsupervised learning algorithm uses coreferring arguments in chains of verbs to learn both rich narrative event structure and argument roles'],\n",
       " 657: ['second we measure the performance of a stateoftheart coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets'],\n",
       " 658: ['we work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses'],\n",
       " 659: ['we present a phrasal synchronous grammar model of translational equivalence',\n",
       "  'this sampler sidesteps the intractability issues of previous models which required inference over derivation forests'],\n",
       " 660: [],\n",
       " 661: ['we consider maximum margin and conditional likelihood objectives including the presentation of a new normal form grammar for canonicalizing derivations',\n",
       "  'even for nonitg sentence pairs we show that it is possible learn itg alignment models by simple relaxations of structured discriminative learning objectives',\n",
       "  'altogether our method results in the best reported aer numbers for chineseenglish and a performance improvement of bleu over giza alignments'],\n",
       " 662: ['we also analyze feature performance showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression'],\n",
       " 663: ['we present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers',\n",
       "  'to demonstrate the power and generality of this approach we apply the method in two very different applications named entity recognition and query classification'],\n",
       " 664: [],\n",
       " 665: [],\n",
       " 666: ['we present algorithms for higherorder dependency parsing that are thirdorder in the sense that they can evaluate substructures containing three dependencies and efficient in the sense that they reonly importantly our new parsers can utilize both siblingstyle and grandchildstyle interactions'],\n",
       " 667: [],\n",
       " 668: ['by simultaneously inferring latent topics and topic distributions over relations the benefits of previous approaches like traditional classbased approaches it produces humaninterpretable classes describing each relations preferences but it is competitive with nonclassbased methods in predictive power compare several stateoftheart methods achieving an increase in recall at precision over mutual information erk'],\n",
       " 669: ['our experiments demonstrate that very large crfs can be trained efficiently and that very large models are able to improve the accuracy while delivering compact parameter sets'],\n",
       " 670: ['incremental parsing techniques such as shiftreduce have gained popularity thanks to their efficiency but there remains a problem the search is only explores a tiny fraction of the whole space even with beam search as opposed to dynamic programming'],\n",
       " 671: ['the research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade'],\n",
       " 672: ['but progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language'],\n",
       " 673: ['our approach is based on comparing the crossentropy according to domainspecific and nondomainspecifc language models for each sentence of the text source used to produce the latter language model'],\n",
       " 674: ['using a single unified internal representation for translation forests the decoder strictly separates modelspecific translation logic from general rescoring pruning and inference algorithms',\n",
       "  'from this unified representation the decoder can not only the or translations but also alignments to a reference or the quantities necessary to drive discriminative training using gradientbased or gradientfree optimization techniques'],\n",
       " 675: ['in this paper we propose to improve targetdependent twitter sentiment classification by incorporating targetdependent features and taking related tweets into consideration',\n",
       "  'according to the experimental results our approach greatly improves the performance of targetdependent sentiment classification'],\n",
       " 676: ['a number of different features are extracted and ablation tests are used to investigate their contribution to overall performance'],\n",
       " 677: ['a lack of standard datasets and evaluation has prevented the field of paraphrasmaking the kind of rapid progress enjoyed by the machine translation community over the last years',\n",
       "  'the highly parallel nature of this data allows us to use simple ngram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates',\n",
       "  'in addition to being simple and efficient to compute experiments show that these metrics correlate highly with human judgments'],\n",
       " 678: ['both word similarity and context are then exploited to select the most probable correction candidate for the word'],\n",
       " 679: ['knowledgebased weak supervision using structured data to heuristically label a training corpus works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors',\n",
       "  'recently researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling but their models assume are for example they extract the pair this paper presents a novel approach for multiinstance learning with overlapping relations that combines a sentencelevel extraction model with a simple corpuslevel component for aggregating the individual facts'],\n",
       " 680: ['in tackling this challenging learning problem we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms'],\n",
       " 681: ['we describe a novel approach for inducing unsupervised partofspeech taggers for languages that have no labeled training data but have translated text in a resourcerich language',\n",
       "  'our method does not assume any knowledge about the target language in particular no tagging dictionary is assumed making it applicable to a wide array of resourcepoor languages'],\n",
       " 682: ['this paper describes an approach to templatebased ie that removes this requirement and performs extraction without knowing the template structure in advance'],\n",
       " 684: ['we address the problem of partofspeech tagging for english data from the popular microblogging service twitter',\n",
       "  'the data and tools have been made available to the research community with the goal of enabling richer text analysis of twitter and related social media data sets'],\n",
       " 685: ['we provide a systematic analysis of the effects of optimizer instabilityan extraneous variable that is seldom controlled foron experimental outcomes and make recommendations for reporting results more accurately'],\n",
       " 687: ['we introduce a new dataset with human judgments on pairs of words in sentential context and evaluate ourmodel on it showing that our model outper forms competitive baselines and other neural language models'],\n",
       " 688: ['natural language parsing has typically been done with small sets of discrete categories such as np and vp but this representation does not capture the full syntactic nor semantic richness of linguistic phrases and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness'],\n",
       " 703: ['in this paper we present a formalization of the centering approach to modeling attentional structure in discourse and use it as the basis for an algorithm to track discourse context and bind pronouns',\n",
       "  'as described in gjw the process of centering attention on entities in the discourse gives rise to the intersentential states of retaining shiftpropose an extension to these states which handles some additional cases of multiple ambiguous pronouns'],\n",
       " 704: ['this paper describes a method of unification by successive approximation resulting in better average performance'],\n",
       " 705: ['to interpret a sentence an approach to abductive inference developed in the tac itus project has resulted in a dramatic simplification of how the problem of interpreting texts is conceptualized',\n",
       "  'its use in solving the local pragmatics problems of reference compound nominals syntactic ambiguity and metonymy is described and illustrated'],\n",
       " 706: ['we applied control criteria to four dialogues and identified levels of discourse structure',\n",
       "  'we investigated the mechanism for changing control between these structures and found that utterance type and not cue words predicted shifts of control',\n",
       "  'participants used certain types of signals when discourse goals were proceeding successfully but resorted to interruptions when they were not'],\n",
       " 708: ['the enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semanticheaddriven fashion'],\n",
       " 711: ['in order to take steps towards establishing a methodology for evaluating natural language systems we conducted a case study',\n",
       "  'we attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the cospecifiers of pronouns in naturally occurring texts and dialogues',\n",
       "  'we present the quantitative results of handsimulating these algorithms but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general'],\n",
       " 712: ['cdg parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraintpropagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees'],\n",
       " 714: ['we describe a program xtract that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism'],\n",
       " 716: ['this approach exploits the differences between mappings of words to senses in different languages'],\n",
       " 719: ['this paper describes an implemented program that takes a raw untagged text corpus as its only input no openclass dictionary and generates a partial list of verbs occurring in the text and the subcategorization frames sfs in which they occur',\n",
       "  'the completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus'],\n",
       " 720: ['we propose that ambiguous prepositional phrase attachment can be resolved on the basis of the relative strength of association of the preposition with noun and verb estimated on the basis of word distribution in a large corpus'],\n",
       " 723: ['we present here criteria and techniques for automatically detecting the presence of a repair its location and making the appropriate correction'],\n",
       " 728: [],\n",
       " 732: ['however there is weak consensus on the nature of segments and the criteria for recognizing or generating them'],\n",
       " 733: ['this paper discusses how to estimate the probability of cooccurrences that do not occur in the training data'],\n",
       " 735: ['we describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts',\n",
       "  'words are represented by the relative frequency distributions of contexts in which they appear and relative entropy between those distributions is used as the similarity measure for clustering',\n",
       "  'clusters are used as the basis for class models of word coocurrence and the models evaluated with respect to heldout test data'],\n",
       " 736: ['further it is argued that this method can be used to learn all subcategorization frames whereas previous methods are not extensible to a general solution to the problem'],\n",
       " 737: ['by repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus the system learns a set of simple structural transformations that can be applied to reduce error'],\n",
       " 738: ['this paper proposes a new indicator of text structure called the lexical cohesion profile lcp which locates segment boundaries in a text',\n",
       "  'comparison with the text segments marked by a number of subjects shows that lcp closely correlates with the human judgments'],\n",
       " 739: ['this paper describes texttiling an algorithm for partitioning expository texts into coherent multiparagraph discourse units which reflect the subtopic structure of the texts',\n",
       "  'the algorithm uses domainindependent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes',\n",
       "  'two fullyimplemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts'],\n",
       " 740: ['our report concerns three related topics progress on the hkust englishchinese parallel bilingual corpus experiments addressing applicability of gale lengthbased statistical method to the task of alignment involving a nonindoeuropean language and an improved statistical method that also incorporates domainspecific lexical cues'],\n",
       " 741: ['by identifying and utilizing only the single best disambiguating evidence in a target context the algorithm avoids the problematic complex modeling of statistical dependencies',\n",
       "  'although directly applicable to a wide class of ambiguities the algorithm is described and evaluated in a realistic case study the problem of restoring missing accents in spanish and french text'],\n",
       " 743: ['further approximating the joint distribution of all variables with a model identifying only the most important systematic interactions among variables limits the number of parameters to be estimated supports computational efficiency and provides an understanding of the data',\n",
       "  'in this paper a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun describe a method for formulating probabilistic models that use multiple contextual features for wordsense disambiguation without requiring untested assumptions regarding the form of the model'],\n",
       " 745: ['in general when a component some dtree a is inserted into a dedge betnodes and two new dedges are created the first of which relates and the root node of the second of which relates the frontier santorini and mahootian provide additional evidence against the standard tag approach to modification from code switching data which can be accounted for by using sisteradjunction']}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_objetivo_cbow_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc58566",
   "metadata": {},
   "source": [
    "### Similaridade fasttext skip-gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8406692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:25.551713Z",
     "start_time": "2023-03-03T02:39:11.199750Z"
    }
   },
   "outputs": [],
   "source": [
    "#Abtract e Introdução\n",
    "sim_ab_intro_sg_ft,indices_ab_intro_sg_ft,lista_idx_ab_intro_sg_ft =aplicando_dicionario_similaridade(list(v_ft_sg_abstract.values()),list(v_ft_sg_introducao.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_sg_ft,indices_ab_conc_sg_ft,lista_idx_ab_c_sg_ft = aplicando_dicionario_similaridade(list(v_ft_sg_abstract.values()),list(v_ft_sg_conclusao.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_sg_ft = frases_interseccao(lista_idx_ab_intro_sg_ft,lista_idx_ab_c_sg_ft)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_sg_ft = get_frases_objetivo(indices_objetivo_sg_ft, papers_treino.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d0cb341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:25.567457Z",
     "start_time": "2023-03-03T02:39:25.551713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 18, 19, 20, 31, 32, 33, 35, 37, 39, 41, 43, 45, 47, 48, 50, 51, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 74, 88, 97, 99, 101, 102, 103, 104, 105, 106, 108, 110, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 198, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 222, 223, 225, 226, 228, 229, 231, 232, 234, 235, 243, 244, 245, 246, 248, 258, 263, 272, 276, 277, 279, 280, 281, 282, 283, 284, 286, 296, 307, 310, 313, 315, 316, 318, 323, 326, 338, 339, 342, 345, 347, 348, 350, 351, 352, 355, 356, 357, 358, 359, 362, 363, 364, 365, 366, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 385, 389, 390, 391, 392, 393, 394, 395, 397, 398, 399, 400, 403, 404, 405, 406, 407, 408, 409, 410, 411, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 427, 428, 430, 431, 432, 433, 434, 435, 436, 437, 439, 440, 442, 443, 445, 446, 448, 449, 450, 451, 452, 454, 455, 456, 457, 458, 460, 464, 465, 466, 467, 470, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 494, 495, 496, 497, 498, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 523, 524, 525, 526, 527, 528, 529, 530, 532, 533, 534, 535, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 559, 560, 561, 562, 563, 564, 565, 566, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 579, 581, 582, 583, 584, 586, 587, 588, 590, 591, 592, 593, 594, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 619, 620, 621, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 636, 637, 638, 639, 640, 641, 642, 644, 645, 646, 647, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 684, 685, 687, 688, 703, 704, 705, 706, 708, 711, 712, 714, 716, 719, 720, 723, 728, 732, 733, 735, 736, 737, 738, 739, 740, 741, 743, 745])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_objetivo_sg_ft.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a59f4e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:25.583290Z",
     "start_time": "2023-03-03T02:39:25.567457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frase_objetivo_sg_ft']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(frase_objetivo_sg_ft,'frase_objetivo_sg_ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731a8ab",
   "metadata": {},
   "source": [
    "As frases que extrai foram da variavel `abstract_frases`, ou seja, são as frases do abstract já pré-processadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a578680",
   "metadata": {},
   "source": [
    "## Validação - Exp1 - ROUGE-N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa2de7",
   "metadata": {},
   "source": [
    "### Pré-Processamento Ouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d79219af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:26.221575Z",
     "start_time": "2023-03-03T02:39:25.583290Z"
    }
   },
   "outputs": [],
   "source": [
    "def separa_frases_ouro(txt: str):\n",
    "    \"\"\"Separa as frases dos papers\"\"\"\n",
    "    txt = txt.split('\\n')\n",
    "    return txt\n",
    "\n",
    "padrao_ouro_treino_frases = {k: separa_frases_ouro(v) for k, v in padrao_ouro_treino2.items()}\n",
    "padrao_ouro_treino_frases = segundo_preprocessamento(padrao_ouro_treino_frases)\n",
    "padrao_ouro_treino_palavras = {k:[word_tokenize(f) for f in v] for k,v in padrao_ouro_treino_frases.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e1f5f",
   "metadata": {},
   "source": [
    "### Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8da15c59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:26.237649Z",
     "start_time": "2023-03-03T02:39:26.223207Z"
    }
   },
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b45d9b3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:26.253364Z",
     "start_time": "2023-03-03T02:39:26.239435Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculo_rouge(frase_objetivo,lista_frases_padrao_ouro):\n",
    "    rouge_total=[]\n",
    "    for i,j in zip(frase_objetivo,lista_frases_padrao_ouro):\n",
    "        rouge_1={}\n",
    "        for h in range(len(i)):\n",
    "            for z in range(len(j)):\n",
    "                rouge_1[(h,z)] = (scorer.score(i[h],j[z])['rouge1'].fmeasure)\n",
    "        rouge_total.append(rouge_1)\n",
    "    return rouge_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe87826d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:26.269106Z",
     "start_time": "2023-03-03T02:39:26.255123Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max(dic, coord, val):\n",
    "    return max(filter(lambda item: item[0][coord] == val, dic.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ebdff2e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:26.285246Z",
     "start_time": "2023-03-03T02:39:26.271097Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_rouge(rouge_total):\n",
    "    \"\"\"pega o máximo rouge score para cada frase selecionada como pertencente ao objetivo, em comparação com cada frase\n",
    "    do padrão ouro\"\"\"\n",
    "    maximo_rouge_por_paper =[]\n",
    "    for h in rouge_total:\n",
    "        x=[]\n",
    "        for i in set([i[0] for i in h]):\n",
    "            x.append(get_max(h,0,i))\n",
    "        maximo_rouge_por_paper.append(x)\n",
    "    return maximo_rouge_por_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e41aff92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:32.867118Z",
     "start_time": "2023-03-03T02:39:26.285246Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_sg_ft = calculo_rouge(list(frase_objetivo_sg_ft.values()),list(padrao_ouro_treino_frases.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08144bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T18:17:43.741437Z",
     "start_time": "2023-02-28T18:17:43.741437Z"
    }
   },
   "source": [
    "lista=[]\n",
    "for idx in list(padrao_ouro_treino_frases.keys()):\n",
    "    dicionario = {}\n",
    "    for f_obj, idx_obj in zip(frase_objetivo_sg_ft[idx],range(len(frase_objetivo_sg_ft[idx]))):\n",
    "        for f_ouro, idx_ouro in zip(padrao_ouro_treino_frases[idx], range(len(padrao_ouro_treino_frases[idx]))):\n",
    "            dicionario[idx_obj, idx_ouro] = scorer.score(f_obj,f_ouro)['rouge1'].fmeasure\n",
    "            #print(scorer.score(f_obj,f_ouro)['rouge1'].fmeasure)\n",
    "        lista.append(dicionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73cd0fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T18:17:43.757459Z",
     "start_time": "2023-02-28T18:17:43.757459Z"
    }
   },
   "source": [
    "dicionario = {}\n",
    "for f_obj, idx_obj in zip(frase_objetivo_sg_ft[8],range(len(frase_objetivo_sg_ft[8]))):\n",
    "    for f_ouro, idx_ouro in zip(padrao_ouro_treino_frases[8], range(len(padrao_ouro_treino_frases[8]))):\n",
    "        dicionario[idx_obj, idx_ouro] = scorer.score(f_obj,f_ouro)['rouge1'].fmeasure\n",
    "        print(scorer.score(f_obj,f_ouro)['rouge1'].fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b36c8d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.330440Z",
     "start_time": "2023-03-03T02:39:32.867118Z"
    }
   },
   "outputs": [],
   "source": [
    "#w2v\n",
    "rouge_sg_w2v = calculo_rouge(list(frase_objetivo_sg_w2v.values()),list(padrao_ouro_treino_frases.values()))\n",
    "rouge_cbow_w2v = calculo_rouge(list(frase_objetivo_cbow_w2v.values()),list(padrao_ouro_treino_frases.values()))\n",
    "\n",
    "#FT\n",
    "rouge_sg_ft = calculo_rouge(list(frase_objetivo_sg_ft.values()),list(padrao_ouro_treino_frases.values()))\n",
    "rouge_cbow_ft = calculo_rouge(list(frase_objetivo_cbow_ft.values()),list(padrao_ouro_treino_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f27e141e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.361838Z",
     "start_time": "2023-03-03T02:39:58.331625Z"
    }
   },
   "outputs": [],
   "source": [
    "#w2v\n",
    "maximo_rouge_por_paper_sg_w2v   = max_rouge(rouge_sg_w2v)\n",
    "maximo_rouge_por_paper_cbow_w2v = max_rouge(rouge_cbow_w2v)\n",
    "\n",
    "#FT\n",
    "maximo_rouge_por_paper_sg_ft   = max_rouge(rouge_sg_ft)\n",
    "maximo_rouge_por_paper_cbow_ft   = max_rouge(rouge_cbow_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56456692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.377472Z",
     "start_time": "2023-03-03T02:39:58.361838Z"
    }
   },
   "outputs": [],
   "source": [
    "def rouge_precisao_revocacao(maximo_rouge_por_paper, lista_padrao_ouro):\n",
    "    precisao=[]\n",
    "    for i in maximo_rouge_por_paper:\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            soma = soma + j[1]\n",
    "        #print(len(i))\n",
    "        try:\n",
    "            precisao.append(soma/len(i))\n",
    "        except:\n",
    "            precisao.append(0)\n",
    "\n",
    "    revocacao=[]\n",
    "    for i,h in zip(maximo_rouge_por_paper,lista_padrao_ouro):\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            soma = soma + j[1]\n",
    "        #print(len(i))\n",
    "        try:\n",
    "            revocacao.append(soma/len(h))\n",
    "        except:\n",
    "            revocacao.append(0)\n",
    "    return precisao, revocacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "332a8724",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.393179Z",
     "start_time": "2023-03-03T02:39:58.377472Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_padrao_ouro_treino  = list(padrao_ouro_treino_frases.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20464a4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.409277Z",
     "start_time": "2023-03-03T02:39:58.393179Z"
    }
   },
   "outputs": [],
   "source": [
    "#w2v\n",
    "precisao_sg_w2v, revocacao_sg_w2v = rouge_precisao_revocacao(maximo_rouge_por_paper_sg_w2v, lista_padrao_ouro_treino)\n",
    "precisao_cbow_w2v, revocacao_cbow_w2v = rouge_precisao_revocacao(maximo_rouge_por_paper_cbow_w2v, lista_padrao_ouro_treino)\n",
    "\n",
    "#FT\n",
    "precisao_sg_ft, revocacao_sg_ft = rouge_precisao_revocacao(maximo_rouge_por_paper_sg_ft, lista_padrao_ouro_treino)\n",
    "precisao_cbow_ft, revocacao_cbow_ft = rouge_precisao_revocacao(maximo_rouge_por_paper_cbow_ft, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f621db0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.424954Z",
     "start_time": "2023-03-03T02:39:58.409277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisao_sg_ft==precisao_cbow_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "576a2781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.441040Z",
     "start_time": "2023-03-03T02:39:58.426529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revocacao_cbow_ft==revocacao_sg_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee2b581c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.456837Z",
     "start_time": "2023-03-03T02:39:58.441040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8052397372035844"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precisao_cbow_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2592b",
   "metadata": {},
   "source": [
    "# Experimento 1 - Conclusão\n",
    "Rouge Usado foi o N=1 e a precisão e recall foram calculadas a partir do f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b6b9caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.472496Z",
     "start_time": "2023-03-03T02:39:58.459528Z"
    }
   },
   "outputs": [],
   "source": [
    "def analise_precisao(precisao, lista_frases_padrao_ouro):\n",
    "    x=[]\n",
    "    qtd_zero=[]\n",
    "    for i in precisao:\n",
    "        if i>=0.8:\n",
    "            x.append(i)\n",
    "        if i==0:\n",
    "            qtd_zero.append(i)\n",
    "    print('qtd de papers com precisao>=0.8:',len(x))\n",
    "    print('% do total:',len(x)/len(lista_frases_padrao_ouro))\n",
    "    print('qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções:',len(qtd_zero))\n",
    "    print('% do total:',len(qtd_zero)/len(lista_frases_padrao_ouro))\n",
    "    print('media precisao:', np.mean(precisao))\n",
    "    return len(x), len(qtd_zero),np.mean(precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8cdd9f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.488449Z",
     "start_time": "2023-03-03T02:39:58.473779Z"
    }
   },
   "outputs": [],
   "source": [
    "def analise_revocacao(revocacao,lista_frases_padrao_ouro,media_ouro):\n",
    "    x=[]\n",
    "    qtd_zero=[]\n",
    "    for i in revocacao:\n",
    "        if i>=(1/media_ouro): #valor de 1/média qtd frases padrao ouro\n",
    "            x.append(i)\n",
    "        if i==0:\n",
    "            qtd_zero.append(i)\n",
    "    print('qtd de papers com revocação>=(1/media_ouro):',len(x))\n",
    "    print('% do total:',len(x)/len(lista_frases_padrao_ouro))\n",
    "    print('qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções:',len(qtd_zero))\n",
    "    print('% do total:',len(qtd_zero)/len(lista_frases_padrao_ouro))\n",
    "    return len(x), np.mean(revocacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1c522a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.504550Z",
     "start_time": "2023-03-03T02:39:58.488449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de papers com precisao>=0.8: 403\n",
      "% do total: 0.8141414141414142\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 59\n",
      "% do total: 0.1191919191919192\n",
      "media precisao: 0.8307540252735163\n"
     ]
    }
   ],
   "source": [
    "qtd_precisao_08_sg_w2v, qtd_zero_sg_w2v, media_sg_w2v = analise_precisao(precisao_sg_w2v, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ac3119ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.520197Z",
     "start_time": "2023-03-03T02:39:58.504550Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de papers com precisao>=0.8: 394\n",
      "% do total: 0.795959595959596\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 74\n",
      "% do total: 0.1494949494949495\n",
      "media precisao: 0.8052397372035844\n"
     ]
    }
   ],
   "source": [
    "qtd_precisao_08_cbow_w2v, qtd_zero_cbow_w2v, media_cbow_w2v = analise_precisao(precisao_cbow_w2v, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "735cbea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.536219Z",
     "start_time": "2023-03-03T02:39:58.520197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de papers com precisao>=0.8: 402\n",
      "% do total: 0.8121212121212121\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 67\n",
      "% do total: 0.13535353535353536\n",
      "media precisao: 0.8207254113547606\n"
     ]
    }
   ],
   "source": [
    "qtd_precisao_08_sg_ft, qtd_zero_sg_ft, media_sg_ft = analise_precisao(precisao_sg_ft, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "671eb315",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:52:04.763878Z",
     "start_time": "2023-03-03T02:52:04.748957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de papers com precisao>=0.8: 409\n",
      "% do total: 0.8262626262626263\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 59\n",
      "% do total: 0.1191919191919192\n",
      "media precisao: 0.831012788967217\n"
     ]
    }
   ],
   "source": [
    "qtd_precisao_08_cbow_ft, qtd_zero_cbow_ft, media_cbow_ft = analise_precisao(precisao_cbow_ft, lista_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "844f3971",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.568350Z",
     "start_time": "2023-03-03T02:39:58.555061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'media_cbow_ft'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisao_mean =  {'media_cbow_ft':media_cbow_ft,\n",
    "                  'media_sg_ft':media_sg_ft,\n",
    "                  'media_cbow_w2v':media_cbow_w2v,\n",
    "                  'media_sg_w2v':media_sg_w2v}\n",
    "max(precisao_mean, key=precisao_mean.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ecb7114e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.584159Z",
     "start_time": "2023-03-03T02:39:58.568350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qtd_zero_sg_w2v'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtd_papers_hipotese_invalida = {'qtd_zero_sg_w2v':qtd_zero_sg_w2v,\n",
    "                              'qtd_zero_cbow_w2v':qtd_zero_cbow_w2v,\n",
    "                               'qtd_zero_cbow_ft':qtd_zero_cbow_ft,\n",
    "                              'qtd_zero_sg_ft':qtd_zero_sg_ft}\n",
    "min(qtd_papers_hipotese_invalida, key=qtd_papers_hipotese_invalida.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "62546538",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.599676Z",
     "start_time": "2023-03-03T02:39:58.586662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qtd_precisao_08_cbow_ft'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtd_precisao_alta = {'qtd_precisao_08_sg_w2v':qtd_precisao_08_sg_w2v,\n",
    "                     'qtd_precisao_08_sg_w2v':qtd_precisao_08_sg_w2v,\n",
    "                     'qtd_precisao_08_cbow_ft':qtd_precisao_08_cbow_ft,\n",
    "                     'qtd_precisao_08_sg_ft':qtd_precisao_08_sg_ft}\n",
    "max(qtd_precisao_alta, key=qtd_precisao_alta.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cdc83040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.618491Z",
     "start_time": "2023-03-03T02:39:58.602670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.642424242424243"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Média de palavras do Padrão Ouro\n",
    "x=[]\n",
    "for i in lista_padrao_ouro_treino:\n",
    "    x.append(len(i))\n",
    "media_ouro=np.mean(x)\n",
    "media_ouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92ff106e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.685156Z",
     "start_time": "2023-03-03T02:39:58.669519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v\n",
      "skip-gram\n",
      "qtd de papers com revocação>=(1/media_ouro): 312\n",
      "% do total: 0.6303030303030303\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 59\n",
      "% do total: 0.1191919191919192\n",
      "cbow\n",
      "qtd de papers com revocação>=(1/media_ouro): 296\n",
      "% do total: 0.597979797979798\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 74\n",
      "% do total: 0.1494949494949495\n",
      "-----------\n",
      "fasttext\n",
      "skip-gram\n",
      "qtd de papers com revocação>=(1/media_ouro): 312\n",
      "% do total: 0.6303030303030303\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 67\n",
      "% do total: 0.13535353535353536\n",
      "cbow\n",
      "qtd de papers com revocação>=(1/media_ouro): 325\n",
      "% do total: 0.6565656565656566\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 59\n",
      "% do total: 0.1191919191919192\n"
     ]
    }
   ],
   "source": [
    "print('w2v')\n",
    "print('skip-gram')\n",
    "qtd_revocacao_sg_w2v, media_revocacao_sg_w2v     = analise_revocacao(revocacao_sg_w2v, lista_padrao_ouro_treino, media_ouro)\n",
    "\n",
    "print('cbow')\n",
    "qtd_revocacao_cbow_w2v,  media_revocacao_cbow_w2v   = analise_revocacao(revocacao_cbow_w2v, lista_padrao_ouro_treino, media_ouro)\n",
    "\n",
    "print('-----------')\n",
    "print('fasttext')\n",
    "print('skip-gram')\n",
    "qtd_revocacao_sg_ft,  media_revocacao_sg_ft       = analise_revocacao(revocacao_sg_ft, lista_padrao_ouro_treino, media_ouro)\n",
    "\n",
    "print('cbow')\n",
    "qtd_revocacao_cbow_ft,  media_revocacao_cbow_ft       = analise_revocacao(revocacao_cbow_ft, lista_padrao_ouro_treino, media_ouro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3074ab50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.701314Z",
     "start_time": "2023-03-03T02:39:58.685156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qtd_revocacao_cbow_ft'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtd_revocacao_alta = {'qtd_revocacao_sg_w2v':qtd_revocacao_sg_w2v,\n",
    "                      'qtd_revocacao_cbow_w2v':qtd_revocacao_cbow_w2v,\n",
    "                     'qtd_revocacao_sg_ft':qtd_revocacao_sg_ft,\n",
    "                     'qtd_revocacao_cbow_ft':qtd_revocacao_cbow_ft}\n",
    "max(qtd_revocacao_alta, key=qtd_revocacao_alta.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f97e7c6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.717267Z",
     "start_time": "2023-03-03T02:39:58.702600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'media_revocacao_sg_w2v'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_revocacao   = {'media_revocacao_sg_w2v':media_revocacao_sg_w2v,\n",
    "                     'media_revocacao_cbow_w2v':media_revocacao_cbow_w2v,\n",
    "                     'media_revocacao_sg_ft':media_revocacao_sg_ft,\n",
    "                     'media_revocacao_cbow_ft':media_revocacao_cbow_ft}\n",
    "max(media_revocacao, key=media_revocacao.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e5a55b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:45:24.654842Z",
     "start_time": "2023-03-03T02:45:24.641195Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19107567596404137"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_revocacao_sg_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "96f65fc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:45:25.023060Z",
     "start_time": "2023-03-03T02:45:25.008035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1771586186176"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_revocacao_cbow_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "62f48e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:45:25.340275Z",
     "start_time": "2023-03-03T02:45:25.325913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19093071913479925"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_revocacao_sg_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6c9a3a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:45:25.739677Z",
     "start_time": "2023-03-03T02:45:25.710605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1877424709115802"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_revocacao_cbow_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7402b7",
   "metadata": {},
   "source": [
    "# Experimento 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0a903",
   "metadata": {},
   "source": [
    "`frases_objetivo_modelo` alterar pro resultado oficial depois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee0782dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.889686Z",
     "start_time": "2023-03-03T02:39:58.717267Z"
    }
   },
   "outputs": [],
   "source": [
    "frases_objetivo_modelo = frase_objetivo_cbow_ft.copy()\n",
    "palavras_objetivo_modelo = {k:[word_tokenize(f) for f in v] for k,v in frases_objetivo_modelo.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2689e",
   "metadata": {},
   "source": [
    "## Bigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "60d0b962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.905716Z",
     "start_time": "2023-03-03T02:39:58.891940Z"
    }
   },
   "outputs": [],
   "source": [
    "#O bigrama é calculado a partir de uma lista com listas de palavras de cada frase\n",
    "frase_objetivo_ft_formato_ml=[]\n",
    "for i in list(palavras_objetivo_modelo.values()):\n",
    "    for j in i:\n",
    "        frase_objetivo_ft_formato_ml.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "095d2420",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.921431Z",
     "start_time": "2023-03-03T02:39:58.905716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "697"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qtd de frases objetivo total\n",
    "len(frase_objetivo_ft_formato_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "31210ffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:58.937237Z",
     "start_time": "2023-03-03T02:39:58.922548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qtd de palavras de cada frase\n",
    "len(frase_objetivo_ft_formato_ml[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74a1f574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:39:59.251343Z",
     "start_time": "2023-03-03T02:39:58.937237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3190 items>\n"
     ]
    }
   ],
   "source": [
    "#Bigrama - O treino do bigrama ocorre a partir das frases objetivo\n",
    "n = 2\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, frase_objetivo_ft_formato_ml)\n",
    "\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa04ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T07:50:49.696170Z",
     "start_time": "2023-02-28T07:50:49.664873Z"
    }
   },
   "source": [
    "`frases_papers_treino`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94e4d5f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:40:17.187360Z",
     "start_time": "2023-03-03T02:39:59.251343Z"
    }
   },
   "outputs": [],
   "source": [
    "palavras_papers_treino = {k:[word_tokenize(f) for f in v] for k,v in frases_papers_treino.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1d5a4d97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:40:18.205960Z",
     "start_time": "2023-03-03T02:40:17.187360Z"
    }
   },
   "outputs": [],
   "source": [
    "#Criando os bigramas do paper de treino completo\n",
    "bigramas=[]\n",
    "for papers in list(palavras_papers_treino.values()):\n",
    "    x=[]\n",
    "    for frases in papers:\n",
    "        x.append(bigrams(frases))\n",
    "    bigramas.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1ae84625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:40:18.941587Z",
     "start_time": "2023-03-03T02:40:18.205960Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_bigramas2=[]\n",
    "for j in bigramas:\n",
    "    lista_bigramas=[]  \n",
    "    for i in j:\n",
    "        lista_bigramas.append(list(i))\n",
    "    lista_bigramas2.append(lista_bigramas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ff75f5f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:40:59.376663Z",
     "start_time": "2023-03-03T02:40:18.941587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Probabilidade de cada frase do paper, com base nos bigramas\n",
    "lista_paper_prob=[]\n",
    "lista_paper_perp=[]\n",
    "for paper in lista_bigramas2:\n",
    "    lista_prob=[]\n",
    "    lista_perp=[]\n",
    "    for frase in paper:\n",
    "        prob=1\n",
    "        for bigrama in frase:\n",
    "            prob = prob * (model.score(bigrama[1], context=bigrama[0].split()))\n",
    "        lista_prob.append(prob)\n",
    "        try:\n",
    "            lista_perp.append(model.perplexity(frase))\n",
    "        except:\n",
    "            lista_perp.append(math.inf)\n",
    "    lista_paper_prob.append(lista_prob)\n",
    "    lista_paper_perp.append(lista_perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b9ceba1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:40:59.407987Z",
     "start_time": "2023-03-03T02:40:59.376663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{16.47238049749035, 12.648706266787169, inf}\n",
      "{11.629476754092662, inf}\n",
      "{11.847763389762102, inf}\n",
      "{16.733996216525092, 15.749670881351575, inf}\n",
      "{20.16839235199321, inf}\n",
      "{inf}\n",
      "{16.212859164282868, inf}\n",
      "{16.58866139577263, 13.231838309201267, inf}\n",
      "{17.368289659722418, inf}\n",
      "{16.812348299400277, 15.371545281857356, inf}\n",
      "{inf}\n",
      "{19.18019637079806, 13.853269779872566, 22.71036541901987, inf}\n",
      "{inf}\n",
      "{16.086055411616357, inf}\n",
      "{32.32604945658536, inf}\n",
      "{26.829833593252555, 15.481870315743803, 278.9999999999999, inf}\n",
      "{12.334862151433546, inf}\n",
      "{44.05592806115084, 30.14331368427415, inf}\n",
      "{1.5491933384829668, 11.926871292303703, 183.00000000000014, inf}\n",
      "{18.29881480333431, 19.155512625666418, inf}\n",
      "{4.999999999999999, 9.000000000000002, 12.648615984565062, 13.773275880747272, inf}\n",
      "{8.259323732443463, 1.7500000000000002, 15.775330878389303, inf}\n",
      "{1.0, 18.48392453281956, 14.771870172870226, inf}\n",
      "{16.22758329523058, 20.60838439654403, inf}\n",
      "{17.49478675041073, inf}\n",
      "{144.69968901141425, 81.3142054010245, 10.214553972569973, inf}\n",
      "{23.040721683998683, 15.703859345019623, inf}\n",
      "{inf}\n",
      "{20.000000000000004, 12.6503986289252, 23.628539023510836, inf}\n",
      "{17.364251357669364, inf}\n",
      "{1.7500000000000002, 19.432975144889596, inf}\n",
      "{24.024143621608324, 15.001182309529337, inf}\n",
      "{16.891616920744728, 8.0, 107.0000000000001, inf}\n",
      "{24.929905008590488, 9.157836393255902, 2.2, inf}\n",
      "{23.47338918861101, inf}\n",
      "{16.183343947588625, 9.618787072942453, 41.99047511043428, inf}\n",
      "{8.156635376488971, inf}\n",
      "{9.77366658122411, 20.997082101247372, 1.0, inf}\n",
      "{68.43610158388627, 9.742974351440571, 15.963053024206598, inf, 17.48194789517245, 535.0000000000002, 57.49782604586021}\n",
      "{8.750000000000002, 17.081996320671443, inf}\n",
      "{17.390362552503927, inf}\n",
      "{inf}\n",
      "{inf}\n",
      "{17.595341840186574, 11.177588848531824, inf}\n",
      "{inf}\n",
      "{9.381799726017594, 57.49782604586021, 14.517891412709057, inf}\n",
      "{inf}\n",
      "{16.86028856368785, inf}\n",
      "{13.366898899524774, 278.9999999999999, inf}\n",
      "{10.245762453968384, 13.414281518884403, 15.20400712948981, inf, 17.881702867883792, 30.191448325058836}\n",
      "{9.594521112190309, 428.9999999999999, 12.942923500817237, inf, 22.61814399063412, 61.000000000000014}\n",
      "{11.730685405406929, 15.811631077432533, inf}\n",
      "{1.2599210498948732, 2.9814239699997196, 19.357941975528313, inf}\n",
      "{16.0, 42.1528819805945, inf}\n",
      "{1.5000000000000002, 4.0, 5.5, 15.255582402243924, inf, 20.772882646388222, 30.236409030879784}\n",
      "{6.666666666666666, 136.83932183404008, 14.68517247678433, inf, 16.949827182336637}\n",
      "{9.435718228168417, 18.666666666666675, 10.532952241261075, inf}\n",
      "{31.000000000000014, 12.069201774197865, 29.02508567524993, inf}\n",
      "{17.718993417357265, 10.579159989814626, inf}\n",
      "{21.992703286649228, inf}\n",
      "{15.52068889291423, 16.980359790860778, 17.767333352512264, inf, 31.061699214860162}\n",
      "{16.195626014563313, 16.261927756495478, inf}\n",
      "{14.98946712778022, 14.653361694251721, inf}\n",
      "{inf}\n",
      "{16.08612671075497, 9.743871228737245, inf}\n",
      "{22.347688999475277, inf}\n",
      "{20.989962749261586, inf}\n",
      "{16.64043706588256, 20.350916247536347, inf}\n",
      "{8.126126308826986, 18.044038929938875, inf}\n",
      "{1.4142135623730951, 12.715228710809843, inf}\n",
      "{16.00965863910035, inf}\n",
      "{13.151896057681661, 14.60007184471041, inf}\n",
      "{16.25390851649275, 19.055946391477068, inf}\n",
      "{16.509267484569204, inf}\n",
      "{16.35905130459114, 33.68495267204216, 10.607087127566453, inf}\n",
      "{10.661291771650069, 13.287491659593371, inf}\n",
      "{10.946738621349803, inf}\n",
      "{17.60741519015943, 113.47889397044715, inf}\n",
      "{18.841887253622957, 15.864860521840384, inf}\n",
      "{10.569065920630393, 14.397087975505366, inf}\n",
      "{12.512521545149653, inf}\n",
      "{16.100841403067427, 3.7996209910274454, inf}\n",
      "{17.345809993314656, inf}\n",
      "{16.75333183381099, 12.438720743713496, inf}\n",
      "{16.616618899891257, 12.36990562092574, inf}\n",
      "{32.449961479175904, inf}\n",
      "{13.223748919010617, inf}\n",
      "{inf}\n",
      "{19.278541119803684, 13.854489400476293, inf}\n",
      "{inf}\n",
      "{14.697782334160564, inf}\n",
      "{15.87962451672959, inf}\n",
      "{17.687562214329603, 11.93129185837723, inf}\n",
      "{16.038501689008555, inf}\n",
      "{21.363964772435722, inf}\n",
      "{7.061684628672931, 7.3162161442984495, inf}\n",
      "{15.363596742431424, 14.578558171529098, inf}\n",
      "{18.00550366287383, inf}\n",
      "{9.877701473571053, 13.896336522662471, inf}\n",
      "{113.47889397044715, 12.253803583844434, 13.716823610616558, inf}\n",
      "{16.688313607543684, inf}\n",
      "{113.47889397044715, 15.136385382199599, 22.34038560356055, inf}\n",
      "{18.184873948087795, inf}\n",
      "{inf}\n",
      "{15.934388453353074, inf}\n",
      "{16.237609032701968, 15.098009952453229, inf}\n",
      "{10.95942781704609, 14.950906814585531, inf}\n",
      "{22.953513818082484, inf}\n",
      "{19.283967221429503, inf}\n",
      "{4.999999999999999, 15.419078258128206, 5.2857142857142865, inf}\n",
      "{16.610382915525925, 23.523188511036725, inf}\n",
      "{24.367536514854216, 22.554687712048963, inf}\n",
      "{17.279579902709916, 19.821574309840535, 12.53223383267717, inf}\n",
      "{23.187561215552556, 13.001646990666414, inf}\n",
      "{14.481613213949085, inf}\n",
      "{inf}\n",
      "{10.880104759208713, inf}\n",
      "{11.503676090785403, 13.467606237169226, inf}\n",
      "{11.732062102837459, 12.744321685230748, inf}\n",
      "{17.168855331392646, 14.959375235964876, inf}\n",
      "{inf}\n",
      "{9.217343680379036, 14.108392912168016, inf}\n",
      "{19.605751542331213, 27.009247421200705, inf}\n",
      "{19.788031087692385, 15.034224666011845, inf}\n",
      "{16.757445203353896, 13.845227121809732, inf}\n",
      "{17.116078601510686, 12.206309257120322, inf}\n",
      "{24.028778302447858, 13.860400266895455, inf}\n",
      "{18.377630466422104, 31.38138814808917, inf}\n",
      "{25.210862381565214, 14.42042946554798, inf}\n",
      "{inf}\n",
      "{18.810445430223027, 99.93331109628402, 12.797202272301547, inf}\n",
      "{25.950365419979658, 23.000000000000004, 14.789634970930585, inf}\n",
      "{40.00839579377937, 17.10675264456359, 10.724707600025518, inf}\n",
      "{1.3333333333333333, 11.730974029396775, 13.917613023069114, inf}\n",
      "{35.0, 15.17505048676858, inf}\n",
      "{26.51868862228135, 14.59814506457421, inf}\n",
      "{9.545673494869305, 11.358667673705906, 9.80841729854312, inf}\n",
      "{1.0, 23.233098992884898, inf}\n",
      "{5.750000000000001, 136.83932183404008, 10.771069086870135, 11.922167379314741, inf, 17.808505191139915}\n",
      "{19.253449458712044, 4.582575694955841, 13.396722498044053, inf}\n",
      "{48.63636363636366, 1.7500000000000002, 11.153596474528548, inf}\n",
      "{11.2503317852938, inf, 16.0, 17.255209495623046, 18.068611236005477, 367.3333333333334}\n",
      "{10.15399928845268, 11.462828134176792, 12.830166887121425, inf}\n",
      "{10.369573014140947, 26.745542808497813, 69.74999999999997, inf}\n",
      "{16.10306990940638, 18.000000000000007, inf}\n",
      "{20.736269514235765, inf}\n",
      "{11.875472820097054, inf}\n",
      "{4.538461538461539, 9.000000000000002, 14.682203462179983, inf, 16.411709145368803}\n",
      "{25.18906406092918, 17.23665983092022, inf}\n",
      "{1.0, 22.619906912143136, inf}\n",
      "{19.242788578960038, 20.277851065341743, inf}\n",
      "{35.0, inf, 17.76039424263452, 18.791077018343064, 19.000000000000007}\n",
      "{inf}\n",
      "{12.928156195600549, 14.837944331484163, inf}\n",
      "{54.727015259922034, inf}\n",
      "{inf, 16.314223777417748, 18.215366258614907, 22.23142864042008, 25.62609411492221}\n",
      "{inf}\n",
      "{16.907054187169734, 41.31562861816402, 8.0408026470434, inf}\n",
      "{81.90500378968885, 12.537878895805475, inf}\n",
      "{13.178928917940286, inf}\n",
      "{14.635564906759582, inf}\n",
      "{inf}\n",
      "{17.194148334116477, 15.032787572745823, 31.445357620084437, inf}\n",
      "{18.38051121477557, 14.312667874572043, inf}\n",
      "{inf}\n",
      "{25.64242843706713, 69.74999999999997, inf}\n",
      "{17.416492715009966, inf}\n",
      "{13.583275493699729, inf}\n",
      "{99.99999999999997, inf}\n",
      "{18.95573159131109, 18.66717102557986, 20.31194544544672, inf}\n",
      "{14.244553125715731, inf}\n",
      "{4.999999999999999, 15.962645370916658, inf, 53.37134062397159, 26.497426886750862}\n",
      "{inf}\n",
      "{15.626680614134967, 30.376833994190058, inf}\n",
      "{9.082771212017535, 69.74999999999997, inf}\n",
      "{16.284534663953867, inf}\n",
      "{11.154047284195444, 11.364505866619607, 15.708900397714512, inf}\n",
      "{19.922187226089854, inf}\n",
      "{18.950816292622285, 12.334722684684372, inf}\n",
      "{inf}\n",
      "{11.034389225966764, inf}\n",
      "{15.54526758936125, inf}\n",
      "{23.363550630549902, 15.81861970210364, 13.029931775245567, inf}\n",
      "{15.019491889407163, inf}\n",
      "{20.832245440913294, 22.261249738979227, inf}\n",
      "{21.27298813314895, inf}\n",
      "{13.663770156798337, inf}\n",
      "{15.619972473403335, 15.562156420542419, 13.235202957925392, inf}\n",
      "{16.077590185947088, inf}\n",
      "{15.225510646081787, 14.386495908392941, inf}\n",
      "{9.615825797911285, inf}\n",
      "{17.00113863577446, inf}\n",
      "{16.25576115764756, 12.694748955078609, inf}\n",
      "{17.811661370852292, 14.429991701779871, inf}\n",
      "{15.117863776563379, inf}\n",
      "{14.707807110530908, inf}\n",
      "{18.062497930356013, 12.342901762719633, inf}\n",
      "{20.588705749275753, 21.00308100482057, inf}\n",
      "{17.61997891835043, 23.252466936582266, inf}\n",
      "{24.186159961358904, inf}\n",
      "{12.527422216980003, inf}\n",
      "{15.428618877696872, inf}\n",
      "{20.483974584003274, inf}\n",
      "{11.175850929743781, 69.74999999999997, inf}\n",
      "{inf}\n",
      "{41.769401512700824, 12.130246980325916, inf, 18.33333333333334, 18.05947405638669}\n",
      "{1.0, inf}\n",
      "{17.8500569108098, inf}\n",
      "{17.99631942135252, 20.367076015717306, inf}\n",
      "{13.639155208330802, inf}\n",
      "{18.49252006463272, inf}\n",
      "{18.136869964929147, 3.0130279780956966, inf}\n",
      "{11.390275452796972, 19.83780966745633, inf}\n",
      "{15.76923925419284, 21.690413529883827, inf}\n",
      "{inf}\n",
      "{16.94299758784783, 23.035516456957847, inf}\n",
      "{13.618394678397504, 13.488103057638108, inf}\n",
      "{14.653027285489049, inf}\n",
      "{21.36803546855926, inf}\n",
      "{19.52175219920624, 15.002735780762336, inf}\n",
      "{19.13529935864628, 21.370567699731627, inf}\n",
      "{13.709194328150742, 14.783803842479404, inf}\n",
      "{16.130470022339512, 20.784259804555365, inf}\n",
      "{10.173985995365209, 10.09950493836208, inf}\n",
      "{11.84281558658128, 13.235950829125859, inf}\n",
      "{11.762959719559904, 12.296441273963383, inf}\n",
      "{1.4142135623730951, 13.364843096438586, inf, 16.834900261507787, 19.36492317258407}\n",
      "{inf}\n",
      "{15.949712603820974, inf}\n",
      "{inf}\n",
      "{12.438290205367696, 21.394673755483574, inf}\n",
      "{19.198569510962056, inf}\n",
      "{10.802642975307577, 13.263588480731071, inf}\n",
      "{9.93647569462597, 11.567869707947565, 14.577143308347978, inf}\n",
      "{16.766316130024258, 9.77377626561372, 22.69649364347093, inf}\n",
      "{9.106314381399088, 12.704456689106383, 13.612782551149488, inf}\n",
      "{15.463803168471914, 15.606309328098087, 12.846903038044236, inf}\n",
      "{14.66826521825481, inf}\n",
      "{inf}\n",
      "{14.417012376606596, inf}\n",
      "{13.176610621700785, 14.461739259680337, inf}\n",
      "{17.172504115945898, 10.114882386816316, inf}\n",
      "{inf}\n",
      "{13.73131161998416, inf}\n",
      "{24.72308170982674, 15.778060296597037, 12.187556652168897, inf}\n",
      "{2.3333333333333335, 20.55948733357379, inf}\n",
      "{inf}\n",
      "{13.963094555858904, 14.93620767246186, inf}\n",
      "{15.120767405831739, 13.428345328920814, inf}\n",
      "{25.58619128341471, 12.014048016402171, inf}\n",
      "{24.018993105353868, 17.393353617089318, inf}\n",
      "{16.877491820227444, 15.300090093416765, inf}\n",
      "{15.888215985497803, 12.812577587834653, inf}\n",
      "{15.847752654809437, inf}\n",
      "{24.467706934421347, 11.15291390000092, inf}\n",
      "{15.927940338339306, inf}\n",
      "{19.767780982484716, 21.595008303582816, inf}\n",
      "{inf}\n",
      "{14.006592332362015, inf}\n",
      "{16.218100299308745, inf}\n",
      "{12.508250250987738, inf}\n",
      "{15.355304160506556, inf}\n",
      "{13.056149172787112, inf}\n",
      "{19.891245735224707, 12.777294552127024, inf}\n",
      "{12.59539044757217, inf}\n",
      "{inf}\n",
      "{136.83932183404008, 20.050055422778104, 13.628771147522265, inf}\n",
      "{15.935471291399715, inf}\n",
      "{16.659115673713195, 10.813374031945512, inf}\n",
      "{9.619829306257193, 14.801240865221708, inf}\n",
      "{inf}\n",
      "{20.202878330020745, 13.046623584045385, inf}\n",
      "{14.282510766146917, inf}\n",
      "{17.00494608922092, inf}\n",
      "{15.642706368174629, 13.285485263450434, inf}\n",
      "{15.454458860181001, inf}\n",
      "{22.048645200723655, inf}\n",
      "{8.76466758706666, inf}\n",
      "{12.313822457144541, 15.31409986126448, inf}\n",
      "{inf}\n",
      "{10.348146013572933, inf}\n",
      "{15.358125582551269, 14.18729973746467, 6.800432017188842, inf}\n",
      "{16.84622564527962, 12.506054030165162, inf}\n",
      "{12.510724286659563, inf}\n",
      "{18.82137643240079, inf}\n",
      "{26.749107622450236, 19.027070671958974, 22.966132153046505, inf}\n",
      "{17.97475321464063, inf}\n",
      "{13.619874636660523, 20.098906960397567, 13.862180360704203, inf}\n",
      "{24.243260981046774, 11.759505567266324, inf}\n",
      "{13.041174917894319, inf}\n",
      "{36.50000000000001, 14.190847425499301, inf}\n",
      "{13.562563590310697, inf}\n",
      "{7.990950682870438, 8.581835173806128, 11.939256948921763, inf, 17.90139495789754}\n",
      "{18.169405741110896, 23.280765941159355, inf}\n",
      "{23.058483866329745, 20.193914066099254, 13.925518523927884, inf}\n",
      "{16.355172363956367, inf}\n",
      "{11.969135666823764, inf}\n",
      "{inf}\n",
      "{9.96143225490779, inf}\n",
      "{19.744383051687937, 21.26577374148302, inf}\n",
      "{15.054281050244926, 12.518008637925906, inf}\n",
      "{12.456138676037364, inf}\n",
      "{25.289458160862385, inf}\n",
      "{15.758544761429738, 15.80148977192704, inf}\n",
      "{23.171703798500076, inf}\n",
      "{19.6677682565453, inf}\n",
      "{20.671059302006807, 14.013128803883667, inf}\n",
      "{9.876508192881635, 13.793277161394627, inf}\n",
      "{15.818804619888502, 12.500938966461412, inf}\n",
      "{inf}\n",
      "{12.609790898882382, 13.310851639447836, inf}\n",
      "{18.498193650751034, 14.485957458287269, inf}\n",
      "{10.984928361672173, 14.852262421654343, inf}\n",
      "{18.36083039003565, 15.488012838608615, inf}\n",
      "{17.88701664590738, 12.459648426777864, inf}\n",
      "{16.899319201147225, inf}\n",
      "{17.35596226502144, 13.092214652972146, inf}\n",
      "{17.65890385067395, inf}\n",
      "{32.68578192449364, inf}\n",
      "{11.5328809596392, inf}\n",
      "{26.57555930163276, 14.356144763779838, inf}\n",
      "{18.09660437552816, inf}\n",
      "{11.59269348407817, inf}\n",
      "{16.585050975662657, 25.69582796546986, inf}\n",
      "{15.872531414981745, inf}\n",
      "{18.84482315498873, 15.152914791383449, 12.802660648653243, inf}\n",
      "{16.498683628282528, inf}\n",
      "{7.483314773547884, 8.0, 12.517765990387236, 12.888888888888893, 15.000000000000002, 15.376710003468947, inf}\n",
      "{12.316052798087966, inf}\n",
      "{16.38760703516409, 17.988381972533308, 12.085994659044113, inf}\n",
      "{18.833231687350178, 10.370566946838373, 26.153388774821124, inf}\n",
      "{76.00000000000003, 14.52869565608375, inf}\n",
      "{18.089089163567515, inf}\n",
      "{16.999936561757735, 14.216388964490479, inf}\n",
      "{15.390562765711367, 12.3418222896176, 14.405641217978227, inf}\n",
      "{16.21862270975546, 13.246391813345683, inf}\n",
      "{13.559419608967513, inf}\n",
      "{13.695752233628214, inf}\n",
      "{19.885021946982462, 28.954679210787358, 20.1711877897301, inf}\n",
      "{27.80648740547516, 7.834847846414602, 21.232225603021018, inf}\n",
      "{15.320163983611957, 21.975184662910845, inf}\n",
      "{8.215775230297359, inf, 18.949824663048176, 22.332087691647406, 22.941133039125983}\n",
      "{17.984571338407424, inf}\n",
      "{14.30786574309078, inf}\n",
      "{16.258594257144807, 24.58498454222149, inf}\n",
      "{22.659397248996473, inf}\n",
      "{inf}\n",
      "{15.854261437890317, 22.44744548077437, inf}\n",
      "{20.725872960381654, inf}\n",
      "{16.109676966609367, 22.6552806784716, inf}\n",
      "{inf}\n",
      "{15.649811038615315, inf}\n",
      "{10.101386602289374, inf}\n",
      "{8.29747494551091, 16.190319656890484, 14.874082317816637, inf}\n",
      "{10.680895529303095, 12.11918478786678, 13.323589965372149, inf}\n",
      "{15.873222715062541, inf}\n",
      "{15.872873576956652, inf}\n",
      "{2.6666666666666665, 42.18154127088804, 11.915235408941484, 14.394911714045936, inf}\n",
      "{18.540505508400944, 11.1713772549397, inf}\n",
      "{20.688585827961422, inf}\n",
      "{10.636526235973138, inf}\n",
      "{16.899926931068027, 15.215920687092247, inf}\n",
      "{18.964671812735617, 4.999999999999999, 29.66433758634418, inf}\n",
      "{12.771269326230144, inf, 16.961531645071215, 16.424538201720846, 23.304214591967302}\n",
      "{15.785952324688765, 13.390161358389587, inf}\n",
      "{14.656993203819853, inf}\n",
      "{21.690413529883827, inf}\n",
      "{18.305347856772553, 13.896400288040782, inf}\n",
      "{11.813755480982666, 21.39537454560055, inf}\n",
      "{16.490977998097737, 9.978211744414025, inf}\n",
      "{inf}\n",
      "{1.0, 4.999999999999999, 11.0288580815025, 12.749714068656742, inf}\n",
      "{13.090987427322972, inf}\n",
      "{16.080833854384423, inf}\n",
      "{inf}\n",
      "{15.362245600983604, 21.08133105523358, inf}\n",
      "{16.187130466444817, inf}\n",
      "{16.39847887843374, inf}\n",
      "{17.199739248825576, 23.972562677659685, inf}\n",
      "{17.961273394220406, 19.585559820796263, inf}\n",
      "{13.703230625627235, inf}\n",
      "{9.738035799952378, 12.085994659044113, inf, 20.394843277731805, 29.669314152855122}\n",
      "{12.11377086668871, inf}\n",
      "{21.689174568037295, inf}\n",
      "{20.000000000000004, inf}\n",
      "{19.655224292057675, inf}\n",
      "{17.526041616138507, inf}\n",
      "{10.397119040159145, 18.856618308880122, inf}\n",
      "{19.701795400162894, 14.572551682523272, inf}\n",
      "{3.0000000000000004, 13.316303851123575, inf}\n",
      "{inf}\n",
      "{inf}\n",
      "{20.361096953980358, 23.529721725251704, inf}\n",
      "{14.618580275983055, inf}\n",
      "{13.328145391595083, inf}\n",
      "{13.198643509639412, 13.844337722686156, inf}\n",
      "{16.422223432162866, 15.659895427883004, 15.234022279471326, inf}\n",
      "{9.243637970371514, 18.962332907881763, 15.487553220997885, inf}\n",
      "{13.490844051377074, 14.644752404260569, inf}\n",
      "{17.148988814690778, inf}\n",
      "{25.327773197632947, 10.739821946433882, 23.40814452356225, inf}\n",
      "{31.270770029834136, 13.693661464863952, 12.493576781627175, inf}\n",
      "{15.875299139421857, 13.812614579359847, inf}\n",
      "{10.785595877509943, 13.537563179445588, inf}\n",
      "{19.20454188010857, 14.697089554224506, inf}\n",
      "{16.719710491044456, 17.92872364192824, inf}\n",
      "{19.265164747373746, inf}\n",
      "{32.256568196257696, 8.84320433740421, inf}\n",
      "{16.046096468060075, 21.33324184846432, inf}\n",
      "{17.50745325052492, 21.737481351469544, inf}\n",
      "{24.256385093774846, 29.18332857414773, 13.935583375618283, inf}\n",
      "{13.31605032328995, inf}\n",
      "{16.77248114149462, 10.995611877961506, inf}\n",
      "{9.800896631496135, 13.296575404493199, inf}\n",
      "{18.10999939332163, 12.677501052739041, 5.733207650820337, inf}\n",
      "{20.672309076007064, inf}\n",
      "{19.29088352484986, inf}\n",
      "{18.21712031340213, 15.468596541419592, inf}\n",
      "{10.61172946750659, 22.762462525021775, inf}\n",
      "{10.111850979168693, 15.721552456566307, 15.000063893099561, 17.521381765352917, inf}\n",
      "{16.682902030403117, 11.236460191582726, 12.353846521360756, inf}\n",
      "{16.626183886517047, 13.594698166892924, inf}\n",
      "{14.530199951313548, inf}\n",
      "{10.928586698105471, 12.83452428398344, inf, 15.021518296271205}\n",
      "{17.762261160924513, inf}\n",
      "{10.603553822493641, inf}\n",
      "{15.084851443793985, 12.329621670355937, 14.686469271530905, inf}\n",
      "{17.033727115737435, 12.447756646222492, 4.616546328154847, inf}\n",
      "{11.709087865305907, 14.406001426506373, inf}\n",
      "{18.205351823373093, 15.843415493155948, inf}\n",
      "{inf}\n",
      "{10.65102152674606, 10.46115323385326, inf}\n",
      "{24.297746600223043, 15.868346744623421, inf}\n",
      "{12.264709177494025, 21.37074826745669, 22.895826496643103, inf}\n",
      "{16.097683989104755, 29.532101593055568, 22.829063242454957, inf}\n",
      "{24.80906507587456, inf}\n",
      "{17.720647913284985, inf}\n",
      "{9.215011188053829, inf}\n",
      "{18.76926294009779, inf}\n",
      "{18.63243489069351, 13.516741009329008, 6.746431013114057, inf}\n",
      "{12.36049354024554, 13.766394978449334, inf}\n",
      "{73.14369419163897, 5.750000000000001, 22.409642743003236, inf}\n",
      "{24.772102495264345, inf}\n",
      "{15.488103414380184, 13.510327203021623, inf}\n",
      "{inf}\n",
      "{16.496919876508567, 15.232159909014253, 13.052014232546988, inf}\n",
      "{16.410972399091417, inf}\n",
      "{25.989507537570447, 22.55899000654215, inf}\n",
      "{inf}\n",
      "{inf}\n",
      "{15.664822155400177, inf}\n",
      "{inf}\n",
      "{12.288910332775309, inf}\n",
      "{11.201378755544823, inf}\n",
      "{11.607596269491857, inf}\n",
      "{16.980690770378953, inf}\n",
      "{18.597171324776586, inf}\n",
      "{17.27452425866487, inf}\n",
      "{19.80384332160924, 11.949166538646152, inf}\n",
      "{2.8602585059832295, 10.114130459839425, 13.85682188758816, inf}\n",
      "{14.834291479484051, inf}\n",
      "{20.26422993979703, 13.640653942636991, 22.242968905980483, inf}\n",
      "{20.439367113430706, inf}\n",
      "{14.62799336246514, 14.125890438158743, inf}\n",
      "{20.692949867142815, inf}\n",
      "{16.92431865832716, 12.581779516645648, inf}\n",
      "{15.821719097559688, inf}\n",
      "{15.530983354391912, 13.93912423378057, inf}\n",
      "{9.910239850303165, inf}\n",
      "{13.124567485019906, inf}\n",
      "{15.578424466955628, inf}\n",
      "{16.824286129548238, 16.33030740045096, inf}\n",
      "{12.06747034369206, inf}\n",
      "{16.736976598266953, 14.28494823619105, inf}\n",
      "{8.561742426223747, 19.14126816369597, 21.341149019672322, inf}\n",
      "{13.583275493699729, inf}\n",
      "{21.548360082276307, 21.990285459117946, 13.638050813618962, inf}\n",
      "{8.0, 10.171325536099118, inf}\n",
      "{15.431446081410417, inf}\n",
      "{26.497426886750862, inf}\n",
      "{16.046482905750988, 13.15696454936577, inf}\n",
      "{14.159875961467918, inf}\n",
      "{19.614233018391978, inf}\n",
      "{inf}\n",
      "{14.473510077460249, inf}\n",
      "{13.908342949894092, inf}\n",
      "{18.947926780144986, 18.500647601712778, 22.69396691124729, inf}\n",
      "{11.669975095220588, inf}\n",
      "{15.752460547201384, inf}\n",
      "{10.506172587618405, 10.199944949528719, inf}\n",
      "{8.874224135873304, 11.353957694466114, 22.668354803878174, inf}\n",
      "{9.225206461812705, 12.876689199593045, inf}\n",
      "{183.00000000000014, 13.480299273801123, 22.2907417965025, inf}\n",
      "{16.940529399616022, 14.208287920630173, inf}\n",
      "{16.558590793104784, inf}\n"
     ]
    }
   ],
   "source": [
    "for i in lista_paper_perp:\n",
    "    print(set(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c46109c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:40:59.423610Z",
     "start_time": "2023-03-03T02:40:59.407987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 8.681939995634711e-34, 9.234446772403475e-18}\n",
      "{0.0, 2.570149734654029e-10}\n",
      "{0.0, 2.64683560122929e-38}\n",
      "{0.0, 1.450823815086472e-54, 1.73061012554813e-17}\n",
      "{0.0, 8.064442902301553e-27}\n",
      "{0.0}\n",
      "{0.0, 7.324420256152657e-42}\n",
      "{0.0, 1.4593197655664175e-27, 5.203014049537852e-31}\n",
      "{0.0, 1, 3.6429874848987405e-08}\n",
      "{0.0, 1.9609181041695747e-15, 3.302017036458673e-29}\n",
      "{0.0, 1}\n",
      "{0.0, 2.223526376731727e-54, 5.0805026833054306e-61, 2.822416263511938e-33}\n",
      "{0.0}\n",
      "{0.0, 2.8715056264536765e-27}\n",
      "{0.0, 1.9924833743257123e-32}\n",
      "{0.0, 2.2002500847419e-17, 1.929870127781526e-06, 0.0035842293906810036}\n",
      "{0.0, 1, 1.2194940675195413e-23}\n",
      "{0.0, 1, 7.859069556986966e-29, 1.1694642147111013e-05}\n",
      "{0.0, 1, 0.4166666666666667, 1.221340290613607e-27, 0.00546448087431694}\n",
      "{0.0, 1.1578587258581836e-19, 1.116572937516703e-18, 1}\n",
      "{0.0, 1, 0.1111111111111111, 0.2, 1.526356358419e-09, 6.340789403705126e-27}\n",
      "{0.0, 1, 0.5714285714285714, 4.617895358484145e-08, 4.4103545515167e-27}\n",
      "{1.3170334487514217e-20, 0.0, 4.634779384501298e-07, 1}\n",
      "{0.0, 1, 1.6818395730637348e-68, 5.2374642540751616e-27}\n",
      "{0.0, 4.834761103853362e-33}\n",
      "{0.0, 1, 0.0001512401693889897, 5.403027543755649e-30, 4.776005349125991e-05}\n",
      "{0.0, 1.63157068821016e-37, 1, 3.2495325440483493e-34}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 0.05, 1.8973014878657172e-25, 9.076445052978373e-23}\n",
      "{0.0, 1, 2.7959846726494668e-24}\n",
      "{0.0, 1, 0.5714285714285714, 8.334887347611318e-37}\n",
      "{0.0, 1, 8.121445829681326e-23, 1.7327867135483978e-12}\n",
      "{0.0, 0.125, 1, 2.033384032305634e-31, 0.009345794392523364}\n",
      "{0.0, 0.45454545454545453, 1, 6.927182493852703e-22, 3.247123406834631e-48}\n",
      "{0.0, 1, 0.0018148820326678765}\n",
      "{0.0, 1, 0.0005671506352087115, 1.936220046368033e-17, 3.666275751600019e-32}\n",
      "{0.0, 1, 0.0018427518427518428}\n",
      "{0.0, 2.033372275510754e-31, 1.5890551315155931e-24, 1.0}\n",
      "{0.0, 2.2969560085929787e-19, 1, 0.001869158878504673, 0.0003024803387779794, 1.7733142182543823e-22, 0.0002135155332550443, 3.279964789999916e-33}\n",
      "{0.0, 1, 2.0101260722134018e-57, 0.11428571428571428}\n",
      "{0.0, 1, 5.166661427586004e-28}\n",
      "{0.0}\n",
      "{0.0, 1}\n",
      "{0.0, 1.3481358191591286e-19, 1.2895999030750364e-30, 1}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 1.6928945228733845e-56, 2.4304803282188457e-49, 0.0003024803387779794}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 1.5959520115675482e-64}\n",
      "{0.0, 1, 0.0035842293906810036, 2.256088184799888e-24}\n",
      "{0.0, 1, 0.0009297520661157025, 9.964632574446614e-14, 3.633692381423779e-05, 3.594841581803438e-31, 4.6828166483900056e-42}\n",
      "{0.0, 1, 1.8605890363953138e-15, 9.625362245113612e-21, 8.642295393656554e-05, 0.002331002331002331, 0.01639344262295082}\n",
      "{0.0, 1, 1.0359765138977618e-18, 4.395464816576839e-37}\n",
      "{0.0, 1, 0.11249999999999999, 0.5, 0.00013785497656465398}\n",
      "{0.0, 1, 0.0625, 1.335113484646195e-05}\n",
      "{0.0, 1, 0.6666666666666666, 0.25, 0.18181818181818182, 1.2445701891786512e-08, 2.6812378510728915e-33, 3.9601258743759824e-29}\n",
      "{0.0, 1, 0.15, 1.8658130909044173e-31, 9.883317793859977e-29, 5.34045393858478e-05}\n",
      "{0.0, 1, 8.124554194958235e-05, 0.05357142857142857, 4.5274217380732293e-26}\n",
      "{0.0, 1, 0.03225806451612903, 0.0011870061779317678, 4.712873775774932e-05}\n",
      "{0.0, 1, 1.9539598389138413e-30, 1.0591933640241144e-20}\n",
      "{0.0, 1, 1.4270122454774454e-27}\n",
      "{0.0, 1, 5.707182208483858e-22, 1.8735711428921248e-36, 1.0742346591817731e-06, 4.1717538103756356e-08}\n",
      "{0.0, 1, 4.46309536344816e-20, 5.978048750456472e-25}\n",
      "{0.0, 1.5256074450756157e-27, 1.58145047614693e-39}\n",
      "{0.0, 1}\n",
      "{0.0, 1.5952632282364288e-18, 2.4748704462194932e-39}\n",
      "{0.0, 4.6356965895603524e-29}\n",
      "{0.0, 2.0215522380034128e-36}\n",
      "{0.0, 6.557360938456128e-44, 8.555514113287031e-51}\n",
      "{0.0, 3.4039199260125777e-16, 2.5782642347080755e-18, 1}\n",
      "{0.0, 0.5, 1.3248442540511614e-20}\n",
      "{0.0, 2.6058714018805172e-51}\n",
      "{0.0, 1.8850172739418245e-55, 1.0057465438359382e-47}\n",
      "{0.0, 1.3163125746788998e-27, 6.721008190128409e-42}\n",
      "{0.0, 1.1834775755548883e-50}\n",
      "{0.0, 2.430483802770062e-25, 2.0321155985814356e-11, 1.2121695443959027e-28}\n",
      "{0.0, 1, 8.230318999156791e-41, 4.6455684856943e-31}\n",
      "{0.0, 1, 7.256752240350744e-31}\n",
      "{0.0, 3.632967621138678e-18, 6.0303097842499885e-09}\n",
      "{0.0, 1.5549308942414936e-23, 3.962867833794523e-21}\n",
      "{0.0, 1, 6.83135461962116e-24, 1.9006484692297213e-31}\n",
      "{0.0, 1.2770457857276354e-44}\n",
      "{0.0, 6.230786010380974e-37, 0.06926589392342818}\n",
      "{0.0, 3.1515525244382654e-29, 1}\n",
      "{0.0, 3.4342455003532483e-29, 5.310882112996194e-35}\n",
      "{0.0, 2.2567740851852106e-15, 6.0702521830862255e-27}\n",
      "{0.0, 1, 0.000949667616334283}\n",
      "{0.0, 4.2779390538011914e-41}\n",
      "{0.0, 1}\n",
      "{0.0, 5.405665683811177e-38, 1.9994645252428407e-14, 1}\n",
      "{0.0, 1}\n",
      "{0.0, 3.049306582251997e-32}\n",
      "{0.0, 3.778451181716861e-33, 1}\n",
      "{0.0, 5.0046754489438696e-33, 1.0897134533416692e-20}\n",
      "{0.0, 2.564127486649778e-45}\n",
      "{0.0, 2.680845694453549e-35}\n",
      "{0.0, 2.986129785603778e-20, 2.028285255618954e-15}\n",
      "{0.0, 1, 4.900985381427458e-10, 5.13645292908475e-28}\n",
      "{0.0, 1.3356070158068408e-29}\n",
      "{0.0, 1, 1.3940899297041085e-27, 9.986090139312556e-17}\n",
      "{0.0, 2.7551387329830964e-32, 6.0303097842499885e-09, 4.4372244435906977e-45}\n",
      "{0.0, 1, 5.875261317243174e-57}\n",
      "{0.0, 1, 9.107077967999998e-34, 6.0303097842499885e-09, 6.7468137626312e-44}\n",
      "{0.0, 1, 5.347019153955133e-36}\n",
      "{0.0}\n",
      "{0.0, 1, 3.6332743641321e-21}\n",
      "{0.0, 2.071115840210365e-18, 2.977251416985978e-38}\n",
      "{0.0, 3.695185962129852e-38, 1.5415802899253295e-46, 1}\n",
      "{0.0, 7.874500243613241e-39}\n",
      "{0.0, 7.414645380040786e-33}\n",
      "{0.0, 0.2, 0.1891891891891892, 8.364170131664468e-33}\n",
      "{0.0, 8.194867226054208e-16, 2.9780068460198496e-20}\n",
      "{0.0, 1, 3.819678545882696e-29, 8.044647070440568e-12}\n",
      "{0.0, 1, 1.3723304449188768e-21, 1.7755642983360247e-25, 1.1408724715221901e-26}\n",
      "{0.0, 2.3880173816083614e-26, 9.212730838330243e-31}\n",
      "{0.0, 2.997105130242757e-62}\n",
      "{0.0}\n",
      "{0.0, 1, 3.821350895110924e-69}\n",
      "{0.0, 5.094433517088327e-12, 3.260669955508811e-09}\n",
      "{0.0, 7.762656134183555e-18, 4.2644673629907464e-36}\n",
      "{0.0, 2.833023282492462e-31, 2.674499043589138e-35}\n",
      "{0.0}\n",
      "{0.0, 1.2989241827615335e-30, 8.322421235778037e-26, 1}\n",
      "{0.0, 4.902443911026036e-33, 1, 6.635141676965381e-18}\n",
      "{0.0, 9.14441805073421e-23, 4.98823072788029e-16, 1}\n",
      "{0.0, 1, 6.969801629014271e-29, 2.861531162642102e-21}\n",
      "{0.0, 0.0005498533724340176, 1.9845361623508013e-41}\n",
      "{0.0, 4.317471104700648e-68, 1.7505362912741274e-32}\n",
      "{0.0, 1.3442270207744939e-33, 3.235813989232829e-05}\n",
      "{0.0, 2.4526318058879303e-34, 9.294926616605273e-29}\n",
      "{0.0}\n",
      "{0.0, 1, 2.163792665872594e-22, 1.0017954049852125e-31, 0.00010013351134846461}\n",
      "{0.0, 0.043478260869565216, 1, 7.274227587251063e-50, 7.220465921131643e-15}\n",
      "{0.0, 1, 7.048110401601329e-06, 3.9029721132642505e-07, 2.960110199688449e-35}\n",
      "{0.0, 1, 0.75, 1.3296935597810265e-31, 1.1445794567513677e-30}\n",
      "{0.0, 1, 0.02857142857142857, 1.91888006282853e-18}\n",
      "{0.0, 6.127200766535521e-62, 3.261194488777097e-89, 1}\n",
      "{0.0, 2.0856454636333908e-38, 3.052405230999719e-24, 6.064532756483063e-24}\n",
      "{0.0, 2.5708185676479396e-25, 1}\n",
      "{0.0, 1, 0.17391304347826086, 2.970952478465592e-22, 1.2495489606568377e-29, 0.0031531531531531535, 5.34045393858478e-05}\n",
      "{0.0, 6.442396843210159e-38, 0.047619047619047616, 1, 8.019856158709349e-51}\n",
      "{0.0, 1, 0.5714285714285714, 1.1375957139432258e-43, 0.020560747663551402}\n",
      "{0.0, 0.0625, 2.1847553670209574e-55, 1, 0.0027223230490018148, 2.574459640822255e-62, 3.788598486284093e-08}\n",
      "{0.0, 1, 9.123830914124223e-07, 1.6288058074073926e-39, 5.686860829768871e-23}\n",
      "{0.0, 1, 3.130776283272741e-33, 0.014336917562724014, 1.9963395879355866e-16}\n",
      "{0.0, 1, 9.227943684205663e-42, 0.05555555555555555}\n",
      "{0.0, 1, 1.70278030725189e-45}\n",
      "{0.0, 1, 3.565287363944177e-07}\n",
      "{0.0, 1, 0.22033898305084745, 0.1111111111111111, 0.00031595576619273305, 4.1797923740405455e-31}\n",
      "{0.0, 2.3491527565405836e-34, 3.073887794649956e-45, 1}\n",
      "{0.0, 1, 1.589286178758327e-30}\n",
      "{0.0, 5.573189598153105e-29, 1.7956946389892448e-46}\n",
      "{0.0, 1, 0.05263157894736842, 0.02857142857142857, 1.0677379978485631e-65}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 7.288300363928051e-118, 5.930584581949698e-13}\n",
      "{0.0, 1.1147897021347439e-07}\n",
      "{0.0, 1, 1.3967257032762717e-43, 1.0218296263345291e-57, 1.1498800885393085e-27, 5.942275042444821e-05}\n",
      "{0.0, 1}\n",
      "{0.0, 8.851277781092425e-10, 7.538452748244847e-50, 1.4179369018078695e-05}\n",
      "{0.0, 2.6241416045095135e-74, 2.2220726859890914e-08, 1}\n",
      "{0.0, 1, 4.834102328371414e-41}\n",
      "{0.0, 1.0534869942520315e-21}\n",
      "{0.0, 1}\n",
      "{0.0, 4.327619909247241e-23, 3.3483013554086504e-66, 1.0578756430136748e-15}\n",
      "{0.0, 6.3813832489471234e-40, 8.450781051798899e-46}\n",
      "{0.0, 1}\n",
      "{0.0, 1.8943949278082906e-51, 1, 0.014336917562724014}\n",
      "{0.0, 4.232043914544054e-18, 1}\n",
      "{0.0, 8.727629295968249e-27}\n",
      "{0.0, 1, 0.01}\n",
      "{0.0, 1.0580460006896627e-89, 2.8769549235278522e-58, 4.653540340814461e-39}\n",
      "{0.0, 1, 2.457735381288401e-35}\n",
      "{0.0, 3.1502996687422273e-19, 0.2, 1, 0.00035106196243636997, 3.786743968139554e-09}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 2.4213904647047216e-33, 7.909193947314006e-20}\n",
      "{0.0, 1, 0.00014693542073465412, 0.014336917562724014}\n",
      "{0.0, 1, 2.1925132908878667e-27}\n",
      "{0.0, 4.839413220808883e-27, 1, 1.6690607394249001e-53, 4.211705972810367e-31}\n",
      "{0.0, 3.2852964878598496e-33}\n",
      "{0.0, 2.276395364276391e-32, 6.048867030719408e-34, 1}\n",
      "{0.0}\n",
      "{0.0, 1.1469313362974174e-23}\n",
      "{0.0, 6.711947343157447e-33}\n",
      "{0.0, 1.0692998767340017e-42, 9.897463484471095e-53, 3.856692588224055e-24}\n",
      "{0.0, 4.369294668021833e-43}\n",
      "{0.0, 6.495897495948558e-14, 7.55813529064393e-44}\n",
      "{0.0, 1, 5.905525392866936e-26}\n",
      "{0.0, 1.170722886597842e-33}\n",
      "{0.0, 7.141133633907976e-39, 6.839183524822609e-30, 9.21113464795898e-32}\n",
      "{0.0, 2.1813226811613172e-51}\n",
      "{0.0, 1.4310803494660488e-43, 9.625402954560191e-27}\n",
      "{0.0, 1, 4.0971594807415474e-36}\n",
      "{0.0, 1, 2.4571961342846773e-25}\n",
      "{0.0, 6.023567853200978e-25, 4.1366169878318114e-26, 1}\n",
      "{0.0, 8.007287037638349e-38, 1.713160738004083e-29}\n",
      "{0.0, 1.424814913089856e-32}\n",
      "{0.0, 2.9936843692660783e-32}\n",
      "{0.0, 1, 4.145347275127171e-41, 5.356579889390417e-51}\n",
      "{0.0, 2.5929253733046017e-28, 1.4647209371299142e-20}\n",
      "{0.0, 3.8715203098574393e-28, 4.684025921421761e-28}\n",
      "{0.0, 9.383280937582674e-66}\n",
      "{0.0, 2.8549296639531094e-29}\n",
      "{0.0, 9.699898333322255e-20}\n",
      "{0.0, 5.288475126554023e-44}\n",
      "{0.0, 1, 2.9438345437015077e-12, 0.014336917562724014}\n",
      "{0.0}\n",
      "{0.0, 1, 1.8830007132708782e-10, 6.597649942917495e-29, 0.00016977928692699492, 0.05454545454545454}\n",
      "{0.0, 1.0}\n",
      "{0.0, 1.5818752893447372e-39}\n",
      "{0.0, 1.597971190474742e-29, 8.261538146927087e-21}\n",
      "{0.0, 3.748223789505694e-21}\n",
      "{0.0, 4.571703204695194e-26}\n",
      "{0.0, 0.03655868001408522, 8.919040285263748e-45}\n",
      "{0.0, 7.677389057553605e-45, 1.0937501966477322e-18}\n",
      "{0.0, 1.0517016994536894e-12, 1.8821028866392848e-27}\n",
      "{0.0}\n",
      "{0.0, 6.924525829192318e-50, 5.475432351550372e-13}\n",
      "{0.0, 1, 1.025958660122606e-26, 2.8288348117375155e-22}\n",
      "{0.0, 1, 2.2828102683326337e-40}\n",
      "{0.0, 1.188347580328044e-28}\n",
      "{0.0, 4.004089964315376e-69, 1, 1.0118174531443029e-20}\n",
      "{0.0, 1, 1.2063116500663756e-27, 2.5332544651393892e-27}\n",
      "{0.0, 6.424052452173656e-19, 1.244278494554065e-27, 1}\n",
      "{0.0, 1.9979688317193915e-16, 3.643810232060622e-48}\n",
      "{0.0, 1, 0.00980392156862745, 4.681554016802481e-45}\n",
      "{0.0, 3.3954760216695708e-22, 1.7846347316105174e-44}\n",
      "{0.0, 2.3192657330166282e-10, 1.3398553663109128e-35}\n",
      "{0.0, 0.5, 9.206722859796088e-12, 5.032766428982997e-57, 1.8962925115960655e-08}\n",
      "{0.0}\n",
      "{0.0, 1.4970084176596386e-11}\n",
      "{0.0, 1}\n",
      "{0.0, 5.642106577624592e-38, 8.227443745109565e-25}\n",
      "{0.0, 1.6544933011739292e-40}\n",
      "{0.0, 1, 1.5093713614756466e-26, 7.825448558865257e-35}\n",
      "{0.0, 1.6503224316949037e-20, 4.6954675505294004e-23, 1.1877523186045675e-27}\n",
      "{0.0, 1.1212127655872737e-05, 1.0389083534519309e-19, 2.3394829328261147e-42}\n",
      "{0.0, 7.191996786242132e-19, 9.666587502209064e-33, 3.7085406687000986e-14}\n",
      "{0.0, 5.656680724625147e-40, 2.479099113321801e-35, 7.339374365229396e-43}\n",
      "{0.0, 1, 3.1938807131304095e-18}\n",
      "{0.0}\n",
      "{0.0, 5.7163136153308347e-39}\n",
      "{0.0, 4.0628701404460173e-51, 2.499163843750825e-12}\n",
      "{0.0, 3.057200116634596e-40, 8.425340102201467e-16}\n",
      "{0.0}\n",
      "{0.0, 6.260479949801736e-19}\n",
      "{0.0, 1, 0.0016360432922225018, 6.932249107443309e-26, 1.4613378515371417e-36}\n",
      "{0.0, 4.070348269125492e-40, 0.42857142857142855}\n",
      "{0.0}\n",
      "{0.0, 1.7001073538514035e-30, 1.4679539952128437e-26, 1}\n",
      "{0.0, 9.374166815661508e-34, 5.96015370534084e-38}\n",
      "{0.0, 1, 1.3421719539554212e-48, 2.3455841015376066e-36}\n",
      "{0.0, 1, 2.9592624967088615e-29, 3.392879554365981e-24}\n",
      "{0.0, 1.230253018802817e-32, 1.6263672816998144e-63}\n",
      "{0.0, 1.4797193687867038e-19, 9.516939305382196e-25}\n",
      "{0.0, 2.518061995032717e-40}\n",
      "{0.0, 2.1721047215456036e-17, 1.9461430417846328e-16}\n",
      "{0.0, 2.1317316385145082e-40}\n",
      "{0.0, 1.7115461687227392e-43, 4.376889834137349e-34}\n",
      "{0.0}\n",
      "{0.0, 1.058511036445338e-39}\n",
      "{0.0, 3.89045484666584e-26}\n",
      "{0.0, 1.3809308229607874e-55, 1}\n",
      "{0.0, 1, 3.3868646200054603e-29}\n",
      "{0.0, 1, 5.186103405767074e-42}\n",
      "{0.0, 3.234858642710046e-16, 1.3103708557081774e-17}\n",
      "{0.0, 1.9788013941331056e-19}\n",
      "{0.0}\n",
      "{0.0, 6.633208286185692e-49, 1, 5.929788786043953e-28, 5.34045393858478e-05}\n",
      "{0.0, 8.96812710574414e-25}\n",
      "{0.0, 1.4156939146845142e-26, 6.368110448717798e-57}\n",
      "{0.0, 1.9326569723834723e-17, 3.926788207902336e-24}\n",
      "{0.0}\n",
      "{0.0, 7.612905224639867e-31, 1.298338771749159e-21}\n",
      "{0.0, 1.3486088254113744e-29}\n",
      "{0.0, 1, 8.459492166528377e-28}\n",
      "{0.0, 1, 1.4813006213483554e-36, 1.4972692929110807e-35}\n",
      "{0.0, 1, 7.424829412064267e-52}\n",
      "{0.0, 2.9904994646429454e-26}\n",
      "{0.0, 5.551868317410612e-13}\n",
      "{0.0, 2.5628273707376366e-17, 1.5977128282227603e-46}\n",
      "{0.0, 1}\n",
      "{0.0, 3.969275130241171e-28}\n",
      "{0.0, 1.8441855023839278e-21, 3.25051705446675e-13, 3.780969829499111e-16}\n",
      "{0.0, 4.6678058572939605e-27, 7.013033501022012e-45, 1}\n",
      "{0.0, 2.219209467563501e-19, 1}\n",
      "{0.0, 1, 2.040616236796929e-36}\n",
      "{0.0, 2.785987251644748e-19, 7.269530915684044e-24, 1.9727572557836167e-31}\n",
      "{0.0, 3.945691242761849e-42}\n",
      "{0.0, 7.132310432265867e-19, 2.834088467560412e-20, 1, 1.0166680696686025e-08}\n",
      "{0.0, 1.0007290855848494e-18, 1.4299842540834358e-13}\n",
      "{0.0, 1, 1.4286718233438847e-18}\n",
      "{0.0, 0.0273972602739726, 1.584133332486297e-29}\n",
      "{0.0, 1, 4.148086706285847e-21}\n",
      "{0.0, 7.003909349984759e-17, 5.3779670934143025e-11, 1.1787373524284748e-10, 2.805667872425499e-23}\n",
      "{0.0, 7.278439827539761e-20, 1.970171753695093e-28}\n",
      "{0.0, 5.091448542108755e-49, 8.496328434658136e-35, 2.53930937932076e-29}\n",
      "{0.0, 1.993366891077158e-27}\n",
      "{0.0, 5.447272497158473e-32}\n",
      "{0.0}\n",
      "{0.0, 1, 1.0637790362732987e-16}\n",
      "{0.0, 4.80856815890489e-24, 1.1690302082062078e-16}\n",
      "{0.0, 3.6443394940180294e-28, 1.3715743377404795e-39}\n",
      "{0.0, 1.919102522439684e-20}\n",
      "{0.0, 1.9955203821825808e-48}\n",
      "{0.0, 2.7730957526887993e-40, 4.2638967942141726e-15}\n",
      "{0.0, 7.269643946403933e-50}\n",
      "{0.0, 1, 6.875540884622994e-50}\n",
      "{0.0, 1.4785754082236715e-37, 7.006888645792618e-49}\n",
      "{0.0, 1.266294911461594e-19, 1.6093508742443597e-23}\n",
      "{0.0, 4.713860915619572e-27, 6.756910492697145e-44}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 1.0296900444303357e-18, 3.6456436174347674e-60}\n",
      "{0.0, 4.5437409997565586e-26, 2.3272755625781114e-41}\n",
      "{0.0, 4.725295399389174e-37, 6.25192637481424e-06}\n",
      "{0.0, 3.3888777343083603e-16, 1.1005637858678046e-19}\n",
      "{0.0, 5.335299448857913e-12, 3.287376991340694e-29}\n",
      "{0.0, 9.413579547891597e-55}\n",
      "{0.0, 9.07107909893865e-30, 9.366500449436438e-27}\n",
      "{0.0, 1, 2.2712714035815427e-44}\n",
      "{0.0, 1, 1.9258249889432272e-23}\n",
      "{0.0, 1, 2.1265673799157885e-29}\n",
      "{0.0, 3.0718127311094324e-19, 5.690833333824402e-15, 1}\n",
      "{0.0, 1, 3.894583927964741e-27}\n",
      "{0.0, 1, 1.0239648637512816e-33}\n",
      "{0.0, 1.2068369764406642e-17, 4.2482737074569437e-36}\n",
      "{0.0, 1.5179600301082269e-35}\n",
      "{0.0, 9.899048345274183e-32, 8.826215809379341e-29, 6.8241069852869136e-15}\n",
      "{0.0, 9.969335697076013e-29}\n",
      "{0.0, 0.06666666666666667, 0.125, 0.07758620689655173, 6.657662784442605e-21, 1.7559699797961445e-20, 0.017857142857142856}\n",
      "{0.0, 1.550847732767391e-22}\n",
      "{0.0, 1.155912532232463e-45, 2.6547474606372184e-08, 4.625240112158856e-22}\n",
      "{0.0, 2.5223399611820675e-31, 4.4610861034709713e-29, 5.586759927576387e-17}\n",
      "{0.0, 9.945229850138403e-44, 0.013157894736842105, 1}\n",
      "{0.0, 2.9893868504787425e-47}\n",
      "{0.0, 1.0321412472927998e-15, 1.2089153374770023e-21}\n",
      "{0.0, 1, 1.0090855658541152e-19, 7.911819302798673e-26, 9.457174902140073e-45}\n",
      "{0.0, 6.01207004202298e-12, 1, 1.4452537865649207e-05}\n",
      "{0.0, 6.702141393411868e-28}\n",
      "{0.0, 3.109321337126842e-38}\n",
      "{0.0, 4.097319929374715e-24, 1.3611321214051689e-30, 5.9190258937914197e-36}\n",
      "{0.0, 2.8846265549955044e-27, 6.3311173539247815e-16, 7.828199837441005e-24}\n",
      "{0.0, 6.217548792744614e-33, 1.2866733560263078e-25}\n",
      "{0.0, 2.824715179554185e-16, 1.891328150928682e-34, 1.922286617559405e-45, 6.621238518673463e-43}\n",
      "{0.0, 1.4351532313697602e-24}\n",
      "{0.0, 1.051004553632375e-37, 1}\n",
      "{0.0, 2.283552640427321e-24, 3.6919613384244466e-26}\n",
      "{0.0, 3.6838983377053313e-48}\n",
      "{0.0}\n",
      "{0.0, 2.1272984155729344e-26, 3.971713305082995e-09}\n",
      "{0.0, 6.624115212717897e-39}\n",
      "{0.0, 2.978555721635616e-56, 2.9918049089947896e-33, 1}\n",
      "{0.0}\n",
      "{0.0, 1, 1.4612585233207369e-36}\n",
      "{0.0, 1, 7.1684826190480385e-34}\n",
      "{0.0, 3.258981341151528e-38, 1.1316253134745783e-12, 1.0816975458252128e-27}\n",
      "{0.0, 1.370064388314456e-35, 4.546758874541256e-70, 7.172875804899321e-42}\n",
      "{0.0, 9.698348892554628e-25}\n",
      "{0.0, 9.629662566264076e-31}\n",
      "{0.0, 0.375, 1.332393995566459e-05, 1, 2.0438400457719065e-20, 8.601282670673042e-16}\n",
      "{0.0, 1, 9.76694900373343e-23, 2.3412457825833693e-27}\n",
      "{0.0, 1, 6.196033495938999e-60}\n",
      "{0.0, 1, 3.725657686920773e-17}\n",
      "{0.0, 1, 4.161259954079021e-35, 1.485036223807246e-25}\n",
      "{0.0, 1.883342745658756e-22, 0.2, 3.591756636771452e-30}\n",
      "{0.0, 8.046922435650789e-24, 1.9068979654545567e-47, 3.3904872614550026e-06, 1.2240153251838458e-20}\n",
      "{0.0, 1, 1.7094446752008738e-23, 8.483353791959128e-46}\n",
      "{0.0, 1.0438867993976132e-35}\n",
      "{0.0, 1.8821028866392848e-27}\n",
      "{0.0, 1, 3.7183000033703104e-28, 1.326356939636161e-38}\n",
      "{0.0, 6.126384576632592e-54, 4.085043984863776e-36}\n",
      "{0.0, 1.0560442272916686e-25, 6.110631658618853e-30, 1}\n",
      "{0.0, 1}\n",
      "{0.0, 5.2537485447181905e-77, 0.2, 1, 2.9437455065967685e-38}\n",
      "{0.0, 1, 4.053220522467827e-33}\n",
      "{0.0, 6.014509698466768e-43}\n",
      "{0.0}\n",
      "{0.0, 3.326897594704485e-27, 4.403675687070391e-22}\n",
      "{0.0, 1, 1.8226988411998352e-45}\n",
      "{0.0, 3.596607587491393e-37}\n",
      "{0.0, 1, 3.506340859489966e-24, 8.673927370327611e-17}\n",
      "{0.0, 1.3048156982748724e-39, 2.839332127979847e-25}\n",
      "{0.0, 1, 1.4755601734841626e-32}\n",
      "{0.0, 1, 1.841444200423388e-23, 4.349731647655729e-08, 3.7283018220280557e-32, 2.6547474606372184e-08}\n",
      "{0.0, 1, 1.782926958772994e-23}\n",
      "{0.0, 3.771084366107947e-42}\n",
      "{0.0, 1, 0.05}\n",
      "{0.0, 4.603094581070444e-33}\n",
      "{0.0, 1, 1.8963177117441596e-55}\n",
      "{0.0, 1.8068054808666446e-50, 6.027410731066952e-14}\n",
      "{0.0, 5.134539988144358e-17, 4.849513319980373e-68}\n",
      "{0.0, 1, 5.768906049638191e-21, 0.3333333333333333}\n",
      "{0.0}\n",
      "{0.0}\n",
      "{0.0, 2.8367025996991704e-32, 1.3576184573551107e-25}\n",
      "{0.0, 1, 7.539977955266517e-30}\n",
      "{0.0, 7.599009002440862e-29}\n",
      "{0.0, 1, 5.568124405773555e-31, 5.928286078441275e-43}\n",
      "{0.0, 2.5047773530824904e-32, 3.5148920061514526e-34, 4.201558131899141e-16}\n",
      "{0.0, 3.8046033667524426e-22, 1.1451680843749813e-30, 1.8872957534446066e-22}\n",
      "{0.0, 7.210230707000915e-30, 1.543662962445111e-42}\n",
      "{0.0, 3.1942393346593536e-40}\n",
      "{0.0, 3.6816361130881636e-15, 7.4804240286972615e-31, 5.9050547890425296e-12}\n",
      "{0.0, 1, 4.279304745075357e-37, 3.911713706036364e-29, 1.257176616057611e-33}\n",
      "{0.0, 1, 2.3976528915313295e-34, 1.6313639908666323e-31}\n",
      "{0.0, 1.203272451119403e-29, 2.0742738800058912e-32}\n",
      "{0.0, 1, 2.9210350885462694e-21, 2.0774111891764365e-33}\n",
      "{0.0, 2.6418843078361136e-28, 4.2998286122586186e-41}\n",
      "{0.0, 2.047087299594171e-35}\n",
      "{0.0, 1, 1.3218955322319222e-20, 2.2567906332895404e-26}\n",
      "{0.0, 2.5470263240699948e-23, 2.8511904766524037e-33}\n",
      "{0.0, 1.7546394282133075e-31, 1.0644079230927796e-05}\n",
      "{0.0, 6.919006095343235e-41, 8.491636129134914e-65, 0.0011741682974559685}\n",
      "{0.0, 3.3303095365969203e-41}\n",
      "{0.0, 2.245039206184029e-42, 5.403200300512779e-24}\n",
      "{0.0, 1, 1.4362017179530837e-18, 3.4299325372628233e-32}\n",
      "{0.0, 1.0869316483008551e-34, 1, 1.0280731023363039e-32, 7.929173830486438e-10}\n",
      "{0.0, 2.695654633960792e-32}\n",
      "{0.0, 1, 1.0539415180318714e-81}\n",
      "{0.0, 3.7311666577751156e-22, 4.958775648354802e-34}\n",
      "{0.0, 9.30150477337072e-42, 1.1764902115237407e-15}\n",
      "{0.0, 4.212423202360753e-09, 1.5138614629702183e-35, 7.829345619243253e-23, 1.5449961614916416e-39}\n",
      "{0.0, 3.581983793345688e-36, 0.0007048739285668215, 2.6285372704535e-43}\n",
      "{0.0, 1.582118599315973e-24, 3.8383839065286677e-25}\n",
      "{0.0, 4.416085841948066e-39}\n",
      "{1.4738170414415348e-39, 0.0, 6.424250450411723e-13, 3.1525934925872185e-14}\n",
      "{0.0, 1.0424685328012416e-45}\n",
      "{0.0, 9.592729402947352e-42}\n",
      "{0.0, 2.2799904233745006e-31, 2.3038660837301967e-32, 9.794303685567746e-43}\n",
      "{0.0, 1, 9.839523990086152e-17, 0.0469208211143695, 4.663637118671752e-16}\n",
      "{0.0, 9.660178276501245e-48, 7.548666009038855e-31}\n",
      "{0.0, 7.81685997912076e-45, 6.422595968216106e-62}\n",
      "{0.0, 1}\n",
      "{0.0, 3.213344409243054e-19, 4.058903509706054e-21, 1}\n",
      "{0.0, 2.2953559193101762e-35, 6.037532122526136e-44}\n",
      "{0.0, 1, 2.0678172900482805e-21, 1.4609313814422457e-26, 2.595053052752559e-31}\n",
      "{0.0, 5.852626590404906e-14, 1.5442807825285636e-26, 2.6144977076836552e-33}\n",
      "{0.0, 1, 7.888834240439088e-26}\n",
      "{0.0, 1.1665349136268869e-55}\n",
      "{0.0, 7.719751651257789e-25}\n",
      "{0.0, 4.064486636496263e-49}\n",
      "{0.0, 1, 5.367285936352893e-49, 5.822644847410909e-46, 8.04897495530409e-15}\n",
      "{0.0, 1.673383494220954e-23, 3.092040780595271e-54}\n",
      "{0.0, 1, 0.17391304347826086, 6.110575906905792e-44, 0.0001869158878504673}\n",
      "{0.0, 9.311929709844701e-40}\n",
      "{0.0, 6.668178165011771e-11, 2.1878857680803628e-17}\n",
      "{0.0}\n",
      "{0.0, 2.2119239610461743e-24, 6.340547565064919e-22, 1.648682271623749e-27}\n",
      "{0.0, 1.1269658438653834e-28}\n",
      "{0.0, 2.8816739598771805e-33, 7.475633600433938e-32}\n",
      "{0.0}\n",
      "{0.0}\n",
      "{0.0, 2.4998083979323587e-47}\n",
      "{0.0, 1}\n",
      "{0.0, 2.7215596435013193e-56}\n",
      "{0.0, 5.235298603281662e-28}\n",
      "{0.0, 1, 3.4657147817971507e-41}\n",
      "{0.0, 1.4823050272009798e-26, 1}\n",
      "{0.0, 3.414087065836355e-31}\n",
      "{0.0, 1, 2.2523159892753324e-35}\n",
      "{0.0, 6.320712924184003e-41, 1.988714732283848e-24, 1}\n",
      "{0.042735042735042736, 0.0, 7.96946148417341e-21, 1.0392238722905883e-16}\n",
      "{0.0, 8.264135930057277e-22}\n",
      "{0.0, 5.22741730023789e-36, 3.740817819849728e-21, 2.090167697147873e-34}\n",
      "{0.0, 1, 5.272947529324485e-23}\n",
      "{0.0, 6.303135772156277e-33, 1, 3.841218286152079e-67}\n",
      "{0.0, 2.631856863147355e-32}\n",
      "{0.0, 1.9374038325893885e-31, 2.0275151175289167e-30}\n",
      "{0.0, 1, 6.541433589812441e-26}\n",
      "{0.0, 6.880630489391461e-33, 6.862267888836796e-18}\n",
      "{0.0, 1.2756382433796416e-27}\n",
      "{0.0, 1.6644853999945205e-36, 1}\n",
      "{0.0, 6.817656091166871e-70, 1}\n",
      "{0.0, 5.888807199795138e-40, 2.1488348666588063e-44}\n",
      "{0.0, 1, 7.200665510086388e-16}\n",
      "{0.0, 3.224916998363715e-34, 5.6251852711694404e-24}\n",
      "{0.0, 1, 1.9112565774701696e-18, 5.717529783753831e-30, 4.1338194875798137e-16}\n",
      "{0.0, 8.727629295968249e-27}\n",
      "{0.0, 1, 5.751307522913622e-37, 3.753691040248239e-21, 9.938723352557316e-45}\n",
      "{0.0, 0.125, 6.1101367009800354e-30}\n",
      "{0.0, 9.353797784678939e-39}\n",
      "{0.0, 3.1502996687422273e-19}\n",
      "{0.0, 1.252514325622368e-23, 2.253863051398131e-44}\n",
      "{0.0, 3.646362450519353e-42}\n",
      "{0.0, 1, 1.4079051292925125e-26}\n",
      "{0.0}\n",
      "{0.0, 1.8631346665884445e-20}\n",
      "{0.0, 3.667062858877272e-20}\n",
      "{0.0, 2.457076807962594e-37, 2.449483128836293e-27, 1.9118382088406034e-22}\n",
      "{0.0, 1.545652679066915e-29}\n",
      "{0.0, 1, 1.9505378419572683e-41}\n",
      "{0.0, 3.5453968404521353e-22, 6.865011536085767e-20}\n",
      "{0.0, 2.057216385514002e-22, 3.243572909488594e-29, 1.38398487382114e-21}\n",
      "{0.0, 1, 1.496647509578544e-05, 3.147945832104675e-46}\n",
      "{0.0, 9.843193228995955e-32, 0.00546448087431694, 1.2853280338752936e-34}\n",
      "{0.0, 2.269631132946856e-43, 8.936392746306113e-71, 1}\n",
      "{0.0, 4.360880681646643e-75}\n"
     ]
    }
   ],
   "source": [
    "for i in lista_paper_prob:\n",
    "    print(set(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfed9d5",
   "metadata": {},
   "source": [
    "## Trigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "41ac47d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:41:00.140082Z",
     "start_time": "2023-03-03T02:40:59.423610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 3190 items>\n"
     ]
    }
   ],
   "source": [
    "#Trigrama\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, frase_objetivo_ft_formato_ml)\n",
    "\n",
    "model_tri = MLE(n)\n",
    "model_tri.fit(train_data, padded_sents)\n",
    "print(model_tri.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d56e3708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:41:02.257322Z",
     "start_time": "2023-03-03T02:41:00.140082Z"
    }
   },
   "outputs": [],
   "source": [
    "#Criando os bigramas da lista\n",
    "trigramas=[]\n",
    "for papers in list(palavras_papers_treino.values()):\n",
    "    x=[]\n",
    "    for frases in papers:\n",
    "        x.append(trigrams(frases))\n",
    "    trigramas.append(x)\n",
    "    \n",
    "lista_trigramas2=[]\n",
    "for j in trigramas:\n",
    "    lista_trigramas=[]  \n",
    "    for i in j:\n",
    "        lista_trigramas.append(list(i))\n",
    "    lista_trigramas2.append(lista_trigramas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a63d9602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:41:46.755914Z",
     "start_time": "2023-03-03T02:41:02.257322Z"
    }
   },
   "outputs": [],
   "source": [
    "# Probabilidade de cada frase do paper, com base nos bigramas\n",
    "lista_paper_prob_tri=[]\n",
    "lista_paper_perp_tri=[]\n",
    "for paper in lista_trigramas2:\n",
    "    lista_prob_tri=[]\n",
    "    lista_perp_tri=[]\n",
    "    for frase in paper:\n",
    "        prob=1\n",
    "        for trigrama in frase:\n",
    "            prob = prob * (model_tri.score(trigrama[2], context=[trigrama[1],trigrama[0]]))\n",
    "        lista_prob_tri.append(prob)\n",
    "        try:\n",
    "            lista_perp_tri.append(model_tri.perplexity(frase))\n",
    "        except:\n",
    "            lista_perp_tri.append(math.inf)\n",
    "    lista_paper_prob_tri.append(lista_prob_tri)\n",
    "    lista_paper_perp_tri.append(lista_perp_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45b9e5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-03T14:15:34.511Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_paper_perp_tri_filtrada=[]\n",
    "for paper in lista_trigramas2:\n",
    "    lista_prob_tri=[]\n",
    "    lista_perp_tri=[]\n",
    "    for frase in paper:\n",
    "        prob=1\n",
    "        for trigrama in frase:\n",
    "            tri=model_tri.score(trigrama[2], context=[trigrama[0],trigrama[1]])\n",
    "            if tri==0:\n",
    "                lista_perp_tri.append(model_tri.perplexity(frase)) \n",
    "    lista_paper_perp_tri_filtrada.append(lista_perp_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3c197",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-03T14:15:35.178Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_paper_perp_tri_filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b771a5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-03T14:15:35.605Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in lista_paper_perp:\n",
    "    print(set(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e178e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-03T14:15:36.157Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in lista_paper_prob:\n",
    "    print(set(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5663f",
   "metadata": {},
   "source": [
    "[1] Validar se não é infinita a perplexidade onde existe a frase objetivo\n",
    "\n",
    "[2] Tirar a média da perplexidade de todos os papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edff9bf",
   "metadata": {},
   "source": [
    "## Conclusão Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6ccac7af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:41:46.819535Z",
     "start_time": "2023-03-03T02:41:46.791254Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_contador_verdadeiro=[]\n",
    "lista_contador_falso=[]\n",
    "for perplexidades_bi_paper, perplexidades_tri_paper in zip(lista_paper_perp,lista_paper_perp_tri):\n",
    "    contador_positivo=0\n",
    "    contador_negativo=0\n",
    "    for p_bi, p_tri in zip(perplexidades_bi_paper, perplexidades_tri_paper):\n",
    "        if p_tri<=p_bi:\n",
    "            contador_positivo=contador_positivo+1\n",
    "        else:\n",
    "            contador_negativo=contador_negativo+1\n",
    "        #lista_contador_negativo.append(contador_negativo)\n",
    "    lista_contador_verdadeiro.append(contador_positivo)\n",
    "    lista_contador_falso.append(contador_negativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1971c6c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T04:57:01.521261Z",
     "start_time": "2023-03-03T04:57:01.505038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_contador_verdadeiro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ac53e6b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:41:46.835210Z",
     "start_time": "2023-03-03T02:41:46.819535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[179,\n",
       " 148,\n",
       " 157,\n",
       " 190,\n",
       " 192,\n",
       " 178,\n",
       " 129,\n",
       " 110,\n",
       " 128,\n",
       " 223,\n",
       " 288,\n",
       " 107,\n",
       " 143,\n",
       " 149,\n",
       " 81,\n",
       " 169,\n",
       " 173,\n",
       " 219,\n",
       " 266,\n",
       " 158,\n",
       " 205,\n",
       " 201,\n",
       " 141,\n",
       " 187,\n",
       " 181,\n",
       " 207,\n",
       " 120,\n",
       " 201,\n",
       " 219,\n",
       " 155,\n",
       " 228,\n",
       " 188,\n",
       " 302,\n",
       " 197,\n",
       " 221,\n",
       " 224,\n",
       " 205,\n",
       " 251,\n",
       " 323,\n",
       " 212,\n",
       " 225,\n",
       " 132,\n",
       " 195,\n",
       " 139,\n",
       " 244,\n",
       " 134,\n",
       " 163,\n",
       " 251,\n",
       " 271,\n",
       " 260,\n",
       " 284,\n",
       " 260,\n",
       " 265,\n",
       " 183,\n",
       " 409,\n",
       " 117,\n",
       " 117,\n",
       " 212,\n",
       " 307,\n",
       " 227,\n",
       " 138,\n",
       " 187,\n",
       " 161,\n",
       " 293,\n",
       " 193,\n",
       " 167,\n",
       " 130,\n",
       " 167,\n",
       " 166,\n",
       " 203,\n",
       " 217,\n",
       " 128,\n",
       " 167,\n",
       " 239,\n",
       " 199,\n",
       " 154,\n",
       " 189,\n",
       " 146,\n",
       " 250,\n",
       " 252,\n",
       " 197,\n",
       " 187,\n",
       " 182,\n",
       " 196,\n",
       " 184,\n",
       " 226,\n",
       " 143,\n",
       " 150,\n",
       " 182,\n",
       " 229,\n",
       " 154,\n",
       " 207,\n",
       " 199,\n",
       " 216,\n",
       " 203,\n",
       " 194,\n",
       " 226,\n",
       " 230,\n",
       " 259,\n",
       " 151,\n",
       " 176,\n",
       " 155,\n",
       " 188,\n",
       " 220,\n",
       " 202,\n",
       " 166,\n",
       " 146,\n",
       " 151,\n",
       " 138,\n",
       " 132,\n",
       " 139,\n",
       " 154,\n",
       " 167,\n",
       " 167,\n",
       " 187,\n",
       " 148,\n",
       " 153,\n",
       " 209,\n",
       " 144,\n",
       " 229,\n",
       " 158,\n",
       " 206,\n",
       " 186,\n",
       " 173,\n",
       " 194,\n",
       " 228,\n",
       " 73,\n",
       " 154,\n",
       " 204,\n",
       " 107,\n",
       " 173,\n",
       " 222,\n",
       " 275,\n",
       " 204,\n",
       " 196,\n",
       " 193,\n",
       " 179,\n",
       " 226,\n",
       " 192,\n",
       " 205,\n",
       " 266,\n",
       " 187,\n",
       " 155,\n",
       " 92,\n",
       " 195,\n",
       " 438,\n",
       " 501,\n",
       " 570,\n",
       " 145,\n",
       " 522,\n",
       " 415,\n",
       " 564,\n",
       " 404,\n",
       " 350,\n",
       " 494,\n",
       " 399,\n",
       " 545,\n",
       " 605,\n",
       " 538,\n",
       " 382,\n",
       " 218,\n",
       " 736,\n",
       " 276,\n",
       " 380,\n",
       " 460,\n",
       " 387,\n",
       " 286,\n",
       " 334,\n",
       " 500,\n",
       " 444,\n",
       " 822,\n",
       " 624,\n",
       " 521,\n",
       " 354,\n",
       " 650,\n",
       " 521,\n",
       " 333,\n",
       " 372,\n",
       " 506,\n",
       " 119,\n",
       " 163,\n",
       " 191,\n",
       " 192,\n",
       " 144,\n",
       " 154,\n",
       " 191,\n",
       " 161,\n",
       " 205,\n",
       " 205,\n",
       " 206,\n",
       " 157,\n",
       " 191,\n",
       " 174,\n",
       " 164,\n",
       " 197,\n",
       " 166,\n",
       " 176,\n",
       " 183,\n",
       " 146,\n",
       " 214,\n",
       " 216,\n",
       " 210,\n",
       " 163,\n",
       " 214,\n",
       " 212,\n",
       " 238,\n",
       " 282,\n",
       " 204,\n",
       " 183,\n",
       " 164,\n",
       " 91,\n",
       " 132,\n",
       " 205,\n",
       " 155,\n",
       " 145,\n",
       " 192,\n",
       " 172,\n",
       " 159,\n",
       " 169,\n",
       " 177,\n",
       " 195,\n",
       " 109,\n",
       " 142,\n",
       " 211,\n",
       " 154,\n",
       " 188,\n",
       " 180,\n",
       " 196,\n",
       " 170,\n",
       " 188,\n",
       " 165,\n",
       " 194,\n",
       " 175,\n",
       " 193,\n",
       " 148,\n",
       " 220,\n",
       " 166,\n",
       " 182,\n",
       " 139,\n",
       " 132,\n",
       " 73,\n",
       " 148,\n",
       " 174,\n",
       " 189,\n",
       " 170,\n",
       " 181,\n",
       " 163,\n",
       " 196,\n",
       " 194,\n",
       " 100,\n",
       " 166,\n",
       " 80,\n",
       " 155,\n",
       " 108,\n",
       " 136,\n",
       " 110,\n",
       " 132,\n",
       " 135,\n",
       " 129,\n",
       " 220,\n",
       " 121,\n",
       " 140,\n",
       " 179,\n",
       " 152,\n",
       " 186,\n",
       " 109,\n",
       " 146,\n",
       " 183,\n",
       " 131,\n",
       " 173,\n",
       " 146,\n",
       " 122,\n",
       " 211,\n",
       " 177,\n",
       " 143,\n",
       " 143,\n",
       " 221,\n",
       " 166,\n",
       " 120,\n",
       " 191,\n",
       " 134,\n",
       " 184,\n",
       " 148,\n",
       " 167,\n",
       " 155,\n",
       " 233,\n",
       " 121,\n",
       " 201,\n",
       " 242,\n",
       " 227,\n",
       " 159,\n",
       " 192,\n",
       " 160,\n",
       " 133,\n",
       " 211,\n",
       " 178,\n",
       " 157,\n",
       " 156,\n",
       " 230,\n",
       " 150,\n",
       " 86,\n",
       " 163,\n",
       " 170,\n",
       " 173,\n",
       " 198,\n",
       " 211,\n",
       " 179,\n",
       " 122,\n",
       " 194,\n",
       " 188,\n",
       " 187,\n",
       " 196,\n",
       " 166,\n",
       " 106,\n",
       " 181,\n",
       " 145,\n",
       " 193,\n",
       " 231,\n",
       " 207,\n",
       " 163,\n",
       " 82,\n",
       " 208,\n",
       " 180,\n",
       " 114,\n",
       " 172,\n",
       " 203,\n",
       " 201,\n",
       " 177,\n",
       " 178,\n",
       " 120,\n",
       " 174,\n",
       " 207,\n",
       " 161,\n",
       " 161,\n",
       " 149,\n",
       " 138,\n",
       " 136,\n",
       " 160,\n",
       " 191,\n",
       " 164,\n",
       " 185,\n",
       " 141,\n",
       " 171,\n",
       " 134,\n",
       " 205,\n",
       " 132,\n",
       " 105,\n",
       " 205,\n",
       " 215,\n",
       " 194,\n",
       " 160,\n",
       " 173,\n",
       " 154,\n",
       " 159,\n",
       " 142,\n",
       " 192,\n",
       " 205,\n",
       " 223,\n",
       " 188,\n",
       " 173,\n",
       " 136,\n",
       " 189,\n",
       " 189,\n",
       " 155,\n",
       " 143,\n",
       " 120,\n",
       " 179,\n",
       " 172,\n",
       " 158,\n",
       " 141,\n",
       " 195,\n",
       " 215,\n",
       " 181,\n",
       " 172,\n",
       " 169,\n",
       " 198,\n",
       " 145,\n",
       " 168,\n",
       " 148,\n",
       " 76,\n",
       " 142,\n",
       " 155,\n",
       " 170,\n",
       " 127,\n",
       " 159,\n",
       " 153,\n",
       " 168,\n",
       " 198,\n",
       " 206,\n",
       " 150,\n",
       " 216,\n",
       " 153,\n",
       " 172,\n",
       " 193,\n",
       " 137,\n",
       " 125,\n",
       " 163,\n",
       " 172,\n",
       " 185,\n",
       " 175,\n",
       " 203,\n",
       " 172,\n",
       " 173,\n",
       " 204,\n",
       " 173,\n",
       " 188,\n",
       " 122,\n",
       " 181,\n",
       " 115,\n",
       " 175,\n",
       " 115,\n",
       " 194,\n",
       " 198,\n",
       " 164,\n",
       " 212,\n",
       " 145,\n",
       " 155,\n",
       " 191,\n",
       " 192,\n",
       " 202,\n",
       " 140,\n",
       " 132,\n",
       " 181,\n",
       " 142,\n",
       " 161,\n",
       " 152,\n",
       " 150,\n",
       " 160,\n",
       " 194,\n",
       " 72,\n",
       " 96,\n",
       " 193,\n",
       " 179,\n",
       " 232,\n",
       " 184,\n",
       " 162,\n",
       " 128,\n",
       " 176,\n",
       " 174,\n",
       " 188,\n",
       " 229,\n",
       " 230,\n",
       " 240,\n",
       " 171,\n",
       " 192,\n",
       " 191,\n",
       " 169,\n",
       " 235,\n",
       " 91,\n",
       " 74,\n",
       " 181,\n",
       " 245,\n",
       " 204,\n",
       " 191,\n",
       " 164,\n",
       " 225,\n",
       " 86,\n",
       " 83,\n",
       " 104,\n",
       " 206,\n",
       " 198,\n",
       " 225,\n",
       " 215,\n",
       " 176,\n",
       " 174,\n",
       " 162,\n",
       " 252,\n",
       " 100,\n",
       " 100,\n",
       " 212,\n",
       " 247,\n",
       " 111,\n",
       " 142,\n",
       " 266,\n",
       " 146,\n",
       " 208,\n",
       " 245,\n",
       " 141,\n",
       " 143,\n",
       " 201,\n",
       " 126,\n",
       " 159,\n",
       " 186,\n",
       " 118,\n",
       " 240,\n",
       " 163,\n",
       " 149,\n",
       " 149,\n",
       " 124,\n",
       " 71,\n",
       " 171,\n",
       " 154,\n",
       " 216,\n",
       " 163,\n",
       " 276]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_contador_verdadeiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b7d22ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:41:46.850850Z",
     "start_time": "2023-03-03T02:41:46.835210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_contador_falso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a2fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_contador_verdadeiro=[]\n",
    "lista_contador_falso=[]\n",
    "for perplexidades_bi_paper, perplexidades_tri_paper in zip(lista_paper_perp,lista_paper_perp_tri):\n",
    "    contador_positivo=0\n",
    "    contador_negativo=0\n",
    "    for p_bi, p_tri in zip(perplexidades_bi_paper, perplexidades_tri_paper):\n",
    "        if p_tri<=p_bi:\n",
    "            contador_positivo=contador_positivo+1\n",
    "        else:\n",
    "            contador_negativo=contador_negativo+1\n",
    "        #lista_contador_negativo.append(contador_negativo)\n",
    "    lista_contador_verdadeiro.append(contador_positivo)\n",
    "    lista_contador_falso.append(contador_negativo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae35819",
   "metadata": {},
   "source": [
    "TIRAR A MÉDIA DE TUDO E COMPARAR QUAL É MENOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae621e",
   "metadata": {},
   "source": [
    "# Experimento 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe48b1",
   "metadata": {},
   "source": [
    "## Parte 1\n",
    "Aplicando o método da similaridade nos 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030880c",
   "metadata": {},
   "source": [
    "### Similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8c4f17e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:41:46.867038Z",
     "start_time": "2023-03-03T02:41:46.850850Z"
    }
   },
   "outputs": [],
   "source": [
    "def vetor_frases_ft_teste(sessao_palavras, modelo_embedding):\n",
    "    \"\"\"input: dicionário de sessão das palavras\n",
    "        output: dicionário de vetores\"\"\"\n",
    "    dicio={}\n",
    "    for k,v in sessao_palavras.items():\n",
    "        x=[]\n",
    "        for lista_frase in v:\n",
    "            soma=0\n",
    "            for palavra in lista_frase:\n",
    "                try:\n",
    "                    soma = soma + modelo_embedding.get_word_vector(palavra)\n",
    "                except:\n",
    "                    soma=soma\n",
    "            x.append(soma)\n",
    "        dicio[k] = x\n",
    "    return dicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9be1f614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:41:46.883076Z",
     "start_time": "2023-03-03T02:41:46.867038Z"
    }
   },
   "outputs": [],
   "source": [
    "modelo_selecionado = model_ft_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8692339f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:41:53.501827Z",
     "start_time": "2023-03-03T02:41:46.883076Z"
    }
   },
   "outputs": [],
   "source": [
    "abstract_teste        = {k: preprocessa_abstract(v) for k, v in papers_teste2.items()}\n",
    "abstract_frases_teste = {k: separa_frases(j) for k, v in abstract_teste.items() for j in v}\n",
    "abstract_frases_teste = segundo_preprocessamento(abstract_frases_teste)\n",
    "abstract_palavras_teste = {k:[word_tokenize(f) for f in v] for k,v in abstract_frases_teste.items()}\n",
    "\n",
    "introducao_teste = {k: preprocessa_intro(v) for k, v in papers_teste2.items()}\n",
    "introducao_frases_teste = {k: separa_frases(j) for k, v in introducao_teste.items() for j in v}\n",
    "introducao_frases_teste = segundo_preprocessamento(introducao_frases_teste)\n",
    "introducao_palavras_teste = {k:[word_tokenize(f) for f in v] for k,v in introducao_frases_teste.items()}\n",
    "\n",
    "conclusao_teste = {k: preprocessa_conclusion(v) for k, v in papers_teste2.items()}\n",
    "conclusao_frases_teste = {k: separa_frases(j) for k, v in conclusao_teste.items() for j in v}\n",
    "conclusao_frases_teste = segundo_preprocessamento(conclusao_frases_teste)\n",
    "conclusao_palavras_teste = {k:[word_tokenize(f) for f in v] for k,v in conclusao_frases_teste.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bfab7bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:07.073025Z",
     "start_time": "2023-03-03T02:41:53.501827Z"
    }
   },
   "outputs": [],
   "source": [
    "v_abstract_teste    = vetor_frases_ft_teste(abstract_palavras_teste,modelo_selecionado)\n",
    "v_introducao_teste  = vetor_frases_ft_teste(introducao_palavras_teste,modelo_selecionado)\n",
    "v_conclusao_teste   = vetor_frases_ft_teste(conclusao_palavras_teste,modelo_selecionado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "12093eb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:07.088745Z",
     "start_time": "2023-03-03T02:42:07.073888Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frases_objetivo_teste(indices_objetivo, idx_paper):\n",
    "    \"\"\"retorna a frase objetivo [do abstract] a partir dos indices\"\"\"\n",
    "    frase_objetivo={}\n",
    "    for idx,idx_f,idx_ind in zip(idx_paper,range(len(list(abstract_frases_teste.values()))),indices_objetivo):\n",
    "        lista_one=[]\n",
    "        for j in idx_ind:\n",
    "            lista_one.append(list(abstract_frases_teste.values())[idx_f][j])\n",
    "        frase_objetivo[idx] = lista_one\n",
    "    return frase_objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d290b7b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:13.099886Z",
     "start_time": "2023-03-03T02:42:07.088745Z"
    }
   },
   "outputs": [],
   "source": [
    "#Abtract e Introdução\n",
    "sim_ab_intro_teste,indices_ab_intro_teste,lista_idx_ab_intro_teste =aplicando_dicionario_similaridade(list(v_abstract_teste.values()),list(v_introducao_teste.values()))\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_teste,indices_ab_conc_teste,lista_idx_ab_c_teste = aplicando_dicionario_similaridade(list(v_abstract_teste.values()),list(v_conclusao_teste.values()))\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_teste = frases_interseccao(lista_idx_ab_intro_teste,lista_idx_ab_c_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0b0b0ee5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:13.115513Z",
     "start_time": "2023-03-03T02:42:13.099886Z"
    }
   },
   "outputs": [],
   "source": [
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_teste  = get_frases_objetivo_teste(indices_objetivo_teste, papers_teste.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dc02191d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:13.147109Z",
     "start_time": "2023-03-03T02:42:13.115513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{746: ['the algorithm is based on two powerful constraints that words tend to have one sense per discourse and one sense per collocation exploited in an iterative bootstrapping procedure'],\n",
       " 747: ['to attack these problems we have built a hybrid generator in which gaps in symbolic knowledge are filled by statistical methods',\n",
       "  'we also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability even when perfect knowledge is in principle obtainable'],\n",
       " 748: ['syntactic natural language parsers have shown themselves to be inadequate for processing highlyambiguous largevocabulary text as is evidenced by their poor performance on domains like the wall street journal and by the movement away from parsingbased approaches to textprocessing in general'],\n",
       " 749: ['this study suggests that the identification of word translations should also be possible with nonparallel and even unrelated texts',\n",
       "  'the method proposed is based on the assumption that there is a correlation between the patterns of word cooccurrences in texts of different languages'],\n",
       " 750: ['this approach integrates a diverse set of knowledge sources to disambiguate word sense including part of speech of neighboring words morphological form the unordered set of surrounding words local collocations and verbobject syntactic relation'],\n",
       " 752: ['this paper addresses the problem for a fairly general form of combinatory categorial grammar by means of an efficient correct and easy to implement normalform parsing tech the parser is proved to find exone in each semantic equivalence class of allowable parses that is spurious ambiguity as carefully defined is shown to be both safely and completely eliminated'],\n",
       " 754: ['experimental results are given showing that the two new algorithms have improved performance over the viterbi algorithm on many criteria especially the ones that they optimize'],\n",
       " 755: ['this paper describes a new statistical parser which is based on probabilities of dependencies between headwords in the parse tree',\n",
       "  'tests using wall street journal data show that the method performs at least as well as spatter magerman jelinek et al which has the best published results for a statistical parser on this task'],\n",
       " 757: ['we examine the effects of speaking style read versus spontaneous and of discourse segmentation method textalone versus textandspeech on the nature of this relationship',\n",
       "  'we also compare the acousticprosodic features of initial medial and final utterances in a discourse segment'],\n",
       " 758: ['we present an extensive empirical comparison of several smoothing techniques in the domain of language modeling including those described by jelinek and mercer katz and church and gale',\n",
       "  'we investigate for the first time how factors such as training data corpus versus wall street journal and ngram order bigram versus trigram affect the relative performance of these methods which we measure through the crossentropy of test data'],\n",
       " 759: ['we also show that sample selection yields a significant reduction in the size of the model used by the tagger'],\n",
       " 760: ['in this paper we first propose a new statistical parsing model which is a generative model of lexicalised contextfree grammar',\n",
       "  'we then extend the model to include a probabilistic treatment of both subcategorisation and whmovement'],\n",
       " 762: ['most previous corpusbased algorithms disambiguate a word with a classifier trained from previous usages of the same word'],\n",
       " 763: ['we derive the rhetorical structures of texts by means of two new surfaceformbased algorithms one that identifies discourse usages of cue phrases and breaks sentences into clauses and one that produces valid rhetorical structure trees for unrestricted natural language texts'],\n",
       " 764: [],\n",
       " 765: ['a loglinear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations achieving accuracy in this task when each conjunction is considered independently',\n",
       "  'combining the constraints across many adjectives a clustering algorithm separates the adjectives into groups of different orientations and finally adjectives are labeled positive or negative'],\n",
       " 766: ['this paper presents paradise paradigm for dialogue system evaluation a general framework for evaluating spoken rlialogue agents',\n",
       "  'the framework decouples task requirements from an agents dialogue behaviors supports comparisons among dialogue strategies enables the calculation of performance over subdialogues and whole dialogues specifies the relative contribution of various factors to performance and makes it possible to compare agents performing different tasks by normalizing for task complexity'],\n",
       " 767: ['this paper presents a trainable rulebased algorithm for performing word segmentation',\n",
       "  'in addition we show the transformationbased algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages'],\n",
       " 768: ['many multilingual nlp applications need to translate words between different languages but cannot afford the computational expense of inducing or applying a full translation model',\n",
       "  'for these applications we have designed a fast algorithm for estimating a partial translation model which accounts for translational equivalence only at the word level'],\n",
       " 769: ['generalization is performed online at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus'],\n",
       " 770: ['computer recognition of this phenomenon is important because it helps break the document boundary by allowing a user to examine information about a particular entity from multiple text sources at the same time',\n",
       "  'in addition we also describe a scoring algorithm for evaluating the crossdocument coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the muc within document coreference task'],\n",
       " 771: ['the projects key features are a a commitment to corpus evidence for semantic and syntactic generalizations and b the representation of the valences of its target words mostly nouns adjectives and verbs in which the semantic portion makes use of frame semantics',\n",
       "  'the resulting database will contain a descriptions of the semantic frames underlying the meanings of the words described and b the valence representation semantic and syntactic of several thousand words and phrases each accompanied by c a representative collection of annotated corpus attestations which jointly exemplify the observed linkings between frame elements and their syntactic realizations eg grammatical function phrase type and other syntactic traits'],\n",
       " 772: ['in this paper we first show that the errors made from three different state of the art part of speech taggers are strongly complementary',\n",
       "  'by using contextual cues to guide tagger combination we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers'],\n",
       " 773: ['while previous empirical methods for base np identification have been rather complex this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task',\n",
       "  'using this simple algorithm with a naive heuristic for matching rules we achieve surprising accuracy in an evaluation on the'],\n",
       " 774: ['the paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history thus enabling the use of long distance dependencies',\n",
       "  'the model assigns probability to every joint sequence of wordsbinaryparsestructure with headword annotation and operates in a lefttoright manner therefore usable for automatic speech recognition'],\n",
       " 777: ['in this paper we examine how the differences in modelling between different data driven systems performing the same nlp task can be exploited to yield a higher accuracy than the best individual system'],\n",
       " 780: ['bootstrapping semantics from text is one of the greatest challenges in natural language learning',\n",
       "  'the evaluation results show that the thesaurus is significantly closer to wordnet than roget thesaurus is'],\n",
       " 783: ['several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task',\n",
       "  'our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only partofspeech tags and morphological base forms as opposed to attachment information'],\n",
       " 785: ['additionally the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand',\n",
       "  'our algorithm finds many terms not included within wordnet many more than previous algorithms and could be viewed as an enhancer of existing broadcoverage resources'],\n",
       " 786: ['i propose a model for determining the hearers attentional state which depends solely on a list of salient discourse entities slist',\n",
       "  'the ordering among the elements of the slist covers also the of the center the centering model',\n",
       "  'the ranking criteria for the slist based on the distinction between entities and incorporate preferences for interand intrasentential anaphora'],\n",
       " 787: ['distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences'],\n",
       " 788: ['we present a method for extracting parts of objects from wholes eg',\n",
       "  'the part list could be scanned by an enduser and added to an existing ontology such as wordnet or used as a part of a rough semantic lexicon'],\n",
       " 789: ['we present a technique for automatic induction of slot annotations for subcategorization frames based on induction of hidden classes in the em framework of statistical estimation',\n",
       "  'we outline an interpretation of the learned representations as theoreticallinguistic decompositional lexical entries'],\n",
       " 790: ['this work goes a step further by automatically creating not just clusters of related words but a hierarchy of nouns and their hypernyms akin to the handbuilt hierarchy in wordnet'],\n",
       " 791: ['this paper presents a case study of analyzing and improving intercoder reliability in discourse using statistical techniques corrected tags are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier'],\n",
       " 792: ['we present a method for automatic identification of noncompositional expressions using their statistical properties in a text corpus',\n",
       "  'our method is based on the hypothesis that when a phrase is noncomposition its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word'],\n",
       " 793: ['we used these to construct and evaluate a baseline system that uses pattern matching bagofwords techniques augmented with additional automated linguistic processing stemming name identification semantic class identification and pronoun resolution',\n",
       "  'this simple system retrieves the sentence containing the answer of the time'],\n",
       " 794: ['we have developed a corpusbased algorithm for automatically identifying definite noun phrases that are nonanaphoric which has the potential to improve the efficiency and accuracy of coreference resolution systems',\n",
       "  'our algorithm generates lists of nonanaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize nonanaphoric noun phrases in new texts'],\n",
       " 796: ['our final results dependency accuracy represent good progress towards the accuracy of the parser on english wall street journal text'],\n",
       " 797: ['the current study which is based on the assumption that there is a correlation between the patterns of word cooccurrences in corpora of different languages makes a significant improvement to about of word translations identified correctly'],\n",
       " 798: ['the most recent endproduct is an automatically acquired parallel corpus comprising englishfrench document pairs approximately million words per language'],\n",
       " 799: ['we describe two computationallytractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses and apply these to estimate a stochastic version of lexical'],\n",
       " 800: ['we present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents',\n",
       "  'our approach is unique in its usage of language generation to reformulate the wording of the summary'],\n",
       " 801: ['tempeval comprises evaluation tasks for time expressions events and temporal relations the latter of which was split up in four sub tasks motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier'],\n",
       " 802: ['this paper presents the description and evaluation framework of semeval word sense induction amp disambiguation task as well as the evaluation results of participating systems',\n",
       "  'in this task participants were required to induce the senses of target words using a training set and then disambiguate unseen instances of the same words using the induced senses'],\n",
       " 803: ['the task was designed to promote research on semantic inference over texts written in different languages targeting at the same time a real application scenario',\n",
       "  'we report on the training and test data used for evaluation the process of their creation the participating systems teams runs the approaches adopted and the results achieved'],\n",
       " 804: ['we also describe two new techniques based on sentence utility and subsumption which we have applied to the evaluation of both single and multiple document summaries'],\n",
       " 805: ['we introduce a semanticbased algorithm for learning morphology which only proposes affixes when the stem and stemplusaffix are sufficiently similar semantically'],\n",
       " 806: ['this paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora',\n",
       "  'an algorithm context distribution clustering cdc is presented which can be naturally extended to handle these problems'],\n",
       " 809: ['this paper presents the firstever results of applying statistical parsing models to the newlyavailable chinese treebank'],\n",
       " 811: ['this paper presents results for a maximumentropybased part of speech tagger which achieves superior performance principally by enriching the information sources used for tagging'],\n",
       " 813: ['we present such a component a fast and robust morphological generator for english based on finitestate techniques that generates a word form given a specification of the lemma partofspeech and the type of inflection required',\n",
       "  'we describe how this morphological generator is used in a prototype system for automatic simplification of english newspaper text and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application'],\n",
       " 814: ['this study examines the learning behavior of cotraining on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels'],\n",
       " 815: ['in this paper we describe a classification algorithm for identifying relationships between twoword noun compounds',\n",
       "  'we find that a very simple approach using a machine learning algorithm and a domainspecific lexical hierarchy successfully generalizes from training instances performing better on previously unseen words than a baseline consisting of training on the words themselves'],\n",
       " 816: ['we provide two major evaluations of nine existing collocationfinders and illustrate the continuing need for improvement',\n",
       "  'we use latent semantic analysis to make modest gains in performance but we show the significant challenges encountered in trying this approach'],\n",
       " 817: ['this paper describes a method for linear text segmentation that is more accurate or at least as accurate as stateoftheart methods utiyama and isahara choi a',\n",
       "  'test results show lsa is a more accurate similarity measure than the'],\n",
       " 818: [],\n",
       " 819: ['we describe a procedure for arranging into a timeline the contents of news stories describing the development of some situation',\n",
       "  'we describe the parts of the system that deal with breaking sentences into eventclauses and resolving both explicit and implicit temporal references'],\n",
       " 821: ['nltk the natural language toolkit is a suite of open source program modules tutorials and problem sets providing readytouse computational linguistics courseware'],\n",
       " 822: ['we explore the use of support vector machines svms for biomedical named entity recognition',\n",
       "  'to make the svm training with the available largest corpus the genia corpus tractable we propose to split the nonentity class into subclasses using partofspeech information'],\n",
       " 824: ['the quality of the segmentations is measured using an evaluation method that compares the segmentations produced to an existing morphological analysis',\n",
       "  'experiments on both finnish and english corpora show that the presented methods perform well compared to a current stateoftheart system'],\n",
       " 825: ['open mind word expert is an implemented active learning system for collecting word sense tagging from the general public over the web',\n",
       "  'we expect the system to yield a large volume of highquality training data at a much lower cost than the traditional method of hiring lexicographers',\n",
       "  'if successful the collection process can be extended to create the definitive corpus of word sense information'],\n",
       " 826: [],\n",
       " 827: ['we propose an approximation based on attributes and coarseand finegrained matching that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty'],\n",
       " 830: ['we conclude by examining factors that make the sentiment classification problem more challenging'],\n",
       " 833: ['this paper describes a bootstrapping algorithm called basilisk that learns highquality semantic lexicons for multiple categories',\n",
       "  'basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts'],\n",
       " 834: ['we demonstrate that while there are cases where coherence is poor there are many regularities which can be exploited by a statistical machine translation system',\n",
       "  'we also compare three variant syntactic representations to determine which one has the best properties with respect to cohesion'],\n",
       " 835: ['this grammar is being developed in a multilingual context requiring mrs structures that are easily comparable across languages'],\n",
       " 836: ['the grammar matrix is an opensource starterkit for the development of broad by using a type hierarchy to represent crosslinguistic generalizations and providing compatibility with other opensource tools for grammar engineering evaluation parsing and generation it facilitates not only quick startup but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding'],\n",
       " 837: ['we report on the parallel grammar pargram project which uses the xle parser and grammar development platform for six languages english'],\n",
       " 838: ['in this paper we propose a new statistical japanese dependency parser using a cascaded chunking model',\n",
       "  'we propose a new method that is simple and efficient since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side'],\n",
       " 839: ['conditional maximum entropy me models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics and which is used for a wide variety of classification problems in natural language processing',\n",
       "  'surprisingly the standardly used iterative scaling algorithms perform quite poorly in comparison to the others and for all of the test problems a limitedmemory variable metric algorithm outperformed the other choices'],\n",
       " 843: [],\n",
       " 844: ['this paper presents a set of algorithms for distinguishing personal names with multiple real referents in text based on little or no supervision'],\n",
       " 845: ['we investigate selecting examples by directly maximising tagger agreement on unlabelled data a method which has been theoretically and empirically motivated in the cotraining literature'],\n",
       " 847: ['this paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy',\n",
       "  'the tagger uses features which can be obtained for a variety of languages and works effectively not only for english but also for other languages such as german and dutch'],\n",
       " 848: ['this paper presents a classifiercombination experimental framework for named entity recognition in which four diverse classifiers robust linear classifier maximum entropy transformationbased learning and hidden markov model are combined under different conditions',\n",
       "  'when no gazetteer or other additional training resources are used the combined system attains a performance of f on the english development data integrating name location and person gazetteers and named entity systems trained on additional more general data reduces the fmeasure error by a factor of to on the english data'],\n",
       " 849: ['we discuss two namedentity recognition models which use characters and character grams either exclusively or as an important part of their data representation',\n",
       "  'the first model is a characterlevel hmm with minimal context information and the second model is a maximumentropy conditional markov model with substantially richer context features',\n",
       "  'this number represents a error reduction over the same model without wordinternal substring features'],\n",
       " 851: ['new methods in automatic of the',\n",
       "  'in notes of the aiii spring on intelligent text summarization'],\n",
       " 852: ['we use deep linguistic features to predict semantic roles on syntactic arguments and show that these perform considerably better than surfaceoriented features',\n",
       "  'we also show that predicting labels from a lightweight parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features'],\n",
       " 853: ['we present a system for automatically identifying propbankstyle semantic roles based on the output of a statistical parser for combinatory categorial grammar'],\n",
       " 854: ['we present a general framework for distributional similarity based on the concepts of precision and recall',\n",
       "  'different parameter settings within this framework approximate different existing similarity measures as well as many more which have until now been unexplored'],\n",
       " 855: ['the learned patterns are then used to identify more subjective sentences'],\n",
       " 856: ['we present a bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories and describe three unsupervised statistical techniques for the significantly harder task of detecting opinions at the sentence level',\n",
       "  'results from a large collection of news stories and a human evaluation of sentences are reported indicating that we achieve very high performance in document classification upwards of precision and recall and respectable performance in detecting opinions and classifying them at the sentence level as positive negative or neutral up to accuracy'],\n",
       " 857: ['in more detail exgives a better precithan and by adding the tags assigned to the term as a feature a dramatic improvement of the results is obtained independent of the term selection approach applied'],\n",
       " 859: ['we give the motivation for having an international segmentation contest given that there have been two withinchina contests to date and we report on the results of this first international contest analyze these results and make some recommendations for the future'],\n",
       " 861: ['through the first bakeoff we could learn more about the development in chinese word segmentation and become more confident on our hhmmbased approach'],\n",
       " 862: ['this paper describes a distributional approach to the semantics of verbparticle eg',\n",
       "  'we then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verbparticle constructions'],\n",
       " 863: ['we examine various measures using the nearest neighbours of the phrasal verb and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set'],\n",
       " 864: ['we use latent semantic analysis to determine the similarity between a multiword expression and its constituent words and claim that higher similarities indicate greater decomposability',\n",
       "  'we test the model over english nounnoun compounds and verbparticles and evaluate its correlation with similarities and hyponymy values in wordnet',\n",
       "  'based on mean hyponymy over partitions of data ranked on similarity we furnish evidence for the calculated similarities being correlated with the semantic relational content of wordnet'],\n",
       " 865: ['in this paper we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework'],\n",
       " 867: ['this paper presents the task definition resources participating systems and comparative results for the english lexical sample task which was orgaas part of the evaluation exercise'],\n",
       " 869: ['for recalloriented understudy for gisting evaluation',\n",
       "  'this paper introduces four different included in the summarization evaluation package and their evaluations'],\n",
       " 871: ['data we describe a new corpus of over handannotated dialog act tags and accompanying adjacency pair annotations for roughly hours of speech from naturallyoccurring meetings',\n",
       "  'we provide a brief summary of the annotation system and labeling procedure interannotator reliability statistics overall distributional statistics a description of auxiliary files distributed with the corpus and information on how to obtain the data'],\n",
       " 872: ['given a collection of discrete random variables representing outcomes of learned local predictors in natural language eg named entities and relations we seek an optimal global assignment to the variables in the presence of general nonsequential constraints',\n",
       "  'we develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations',\n",
       "  'our approach allows us to efficiently incorporate domain and task specific constraints at decision time resulting in significant improvements in the accuracy and the humanlike quality of the inferences'],\n",
       " 873: ['second order context vectors are an indirect representation of the context based on the average of vectors that represent the words that occur in the context'],\n",
       " 874: ['using data from a small treebank of swedish memorybased classifiers for predicting the next action of the parser are constructed'],\n",
       " 876: ['this paper describes nombank a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus'],\n",
       " 877: ['results from a manual annotation experiment suggest that the notion of speculative sentence can be reliably annotated by humans'],\n",
       " 878: ['crucial to this approach is the proper characterization of entities as relation components which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events'],\n",
       " 879: ['our models can condition on arbitrary features of input sentences thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness'],\n",
       " 880: ['analysis of types shows that on the relation achieved accuracy'],\n",
       " 881: ['our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the web for related syntactic entailment templates'],\n",
       " 882: ['we describe how simple commonly understood statistical models such as statistical dependency parsers probabilistic contextfree grammars and wordtoword translation models can be effectively combined into a unified bilingual parser that jointly searches for the best english parse korean parse and word alignment where these hidden structures all constrain each other',\n",
       "  'the model used for parsing is completely factored into the two parsers and the tm allowing separate parameter estimation'],\n",
       " 884: ['we propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed',\n",
       "  'we further show that different features are needed for different subtasks'],\n",
       " 885: ['we present an unsupervised method for labelling the arguments of verbs with their semantic roles',\n",
       "  'a novel aspect of our approach is the use of verb slot and noun class information as the basis for backing off in our probability model'],\n",
       " 886: ['we apply statistical machine translation smt tools to generate novel paraphrases of input sentences in the same language',\n",
       "  'human evaluation shows that this system outperforms baseline paraphrase generation techniques and in a departure from previous work offers better coverage and scalability than the current bestofbreed paraphrasing approaches'],\n",
       " 887: [],\n",
       " 888: ['this paper presents an indepth study on such issues of processing architecture and feature representation for chinese pos tagging within a maximum entropy framework'],\n",
       " 889: ['when evaluating on the mismatched outofdomain test data the gram baseline is outperformed by the improvement brought by the adaptation technique using a very small amount of matched bn data kwds is about relative'],\n",
       " 890: ['the proposal consists of i decision stumps that use subtrees as features and ii the boosting algorithm which employs the subtreebased decision stumps as weak learners',\n",
       "  'two experiments on opinionmodality classification confirm that subtree features are important'],\n",
       " 891: ['multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document',\n",
       "  'centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudosentence'],\n",
       " 892: ['if two translation systems differ differ in performance on a test set can we trust that this indicates a difference in true system quality',\n",
       "  'to answer this question we describe bootstrap resampling methods to compute statistical significance of test results and validate them on the concrete example of the even for small test sizes of only sentences our methods may give us assurances that test result differences are real'],\n",
       " 893: ['in this paper we introduce textrank a graphbased ranking model for text processing and show how this model can be successfully used in natural language applications',\n",
       "  'in particular we propose two innovative unsupervised methods for keyword and sentence extraction and show that the results obtained compare favorably with previously published results on established benchmarks'],\n",
       " 894: ['this paper introduces an approach to sentiment analysis which uses support vector machines svms to bring together diverse sources of potentially pertinent information including several favorability measures for phrases and adjectives and where available knowledge of the topic of the text'],\n",
       " 897: ['we explore the use of syntactic information including constituent labels and headmodifier dependencies in computing similarity between output and reference',\n",
       "  'our results show that adding syntactic information to the evaluation metric improves both sentencelevel and corpuslevel correlation with human judgments'],\n",
       " 899: ['while there is a large body of previous work focused on finding the semantic similarity of concepts and words the application of these wordoriented methods to text similarity has not been yet explored',\n",
       "  'in this paper we introduce a method that combines wordtoword similarity metrics into a texttotext metric and we show that this method outperforms the traditional text similarity metrics based on lexical matching'],\n",
       " 900: ['we discuss the relevance of kbest parsing torecent applications in natural language pro cessing and develop efficient algorithms for kbest trees in the framework of hypergraphparsing',\n",
       "  'to demonstrate the efficiency scal ability and accuracy of these algorithms we present experiments on bikels implementation of collins',\n",
       "  'we show in particu lar how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications'],\n",
       " 901: ['we present a classifierbased parser that produces constituent trees in linear time',\n",
       "  'the parser uses a basic bottomup shiftreduce algorithm but employs a classifier to determine parser actions instead of a grammar'],\n",
       " 902: ['we introduce an approach of exploiting the semantic structure of a sentence anchored to an opinion bearing verb or adjective',\n",
       "  'this method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from framenet',\n",
       "  'our experimental results show that our system performs significantly better than the baseline'],\n",
       " 904: ['we introduce spmt a new class of statistical translation models that use syntactified target language phrases',\n",
       "  'the spmt models outperform a state of the art phrasebased baseline model by bleu points on the nist chineseenglish test corpus and points on a humanbased quality metric that ranks translations on a scale from to'],\n",
       " 905: ['we discuss different strategies for smoothing the phrasetable in statistical mt and give results over a range of translation settings',\n",
       "  'we show that any type of smoothing is a better idea than the relativefrequency estimates that are often used'],\n",
       " 906: [],\n",
       " 907: ['however in certain applications such as nonprojective dependency parsing and machine translation the complete formulation of the decoding problem as an integer linear program renders solving intractable',\n",
       "  'this approach is applied to dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over stateoftheart'],\n",
       " 908: ['we investigate whether one can determine from the transcripts of us congressional floor debates whether the speeches represent support of or opposition to proposed legislation',\n",
       "  'to address this problem we exploit the fact that these speeches occur as part of a discussion this allows us to use sources of information regarding relationships between discourse segments such as whether a given utterance indicates agreement with the opinion expressed by another'],\n",
       " 909: ['the experimental results show that the precision of polarity assignment with the automatically acquired lexicon was on average and our method is robust for corpora in diverse domains and for the size of the initial lexicon'],\n",
       " 910: ['we present an approach for the joint extraction of entities and relations in the con text of opinion recognition and analysis',\n",
       "  'the resulting system achieves fmeasuresof and for entity and relation extrac tion respectively improving substantially over prior results in the area'],\n",
       " 911: ['in this paper we approach word sense disambiguation and information extraction as a unified tagging problem',\n",
       "  'experimental evaluation on the main senseannotated datasets available ie semcor and senseval shows considerable improvements over the best known firstsense baseline'],\n",
       " 912: ['we show that this measure compares favorably to other measures with respect to human judgments of semantic relatedness and that it performs well when used in a word sense disambiguation algorithm that relies on semantic relatedness',\n",
       "  'this measure is flexible in that it can make comparisons between any two concepts without regard to their part of speech'],\n",
       " 913: ['in this paper we investigate a new problem identifying the which a document is written',\n",
       "  'the results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy'],\n",
       " 914: ['we also give an overview of the parsing approaches that participants took and the results that they achieved',\n",
       "  'acknowledgement many thanks to amit dubey and yuval krymolowski the other two organizers of the shared task for discussions converting treebanks writing and helping with the also to alexander yeh for additional help with the paper reviews'],\n",
       " 916: ['the first stage based on the unlabeled dependency parsing models described by mcdonald and pereira augmented with morphological features for a subset of the languages',\n",
       "  'the second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph'],\n",
       " 917: ['nonprojective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser'],\n",
       " 918: ['we first propose a simple generative phrasebased model and verify that its estimates are inferior to those given by surface statistics'],\n",
       " 919: ['we evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a wordaligned corpus'],\n",
       " 921: ['we present translation results on the shared task exploiting parallel texts for statistical machine translation generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories'],\n",
       " 922: [],\n",
       " 924: ['we introduce chinese whispers a randomized graphclustering algorithm which is timelinear in the number of edges',\n",
       "  'after a detailed definition of the algorithm and a discussion of its strengths and weaknesses the performance of chinese whispers is measured on natural language processing nlp problems as diverse as language separation acquisition of syntactic word classes and word sense disambiguation'],\n",
       " 925: ['we also show that the phrasal translation tables produced by the itg are superior to those of the flat joint phrasal model producing up to a point improvement in bleu score',\n",
       "  'finally we explore for the first time the utility of a joint phrasal translation model as a word alignment method'],\n",
       " 926: ['we show that this results in an im provement in the quality of translation and that the value of syntactic supertags in flat structured phrasebased models is largely due to better local reorderings'],\n",
       " 927: ['we describe a mixturemodel approach to adapting a statistical machine translation system for new domains using weights that depend on text distances to mixture components',\n",
       "  'we investigate a number of variants on this approach including crossdomain versus dynamic adaptation linear versus loglinear mixtures language and translation model adaptation different methods of assigning weights and granularity of the source unit being adapted to'],\n",
       " 928: ['we carried out an extensive human evaluation which allowed us not only to rank the different mt systems but also to perform higherlevel analysis of the evaluation process',\n",
       "  'we measured the correlation of automatic evaluation metrics with human judgments'],\n",
       " 930: ['this paper recaps the technical details underlying the metric and describes recent improvements in the metric',\n",
       "  'the latest release includes improved metric parameters and extends the metric to support evaluation of mt output in spanish french and'],\n",
       " 932: ['since prepositions account for a substantial proportion of all grammatical errors by esl english as a second language learners developing an nlp application that can reliably detect these types of errors will provide an invaluable learning resource to esl students',\n",
       "  'to address this problem we use a maximum entropy classifier combined with rulebased filters to detect preposition errors in a corpus of student essays'],\n",
       " 933: ['the goal of this task is to allow for comparison across senseinduction and discrim ination systems and also to compare thesesystems to other supervised and knowledgebased systems',\n",
       "  'we reused the semeval english lexical sample subtask of task and set up both clusteringstyle unsuper vised evaluation using ontonotes senses as goldstandard and a supervised evaluation using the part of the dataset for mapping'],\n",
       " 934: ['we describe our experience in producing acoarse version of the wordnet sense inven tory and preparing the sensetagged corpusfor the task'],\n",
       " 935: [],\n",
       " 936: ['this task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name'],\n",
       " 937: ['the tempeval task proposes a simple way to evaluate automatic extraction of temporalrelations',\n",
       "  'it avoids the pitfalls of evaluating a graph of interrelated labels by defining three sub tasks that allow pairwise eval uation of temporal relations'],\n",
       " 938: ['this paper describes our experience in preparing the data and evaluating the results for three subtasks of semeval task lexical sample semantic role labelingsrl and allwords respectively'],\n",
       " 940: ['in this paper we investigate several nonprojective parsing algorithms for dependency parsing providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others called here the edgefactored model',\n",
       "  'this suggests that it is unlikely that exact nonprojective dependency parsing is tractable for any model richer than the edgefactored model'],\n",
       " 941: ['we use the human judgments of the systems to analyze automatic evaluation metrics for translation quality and we report the strength of the correlation with human judgments at both the systemlevel and at the sentencelevel'],\n",
       " 942: ['in this paper we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better mt performance',\n",
       "  'based on these findings we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the mt task providing an improvement of bleu',\n",
       "  'we also show that improving segmentation consistency using external lexicon and proper noun features yields a bleu increase'],\n",
       " 943: ['this paper describes two parallel implementations of giza that accelerate this word alignment process',\n",
       "  'results show a nearlinear speedup according to the number of cpus used and alignment quality is preserved'],\n",
       " 944: ['this paper examines the stanford typed dependencies representation which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding'],\n",
       " 945: ['we describe a parsing approach that makes use of the perceptron algorithm in conjunction with dynamic programming methods to recover full constituentbased parse trees',\n",
       "  'a severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved',\n",
       "  'a lowerorder dependency parsing model is used to restrict the search space of the full model thereby making it efficient'],\n",
       " 946: ['in the shared task was dedicated to the joint parsing of syntactic and semantic dependencies'],\n",
       " 948: ['we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for more than metrics'],\n",
       " 949: ['joshua implements all of the algorithms required for synchronous context grammars scfgs chartparsing gram language model integration beamcubepruning and extraction',\n",
       "  'it uses parallel and distributed computing techniques for scalability'],\n",
       " 950: ['we propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language'],\n",
       " 951: ['automatic machine translation mt evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to mt output with human judgments of translation performance',\n",
       "  'terplus was shown to be one of the top metrics in nists metrics matr challenge having the highest average rank in terms of pearson and spearman correlation',\n",
       "  'optimizing terplus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits demonstrating significant differences between the types of human judgments'],\n",
       " 952: ['in this paper we present a machine learning system that finds the scope of negation in biomedical texts',\n",
       "  'to investigate the robustness of the approach the system is tested on the three subcorpora of the bioscope corpus representing different text types'],\n",
       " 953: ['in the process of comparing several solutions to these challenges we reach some surprising conclusions as well as develop an system that achieves on the conll ner shared task the best reported result for this dataset'],\n",
       " 954: ['in this paper we present a machine learning system that finds the scope of hedge cues in biomedical texts',\n",
       "  'the system is based on a similar system that finds the scope of negation cues',\n",
       "  'to investigate the robustness of the approach the system is tested on the three subcorpora of the bioscope corpus that represent different text types'],\n",
       " 955: ['the paper presents the design and implementation of the bionlp shared task and reports the final results with analysis',\n",
       "  'the data was developed based on the genia event corpus',\n",
       "  'the evaluation results are encouraging indicating that stateoftheart performance is approaching a practically applicable level and revealing some remaining challenges'],\n",
       " 956: ['in addition to questions about emotions evoked by terms we show how the inclusion of a word choice question can discourage malicious data entry help identify instances where the annotator may not be familiar with the target term allowing us to reject such annotations and help obtain annotations at sense level rather than at word level'],\n",
       " 957: ['researchers participated in the workshops shared task to create data for speech and language applications with'],\n",
       " 958: [],\n",
       " 959: ['in particular we model the semantic composition of pairs of adjacent english adjecand nouns from the national we build a vectorbased semantic space from a lemmatised version of the bnc where the most frequent an lemma pairs are treated as single tokens'],\n",
       " 960: ['current approaches to semantic parsing the task of converting text to a formal meaning representation rely on annotated training data mapping sentences to logical forms',\n",
       "  'in addition we reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns thus allowing our parser to scale better using less supervision'],\n",
       " 961: ['the conll shared task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts',\n",
       "  'this paper provides a general overview of the shared task including the annotation protocols of the training and evaluation datasets the exact task definitions the evaluation metrics employed and the overall results',\n",
       "  'the paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task'],\n",
       " 962: ['the contributions of this paper are we introduce posspecific prior polarity features'],\n",
       " 963: ['five main tasks and three supporting tasks were arranged and their results show advances in the state of the art in finegrained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects'],\n",
       " 964: ['as its second time to be arranged for communitywide focused efforts it aimed to measure the advance of the community since and to evaluate generalization of the technology to full text papers',\n",
       "  'the results show the community has made a significant advancement in terms of both performance improvement and generalization'],\n",
       " 965: ['this paper briefly describes the ontonotes annotation coreference and other layers and then describes the parameters of the shared task including the format preprocessing information and evaluation criteria and presents and discusses the results achieved by the participating systems'],\n",
       " 966: ['all these models use global documentlevel information by sharing mention attributes such as gender and number across mentions in the same cluster'],\n",
       " 967: ['we also conducted a pilot tunable metrics task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality'],\n",
       " 968: ['we include ranking and adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced tuning version shown to outperform bleu in minimum error rate training for a phrasebased urduenglish system'],\n",
       " 969: [],\n",
       " 970: ['this paper presents the results of the wmt shared tasks which included a translation task a task for machine translation evaluation metrics and a task for runtime estimation of machine translation quality',\n",
       "  'we used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for evaluation metrics'],\n",
       " 971: ['the combination of wordalign plus charalign reduces the variance average square error by a factor of over more importantly because wordalign and charalign were designed to work robustly on texts that are smaller and more noisy than the hansards it has been possible to successfully deploy the programs at atampt language line services a commercial translation service to help them with difficult terminology'],\n",
       " 972: ['i survey some recent applicationsoriented nl generation systems and claim that despite very different theoretical backgrounds these systems have a remarkably similar architecture in terms of the modules they divide the generation process into the computations these modules perform and the way the modules interact with each other',\n",
       "  'i also compare this consensus architecture among applied nlg systems with psycholinguistic knowledge about how humans speak and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems'],\n",
       " 973: ['in this paper we describe an unsupervised learning algorithm for automatically training a rulebased part of speech tagger without using a manually tagged corpus'],\n",
       " 974: ['typically ambiguous verb phrases of the form v p np through a model which considers values of the four head words v nl paper shows that the problem is analogous to ngram language models in speech recognition and that one of the most common methods for language modeling the backedoff estimate is applicable',\n",
       "  'results on wall street journal data of accuracy are obtained using this method'],\n",
       " 975: ['yarowsky has exploited this complementarity by combining the two methods using decision lists'],\n",
       " 976: ['this paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns the kind of data one finds in online thesauri or as the output of distributional clustering algorithms'],\n",
       " 977: ['for this purpose it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word',\n",
       "  'some interesting adaptations to the transformationbased learning approach are also suggested by this application'],\n",
       " 978: ['a new objective evaluation measure is used to compare the quality of lexicons induced with different filter cascades'],\n",
       " 979: ['the part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory'],\n",
       " 980: ['this paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context'],\n",
       " 981: ['furthermore this paper demonstrates the use of specialized features to model difficult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems'],\n",
       " 982: ['previous algorithms are expensive due to two factors the exponential number of rules that must be generated and the use of a monte carlo parsing algorithm',\n",
       "  'we show that bods results are at least partially due to an extremely fortuitous choice of test data and partially due to using cleaner data than other researchers'],\n",
       " 984: ['we present a statistical word feature the word relation matrix which can be used to find translated pairs of words and terms from nonparallel corpora across language groups'],\n",
       " 985: ['selectional preference is traditionally connected with sense ambiguity this paper explores how a statistical model of selectional preference requiring neither manual annotation of selection restrictions nor supervised training can be used in sense disambiguation'],\n",
       " 986: ['the parser itself requires very little human intervention since the information it uses to make parsing decisions is specified in a concise and simple manner and is combined in a fully automatic way under the maximum entropy framework running time of the parser on test sentence linear with respect to the sentence length',\n",
       "  'furthermore the parser returns several scored parses for a sentence and this paper shows that a scheme to pick the best parse from the highest scoring a dramatically higher accuracy of precision and recall'],\n",
       " 987: ['we also present a new thresholding technique global thresholding which combined with the new beam thresholding gives an additional factor of two improvement and a novel technique multiple pass parsing that can be combined with the others to yield yet another improvement',\n",
       "  'we use a new search algorithm to simultaneously optimize the thresholding parameters of the various algorithms'],\n",
       " 988: ['this paper presents an efficient automatic method for discovering sequences of words that are translated as a unit',\n",
       "  'the method proceeds by comparing pairs of statistical translation models induced from parallel texts in two languages',\n",
       "  'objective evaluation on a simple machine translation task has shown the methods potential to improve the quality of mt output'],\n",
       " 989: [],\n",
       " 990: ['these methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs',\n",
       "  'overall the most accurate of these procedures is mcquittys similarity analysis in combination with a high dimensional feature set'],\n",
       " 992: ['we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text we show how these concepts can be implemented and we discuss results that we obtained with a discoursebased summanzation program motivation the evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some consamples of the output in very few cases output of a summarization program with a humanmade summary or evaluated with the help of human subjects usually the results are modest unfortunately evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions the position that we take in this paper is that in order to build highquality summarization programs one needs to evaluate not only a representative set of automatically generated outputs a highly difficult problem by itself but also the adequacy of the assumptions that these programs use that way one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each with few exceptions automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text see paice for an excellent overview determining the salient parts is considered to be achievable because one or more of the following assumptions hold i important sentences in a text contain words that are used frequently lahn edmundson n important sentences contain words that are used in the tide and section headings edmundson in important sentences are located at the beginning or end of paragraphs baxendale iv important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically through training techniques lin and hovy v important sentences use words as greatest and significant or indiphrases as the main aim of this paper and the purpose of this article while nonimportant senuse words as impossible edmundson rush salvador and zamora vi important sentences and concepts highest connected entities in elaborate semantic structures skorochodko lin barzilay and elhadad and vn imponant and nonimportant sentences are derivable from a discourse representation of the text sparck jones ono surmta and mike in determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections computers are accurate tools flowever in determining the concepts that are semantically related or the discourse structure of a text computers are no longer so accurate rather they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement although it is plausible that elaborate cohesionand coherencebased structures can be used effectively in summarization we believe that before building summarization programs we should determine the extent to which these assumptions hold in this paper we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text we show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program from discourse trees to summaries an empirical view'],\n",
       " 993: ['it is compatible with the princeton wordnet but integrates principlebased modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations',\n",
       "  'germanet includes a new treatment of regular polysemy artificial concepts and of particle verbs',\n",
       "  'it furthermore encodes crossclassification and basic syntactic information constituting an interesting tool in exploring the interacof syntax and development of such a large scale resource is particularly important as german up to now lacks basic online tools for the semantic exploration of very large corpora'],\n",
       " 994: ['given the nature of the systems rules it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension',\n",
       "  'the system has been evaluated in two distinct experiments which support the overall validity of the approach'],\n",
       " 995: ['experiments on using semantic distances between words in imcaption retrieval'],\n",
       " 997: ['recent work has used probabilistic contextfree grammars pcfgs to assign probabilities to constituents and to use these probabilities as the starting point for the fom'],\n",
       " 998: ['by working within the framework of maximum entropy theory and utilizing a flexible objectbased architecture the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions'],\n",
       " 999: ['our first experiment shows the relative contribution of each source of information and demonstrates a success rate for all sources combined experiment investigates a method for unsupervised learning of gendernumberanimaticity information'],\n",
       " 1000: ['marcu has characterised an important and difficult problem in text planning given a set of facts to convey and a set of rhetorical relations that can be used to link them together how can one arrange this material so as to yield the best possible text'],\n",
       " 1003: ['this paper introduces a new unsupervised algorithm for noun phrase coreference resolution',\n",
       "  'in an evaluation on the muc coreference resolution corpus the algorithm achieves an fmeasure of placing it firmly between the worst and best systems in the muc evaluation',\n",
       "  'the clustering algorithm appears to provide a flexible mechanism for coordinating the application of contextindependent and contextdependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes'],\n",
       " 1004: ['this paper describes and evaluates a languageindependent bootstrapping algorithm based on iterative learning and reestimation of contextual and morphological patterns captured in hierarchically smoothed trie models',\n",
       "  'the algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required languagespecific information tokenizers or tools'],\n",
       " 1005: ['a large number of rules is needed for coverage of the domain suggesting that a fairly large number of labeled examples should be required to train a classi however we show that the use of data can reduce the requirements for supervision to just simple seed rules'],\n",
       " 1006: ['three stateoftheart statistical parsers are combined to produce more accurate parses as well as new bounds on achievable treebank parsing accuracy'],\n",
       " 1008: ['in this paper we discuss cascaded memory based grammatical relations assignment',\n",
       "  'we studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder']}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_objetivo_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c01012",
   "metadata": {},
   "source": [
    "### ROUGE-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7006f283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:13.405014Z",
     "start_time": "2023-03-03T02:42:13.147109Z"
    }
   },
   "outputs": [],
   "source": [
    "padrao_ouro_teste_frases = {k: separa_frases_ouro(v) for k, v in padrao_ouro_teste2.items()}\n",
    "padrao_ouro_teste_frases = segundo_preprocessamento(padrao_ouro_teste_frases)\n",
    "padrao_ouro_teste_palavras = {k:[word_tokenize(f) for f in v] for k,v in padrao_ouro_teste_frases.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e527bbab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:13.421120Z",
     "start_time": "2023-03-03T02:42:13.405014Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_padrao_ouro_teste = list(padrao_ouro_teste_frases.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "537fc4e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:16.751597Z",
     "start_time": "2023-03-03T02:42:13.421120Z"
    }
   },
   "outputs": [],
   "source": [
    "#w2v\n",
    "rouge_teste = calculo_rouge(list(frase_objetivo_teste.values()),(padrao_ouro_teste_frases.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a9f61441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:16.767192Z",
     "start_time": "2023-03-03T02:42:16.751597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.642424242424243"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Média de palavras do Padrão Ouro\n",
    "x=[]\n",
    "for i in lista_padrao_ouro_treino:\n",
    "    x.append(len(i))\n",
    "media_ouro_teste=np.mean(x)\n",
    "media_ouro_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8914ea2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:16.783282Z",
     "start_time": "2023-03-03T02:42:16.767192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de papers com precisao>=0.8: 187\n",
      "% do total: 0.8820754716981132\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 11\n",
      "% do total: 0.05188679245283019\n",
      "media precisao: 0.8928227222746559\n",
      "------\n",
      "qtd de papers com revocação>=(1/media_ouro): 159\n",
      "% do total: 0.75\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 11\n",
      "% do total: 0.05188679245283019\n"
     ]
    }
   ],
   "source": [
    "maximo_rouge_por_paper_ft_teste   = max_rouge(rouge_teste)\n",
    "\n",
    "\n",
    "precisao_sg_ft_teste, revocacao_sg_ft_teste = rouge_precisao_revocacao(maximo_rouge_por_paper_ft_teste, lista_padrao_ouro_teste)\n",
    "qtd_precisao_ft_teste, qtd_zero_ft_teste, media_ft_teste = analise_precisao(precisao_sg_ft_teste, lista_padrao_ouro_teste)\n",
    "print('------')\n",
    "qtd_revocacao_ft,  media_revocacao_ft       = analise_revocacao(revocacao_sg_ft_teste, lista_padrao_ouro_teste, media_ouro_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fae9f7",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbdeb0",
   "metadata": {},
   "source": [
    "### Modelo de Linguagem\n",
    "Aplicando o modelo de linguagem nos 30%\n",
    "\n",
    "Escolhido: trigrama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "29f4d6a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:23.159290Z",
     "start_time": "2023-03-03T02:42:16.783282Z"
    }
   },
   "outputs": [],
   "source": [
    "palavras_papers_teste = {k:[word_tokenize(f) for f in v] for k,v in frases_papers_teste.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "48744cbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:23.517038Z",
     "start_time": "2023-03-03T02:42:23.162383Z"
    }
   },
   "outputs": [],
   "source": [
    "#Criando os bigramas da lista\n",
    "trigramas=[]\n",
    "for papers in list(palavras_papers_teste.values()):\n",
    "    x=[]\n",
    "    for frases in papers:\n",
    "        x.append(trigrams(frases))\n",
    "    trigramas.append(x)\n",
    "    \n",
    "lista_trigramas2=[]\n",
    "for j in trigramas:\n",
    "    lista_trigramas=[]  \n",
    "    for i in j:\n",
    "        lista_trigramas.append(list(i))\n",
    "    lista_trigramas2.append(lista_trigramas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "01d3c72a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:41.773104Z",
     "start_time": "2023-03-03T02:42:23.517038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 8.681939995634711e-34, 9.234446772403475e-18}\n",
      "{0.0, 2.570149734654029e-10}\n",
      "{0.0, 2.64683560122929e-38}\n",
      "{0.0, 1.450823815086472e-54, 1.73061012554813e-17}\n",
      "{0.0, 8.064442902301553e-27}\n",
      "{0.0}\n",
      "{0.0, 7.324420256152657e-42}\n",
      "{0.0, 1.4593197655664175e-27, 5.203014049537852e-31}\n",
      "{0.0, 1, 3.6429874848987405e-08}\n",
      "{0.0, 1.9609181041695747e-15, 3.302017036458673e-29}\n",
      "{0.0, 1}\n",
      "{0.0, 2.223526376731727e-54, 5.0805026833054306e-61, 2.822416263511938e-33}\n",
      "{0.0}\n",
      "{0.0, 2.8715056264536765e-27}\n",
      "{0.0, 1.9924833743257123e-32}\n",
      "{0.0, 2.2002500847419e-17, 1.929870127781526e-06, 0.0035842293906810036}\n",
      "{0.0, 1, 1.2194940675195413e-23}\n",
      "{0.0, 1, 7.859069556986966e-29, 1.1694642147111013e-05}\n",
      "{0.0, 1, 0.4166666666666667, 1.221340290613607e-27, 0.00546448087431694}\n",
      "{0.0, 1.1578587258581836e-19, 1.116572937516703e-18, 1}\n",
      "{0.0, 1, 0.1111111111111111, 0.2, 1.526356358419e-09, 6.340789403705126e-27}\n",
      "{0.0, 1, 0.5714285714285714, 4.617895358484145e-08, 4.4103545515167e-27}\n",
      "{1.3170334487514217e-20, 0.0, 4.634779384501298e-07, 1}\n",
      "{0.0, 1, 1.6818395730637348e-68, 5.2374642540751616e-27}\n",
      "{0.0, 4.834761103853362e-33}\n",
      "{0.0, 1, 0.0001512401693889897, 5.403027543755649e-30, 4.776005349125991e-05}\n",
      "{0.0, 1.63157068821016e-37, 1, 3.2495325440483493e-34}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 0.05, 1.8973014878657172e-25, 9.076445052978373e-23}\n",
      "{0.0, 1, 2.7959846726494668e-24}\n",
      "{0.0, 1, 0.5714285714285714, 8.334887347611318e-37}\n",
      "{0.0, 1, 8.121445829681326e-23, 1.7327867135483978e-12}\n",
      "{0.0, 0.125, 1, 2.033384032305634e-31, 0.009345794392523364}\n",
      "{0.0, 0.45454545454545453, 1, 6.927182493852703e-22, 3.247123406834631e-48}\n",
      "{0.0, 1, 0.0018148820326678765}\n",
      "{0.0, 1, 0.0005671506352087115, 1.936220046368033e-17, 3.666275751600019e-32}\n",
      "{0.0, 1, 0.0018427518427518428}\n",
      "{0.0, 2.033372275510754e-31, 1.5890551315155931e-24, 1.0}\n",
      "{0.0, 2.2969560085929787e-19, 1, 0.001869158878504673, 0.0003024803387779794, 1.7733142182543823e-22, 0.0002135155332550443, 3.279964789999916e-33}\n",
      "{0.0, 1, 2.0101260722134018e-57, 0.11428571428571428}\n",
      "{0.0, 1, 5.166661427586004e-28}\n",
      "{0.0}\n",
      "{0.0, 1}\n",
      "{0.0, 1.3481358191591286e-19, 1.2895999030750364e-30, 1}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 1.6928945228733845e-56, 2.4304803282188457e-49, 0.0003024803387779794}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 1.5959520115675482e-64}\n",
      "{0.0, 1, 0.0035842293906810036, 2.256088184799888e-24}\n",
      "{0.0, 1, 0.0009297520661157025, 9.964632574446614e-14, 3.633692381423779e-05, 3.594841581803438e-31, 4.6828166483900056e-42}\n",
      "{0.0, 1, 1.8605890363953138e-15, 9.625362245113612e-21, 8.642295393656554e-05, 0.002331002331002331, 0.01639344262295082}\n",
      "{0.0, 1, 1.0359765138977618e-18, 4.395464816576839e-37}\n",
      "{0.0, 1, 0.11249999999999999, 0.5, 0.00013785497656465398}\n",
      "{0.0, 1, 0.0625, 1.335113484646195e-05}\n",
      "{0.0, 1, 0.6666666666666666, 0.25, 0.18181818181818182, 1.2445701891786512e-08, 2.6812378510728915e-33, 3.9601258743759824e-29}\n",
      "{0.0, 1, 0.15, 1.8658130909044173e-31, 9.883317793859977e-29, 5.34045393858478e-05}\n",
      "{0.0, 1, 8.124554194958235e-05, 0.05357142857142857, 4.5274217380732293e-26}\n",
      "{0.0, 1, 0.03225806451612903, 0.0011870061779317678, 4.712873775774932e-05}\n",
      "{0.0, 1, 1.9539598389138413e-30, 1.0591933640241144e-20}\n",
      "{0.0, 1, 1.4270122454774454e-27}\n",
      "{0.0, 1, 5.707182208483858e-22, 1.8735711428921248e-36, 1.0742346591817731e-06, 4.1717538103756356e-08}\n",
      "{0.0, 1, 4.46309536344816e-20, 5.978048750456472e-25}\n",
      "{0.0, 1.5256074450756157e-27, 1.58145047614693e-39}\n",
      "{0.0, 1}\n",
      "{0.0, 1.5952632282364288e-18, 2.4748704462194932e-39}\n",
      "{0.0, 4.6356965895603524e-29}\n",
      "{0.0, 2.0215522380034128e-36}\n",
      "{0.0, 6.557360938456128e-44, 8.555514113287031e-51}\n",
      "{0.0, 3.4039199260125777e-16, 2.5782642347080755e-18, 1}\n",
      "{0.0, 0.5, 1.3248442540511614e-20}\n",
      "{0.0, 2.6058714018805172e-51}\n",
      "{0.0, 1.8850172739418245e-55, 1.0057465438359382e-47}\n",
      "{0.0, 1.3163125746788998e-27, 6.721008190128409e-42}\n",
      "{0.0, 1.1834775755548883e-50}\n",
      "{0.0, 2.430483802770062e-25, 2.0321155985814356e-11, 1.2121695443959027e-28}\n",
      "{0.0, 1, 8.230318999156791e-41, 4.6455684856943e-31}\n",
      "{0.0, 1, 7.256752240350744e-31}\n",
      "{0.0, 3.632967621138678e-18, 6.0303097842499885e-09}\n",
      "{0.0, 1.5549308942414936e-23, 3.962867833794523e-21}\n",
      "{0.0, 1, 6.83135461962116e-24, 1.9006484692297213e-31}\n",
      "{0.0, 1.2770457857276354e-44}\n",
      "{0.0, 6.230786010380974e-37, 0.06926589392342818}\n",
      "{0.0, 3.1515525244382654e-29, 1}\n",
      "{0.0, 3.4342455003532483e-29, 5.310882112996194e-35}\n",
      "{0.0, 2.2567740851852106e-15, 6.0702521830862255e-27}\n",
      "{0.0, 1, 0.000949667616334283}\n",
      "{0.0, 4.2779390538011914e-41}\n",
      "{0.0, 1}\n",
      "{0.0, 5.405665683811177e-38, 1.9994645252428407e-14, 1}\n",
      "{0.0, 1}\n",
      "{0.0, 3.049306582251997e-32}\n",
      "{0.0, 3.778451181716861e-33, 1}\n",
      "{0.0, 5.0046754489438696e-33, 1.0897134533416692e-20}\n",
      "{0.0, 2.564127486649778e-45}\n",
      "{0.0, 2.680845694453549e-35}\n",
      "{0.0, 2.986129785603778e-20, 2.028285255618954e-15}\n",
      "{0.0, 1, 4.900985381427458e-10, 5.13645292908475e-28}\n",
      "{0.0, 1.3356070158068408e-29}\n",
      "{0.0, 1, 1.3940899297041085e-27, 9.986090139312556e-17}\n",
      "{0.0, 2.7551387329830964e-32, 6.0303097842499885e-09, 4.4372244435906977e-45}\n",
      "{0.0, 1, 5.875261317243174e-57}\n",
      "{0.0, 1, 9.107077967999998e-34, 6.0303097842499885e-09, 6.7468137626312e-44}\n",
      "{0.0, 1, 5.347019153955133e-36}\n",
      "{0.0}\n",
      "{0.0, 1, 3.6332743641321e-21}\n",
      "{0.0, 2.071115840210365e-18, 2.977251416985978e-38}\n",
      "{0.0, 3.695185962129852e-38, 1.5415802899253295e-46, 1}\n",
      "{0.0, 7.874500243613241e-39}\n",
      "{0.0, 7.414645380040786e-33}\n",
      "{0.0, 0.2, 0.1891891891891892, 8.364170131664468e-33}\n",
      "{0.0, 8.194867226054208e-16, 2.9780068460198496e-20}\n",
      "{0.0, 1, 3.819678545882696e-29, 8.044647070440568e-12}\n",
      "{0.0, 1, 1.3723304449188768e-21, 1.7755642983360247e-25, 1.1408724715221901e-26}\n",
      "{0.0, 2.3880173816083614e-26, 9.212730838330243e-31}\n",
      "{0.0, 2.997105130242757e-62}\n",
      "{0.0}\n",
      "{0.0, 1, 3.821350895110924e-69}\n",
      "{0.0, 5.094433517088327e-12, 3.260669955508811e-09}\n",
      "{0.0, 7.762656134183555e-18, 4.2644673629907464e-36}\n",
      "{0.0, 2.833023282492462e-31, 2.674499043589138e-35}\n",
      "{0.0}\n",
      "{0.0, 1.2989241827615335e-30, 8.322421235778037e-26, 1}\n",
      "{0.0, 4.902443911026036e-33, 1, 6.635141676965381e-18}\n",
      "{0.0, 9.14441805073421e-23, 4.98823072788029e-16, 1}\n",
      "{0.0, 1, 6.969801629014271e-29, 2.861531162642102e-21}\n",
      "{0.0, 0.0005498533724340176, 1.9845361623508013e-41}\n",
      "{0.0, 4.317471104700648e-68, 1.7505362912741274e-32}\n",
      "{0.0, 1.3442270207744939e-33, 3.235813989232829e-05}\n",
      "{0.0, 2.4526318058879303e-34, 9.294926616605273e-29}\n",
      "{0.0}\n",
      "{0.0, 1, 2.163792665872594e-22, 1.0017954049852125e-31, 0.00010013351134846461}\n",
      "{0.0, 0.043478260869565216, 1, 7.274227587251063e-50, 7.220465921131643e-15}\n",
      "{0.0, 1, 7.048110401601329e-06, 3.9029721132642505e-07, 2.960110199688449e-35}\n",
      "{0.0, 1, 0.75, 1.3296935597810265e-31, 1.1445794567513677e-30}\n",
      "{0.0, 1, 0.02857142857142857, 1.91888006282853e-18}\n",
      "{0.0, 6.127200766535521e-62, 3.261194488777097e-89, 1}\n",
      "{0.0, 2.0856454636333908e-38, 3.052405230999719e-24, 6.064532756483063e-24}\n",
      "{0.0, 2.5708185676479396e-25, 1}\n",
      "{0.0, 1, 0.17391304347826086, 2.970952478465592e-22, 1.2495489606568377e-29, 0.0031531531531531535, 5.34045393858478e-05}\n",
      "{0.0, 6.442396843210159e-38, 0.047619047619047616, 1, 8.019856158709349e-51}\n",
      "{0.0, 1, 0.5714285714285714, 1.1375957139432258e-43, 0.020560747663551402}\n",
      "{0.0, 0.0625, 2.1847553670209574e-55, 1, 0.0027223230490018148, 2.574459640822255e-62, 3.788598486284093e-08}\n",
      "{0.0, 1, 9.123830914124223e-07, 1.6288058074073926e-39, 5.686860829768871e-23}\n",
      "{0.0, 1, 3.130776283272741e-33, 0.014336917562724014, 1.9963395879355866e-16}\n",
      "{0.0, 1, 9.227943684205663e-42, 0.05555555555555555}\n",
      "{0.0, 1, 1.70278030725189e-45}\n",
      "{0.0, 1, 3.565287363944177e-07}\n",
      "{0.0, 1, 0.22033898305084745, 0.1111111111111111, 0.00031595576619273305, 4.1797923740405455e-31}\n",
      "{0.0, 2.3491527565405836e-34, 3.073887794649956e-45, 1}\n",
      "{0.0, 1, 1.589286178758327e-30}\n",
      "{0.0, 5.573189598153105e-29, 1.7956946389892448e-46}\n",
      "{0.0, 1, 0.05263157894736842, 0.02857142857142857, 1.0677379978485631e-65}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 7.288300363928051e-118, 5.930584581949698e-13}\n",
      "{0.0, 1.1147897021347439e-07}\n",
      "{0.0, 1, 1.3967257032762717e-43, 1.0218296263345291e-57, 1.1498800885393085e-27, 5.942275042444821e-05}\n",
      "{0.0, 1}\n",
      "{0.0, 8.851277781092425e-10, 7.538452748244847e-50, 1.4179369018078695e-05}\n",
      "{0.0, 2.6241416045095135e-74, 2.2220726859890914e-08, 1}\n",
      "{0.0, 1, 4.834102328371414e-41}\n",
      "{0.0, 1.0534869942520315e-21}\n",
      "{0.0, 1}\n",
      "{0.0, 4.327619909247241e-23, 3.3483013554086504e-66, 1.0578756430136748e-15}\n",
      "{0.0, 6.3813832489471234e-40, 8.450781051798899e-46}\n",
      "{0.0, 1}\n",
      "{0.0, 1.8943949278082906e-51, 1, 0.014336917562724014}\n",
      "{0.0, 4.232043914544054e-18, 1}\n",
      "{0.0, 8.727629295968249e-27}\n",
      "{0.0, 1, 0.01}\n",
      "{0.0, 1.0580460006896627e-89, 2.8769549235278522e-58, 4.653540340814461e-39}\n",
      "{0.0, 1, 2.457735381288401e-35}\n",
      "{0.0, 3.1502996687422273e-19, 0.2, 1, 0.00035106196243636997, 3.786743968139554e-09}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 2.4213904647047216e-33, 7.909193947314006e-20}\n",
      "{0.0, 1, 0.00014693542073465412, 0.014336917562724014}\n",
      "{0.0, 1, 2.1925132908878667e-27}\n",
      "{0.0, 4.839413220808883e-27, 1, 1.6690607394249001e-53, 4.211705972810367e-31}\n",
      "{0.0, 3.2852964878598496e-33}\n",
      "{0.0, 2.276395364276391e-32, 6.048867030719408e-34, 1}\n",
      "{0.0}\n",
      "{0.0, 1.1469313362974174e-23}\n",
      "{0.0, 6.711947343157447e-33}\n",
      "{0.0, 1.0692998767340017e-42, 9.897463484471095e-53, 3.856692588224055e-24}\n",
      "{0.0, 4.369294668021833e-43}\n",
      "{0.0, 6.495897495948558e-14, 7.55813529064393e-44}\n",
      "{0.0, 1, 5.905525392866936e-26}\n",
      "{0.0, 1.170722886597842e-33}\n",
      "{0.0, 7.141133633907976e-39, 6.839183524822609e-30, 9.21113464795898e-32}\n",
      "{0.0, 2.1813226811613172e-51}\n",
      "{0.0, 1.4310803494660488e-43, 9.625402954560191e-27}\n",
      "{0.0, 1, 4.0971594807415474e-36}\n",
      "{0.0, 1, 2.4571961342846773e-25}\n",
      "{0.0, 6.023567853200978e-25, 4.1366169878318114e-26, 1}\n",
      "{0.0, 8.007287037638349e-38, 1.713160738004083e-29}\n",
      "{0.0, 1.424814913089856e-32}\n",
      "{0.0, 2.9936843692660783e-32}\n",
      "{0.0, 1, 4.145347275127171e-41, 5.356579889390417e-51}\n",
      "{0.0, 2.5929253733046017e-28, 1.4647209371299142e-20}\n",
      "{0.0, 3.8715203098574393e-28, 4.684025921421761e-28}\n",
      "{0.0, 9.383280937582674e-66}\n",
      "{0.0, 2.8549296639531094e-29}\n",
      "{0.0, 9.699898333322255e-20}\n",
      "{0.0, 5.288475126554023e-44}\n",
      "{0.0, 1, 2.9438345437015077e-12, 0.014336917562724014}\n",
      "{0.0}\n",
      "{0.0, 1, 1.8830007132708782e-10, 6.597649942917495e-29, 0.00016977928692699492, 0.05454545454545454}\n",
      "{0.0, 1.0}\n",
      "{0.0, 1.5818752893447372e-39}\n",
      "{0.0, 1.597971190474742e-29, 8.261538146927087e-21}\n",
      "{0.0, 3.748223789505694e-21}\n",
      "{0.0, 4.571703204695194e-26}\n",
      "{0.0, 0.03655868001408522, 8.919040285263748e-45}\n",
      "{0.0, 7.677389057553605e-45, 1.0937501966477322e-18}\n",
      "{0.0, 1.0517016994536894e-12, 1.8821028866392848e-27}\n",
      "{0.0}\n",
      "{0.0, 6.924525829192318e-50, 5.475432351550372e-13}\n",
      "{0.0, 1, 1.025958660122606e-26, 2.8288348117375155e-22}\n",
      "{0.0, 1, 2.2828102683326337e-40}\n",
      "{0.0, 1.188347580328044e-28}\n",
      "{0.0, 4.004089964315376e-69, 1, 1.0118174531443029e-20}\n",
      "{0.0, 1, 1.2063116500663756e-27, 2.5332544651393892e-27}\n",
      "{0.0, 6.424052452173656e-19, 1.244278494554065e-27, 1}\n",
      "{0.0, 1.9979688317193915e-16, 3.643810232060622e-48}\n",
      "{0.0, 1, 0.00980392156862745, 4.681554016802481e-45}\n",
      "{0.0, 3.3954760216695708e-22, 1.7846347316105174e-44}\n",
      "{0.0, 2.3192657330166282e-10, 1.3398553663109128e-35}\n",
      "{0.0, 0.5, 9.206722859796088e-12, 5.032766428982997e-57, 1.8962925115960655e-08}\n",
      "{0.0}\n",
      "{0.0, 1.4970084176596386e-11}\n",
      "{0.0, 1}\n",
      "{0.0, 5.642106577624592e-38, 8.227443745109565e-25}\n",
      "{0.0, 1.6544933011739292e-40}\n",
      "{0.0, 1, 1.5093713614756466e-26, 7.825448558865257e-35}\n",
      "{0.0, 1.6503224316949037e-20, 4.6954675505294004e-23, 1.1877523186045675e-27}\n",
      "{0.0, 1.1212127655872737e-05, 1.0389083534519309e-19, 2.3394829328261147e-42}\n",
      "{0.0, 7.191996786242132e-19, 9.666587502209064e-33, 3.7085406687000986e-14}\n",
      "{0.0, 5.656680724625147e-40, 2.479099113321801e-35, 7.339374365229396e-43}\n",
      "{0.0, 1, 3.1938807131304095e-18}\n",
      "{0.0}\n",
      "{0.0, 5.7163136153308347e-39}\n",
      "{0.0, 4.0628701404460173e-51, 2.499163843750825e-12}\n",
      "{0.0, 3.057200116634596e-40, 8.425340102201467e-16}\n",
      "{0.0}\n",
      "{0.0, 6.260479949801736e-19}\n",
      "{0.0, 1, 0.0016360432922225018, 6.932249107443309e-26, 1.4613378515371417e-36}\n",
      "{0.0, 4.070348269125492e-40, 0.42857142857142855}\n",
      "{0.0}\n",
      "{0.0, 1.7001073538514035e-30, 1.4679539952128437e-26, 1}\n",
      "{0.0, 9.374166815661508e-34, 5.96015370534084e-38}\n",
      "{0.0, 1, 1.3421719539554212e-48, 2.3455841015376066e-36}\n",
      "{0.0, 1, 2.9592624967088615e-29, 3.392879554365981e-24}\n",
      "{0.0, 1.230253018802817e-32, 1.6263672816998144e-63}\n",
      "{0.0, 1.4797193687867038e-19, 9.516939305382196e-25}\n",
      "{0.0, 2.518061995032717e-40}\n",
      "{0.0, 2.1721047215456036e-17, 1.9461430417846328e-16}\n",
      "{0.0, 2.1317316385145082e-40}\n",
      "{0.0, 1.7115461687227392e-43, 4.376889834137349e-34}\n",
      "{0.0}\n",
      "{0.0, 1.058511036445338e-39}\n",
      "{0.0, 3.89045484666584e-26}\n",
      "{0.0, 1.3809308229607874e-55, 1}\n",
      "{0.0, 1, 3.3868646200054603e-29}\n",
      "{0.0, 1, 5.186103405767074e-42}\n",
      "{0.0, 3.234858642710046e-16, 1.3103708557081774e-17}\n",
      "{0.0, 1.9788013941331056e-19}\n",
      "{0.0}\n",
      "{0.0, 6.633208286185692e-49, 1, 5.929788786043953e-28, 5.34045393858478e-05}\n",
      "{0.0, 8.96812710574414e-25}\n",
      "{0.0, 1.4156939146845142e-26, 6.368110448717798e-57}\n",
      "{0.0, 1.9326569723834723e-17, 3.926788207902336e-24}\n",
      "{0.0}\n",
      "{0.0, 7.612905224639867e-31, 1.298338771749159e-21}\n",
      "{0.0, 1.3486088254113744e-29}\n",
      "{0.0, 1, 8.459492166528377e-28}\n",
      "{0.0, 1, 1.4813006213483554e-36, 1.4972692929110807e-35}\n",
      "{0.0, 1, 7.424829412064267e-52}\n",
      "{0.0, 2.9904994646429454e-26}\n",
      "{0.0, 5.551868317410612e-13}\n",
      "{0.0, 2.5628273707376366e-17, 1.5977128282227603e-46}\n",
      "{0.0, 1}\n",
      "{0.0, 3.969275130241171e-28}\n",
      "{0.0, 1.8441855023839278e-21, 3.25051705446675e-13, 3.780969829499111e-16}\n",
      "{0.0, 4.6678058572939605e-27, 7.013033501022012e-45, 1}\n",
      "{0.0, 2.219209467563501e-19, 1}\n",
      "{0.0, 1, 2.040616236796929e-36}\n",
      "{0.0, 2.785987251644748e-19, 7.269530915684044e-24, 1.9727572557836167e-31}\n",
      "{0.0, 3.945691242761849e-42}\n",
      "{0.0, 7.132310432265867e-19, 2.834088467560412e-20, 1, 1.0166680696686025e-08}\n",
      "{0.0, 1.0007290855848494e-18, 1.4299842540834358e-13}\n",
      "{0.0, 1, 1.4286718233438847e-18}\n",
      "{0.0, 0.0273972602739726, 1.584133332486297e-29}\n",
      "{0.0, 1, 4.148086706285847e-21}\n",
      "{0.0, 7.003909349984759e-17, 5.3779670934143025e-11, 1.1787373524284748e-10, 2.805667872425499e-23}\n",
      "{0.0, 7.278439827539761e-20, 1.970171753695093e-28}\n",
      "{0.0, 5.091448542108755e-49, 8.496328434658136e-35, 2.53930937932076e-29}\n",
      "{0.0, 1.993366891077158e-27}\n",
      "{0.0, 5.447272497158473e-32}\n",
      "{0.0}\n",
      "{0.0, 1, 1.0637790362732987e-16}\n",
      "{0.0, 4.80856815890489e-24, 1.1690302082062078e-16}\n",
      "{0.0, 3.6443394940180294e-28, 1.3715743377404795e-39}\n",
      "{0.0, 1.919102522439684e-20}\n",
      "{0.0, 1.9955203821825808e-48}\n",
      "{0.0, 2.7730957526887993e-40, 4.2638967942141726e-15}\n",
      "{0.0, 7.269643946403933e-50}\n",
      "{0.0, 1, 6.875540884622994e-50}\n",
      "{0.0, 1.4785754082236715e-37, 7.006888645792618e-49}\n",
      "{0.0, 1.266294911461594e-19, 1.6093508742443597e-23}\n",
      "{0.0, 4.713860915619572e-27, 6.756910492697145e-44}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 1.0296900444303357e-18, 3.6456436174347674e-60}\n",
      "{0.0, 4.5437409997565586e-26, 2.3272755625781114e-41}\n",
      "{0.0, 4.725295399389174e-37, 6.25192637481424e-06}\n",
      "{0.0, 3.3888777343083603e-16, 1.1005637858678046e-19}\n",
      "{0.0, 5.335299448857913e-12, 3.287376991340694e-29}\n",
      "{0.0, 9.413579547891597e-55}\n",
      "{0.0, 9.07107909893865e-30, 9.366500449436438e-27}\n",
      "{0.0, 1, 2.2712714035815427e-44}\n",
      "{0.0, 1, 1.9258249889432272e-23}\n",
      "{0.0, 1, 2.1265673799157885e-29}\n",
      "{0.0, 3.0718127311094324e-19, 5.690833333824402e-15, 1}\n",
      "{0.0, 1, 3.894583927964741e-27}\n",
      "{0.0, 1, 1.0239648637512816e-33}\n",
      "{0.0, 1.2068369764406642e-17, 4.2482737074569437e-36}\n",
      "{0.0, 1.5179600301082269e-35}\n",
      "{0.0, 9.899048345274183e-32, 8.826215809379341e-29, 6.8241069852869136e-15}\n",
      "{0.0, 9.969335697076013e-29}\n",
      "{0.0, 0.06666666666666667, 0.125, 0.07758620689655173, 6.657662784442605e-21, 1.7559699797961445e-20, 0.017857142857142856}\n",
      "{0.0, 1.550847732767391e-22}\n",
      "{0.0, 1.155912532232463e-45, 2.6547474606372184e-08, 4.625240112158856e-22}\n",
      "{0.0, 2.5223399611820675e-31, 4.4610861034709713e-29, 5.586759927576387e-17}\n",
      "{0.0, 9.945229850138403e-44, 0.013157894736842105, 1}\n",
      "{0.0, 2.9893868504787425e-47}\n",
      "{0.0, 1.0321412472927998e-15, 1.2089153374770023e-21}\n",
      "{0.0, 1, 1.0090855658541152e-19, 7.911819302798673e-26, 9.457174902140073e-45}\n",
      "{0.0, 6.01207004202298e-12, 1, 1.4452537865649207e-05}\n",
      "{0.0, 6.702141393411868e-28}\n",
      "{0.0, 3.109321337126842e-38}\n",
      "{0.0, 4.097319929374715e-24, 1.3611321214051689e-30, 5.9190258937914197e-36}\n",
      "{0.0, 2.8846265549955044e-27, 6.3311173539247815e-16, 7.828199837441005e-24}\n",
      "{0.0, 6.217548792744614e-33, 1.2866733560263078e-25}\n",
      "{0.0, 2.824715179554185e-16, 1.891328150928682e-34, 1.922286617559405e-45, 6.621238518673463e-43}\n",
      "{0.0, 1.4351532313697602e-24}\n",
      "{0.0, 1.051004553632375e-37, 1}\n",
      "{0.0, 2.283552640427321e-24, 3.6919613384244466e-26}\n",
      "{0.0, 3.6838983377053313e-48}\n",
      "{0.0}\n",
      "{0.0, 2.1272984155729344e-26, 3.971713305082995e-09}\n",
      "{0.0, 6.624115212717897e-39}\n",
      "{0.0, 2.978555721635616e-56, 2.9918049089947896e-33, 1}\n",
      "{0.0}\n",
      "{0.0, 1, 1.4612585233207369e-36}\n",
      "{0.0, 1, 7.1684826190480385e-34}\n",
      "{0.0, 3.258981341151528e-38, 1.1316253134745783e-12, 1.0816975458252128e-27}\n",
      "{0.0, 1.370064388314456e-35, 4.546758874541256e-70, 7.172875804899321e-42}\n",
      "{0.0, 9.698348892554628e-25}\n",
      "{0.0, 9.629662566264076e-31}\n",
      "{0.0, 0.375, 1.332393995566459e-05, 1, 2.0438400457719065e-20, 8.601282670673042e-16}\n",
      "{0.0, 1, 9.76694900373343e-23, 2.3412457825833693e-27}\n",
      "{0.0, 1, 6.196033495938999e-60}\n",
      "{0.0, 1, 3.725657686920773e-17}\n",
      "{0.0, 1, 4.161259954079021e-35, 1.485036223807246e-25}\n",
      "{0.0, 1.883342745658756e-22, 0.2, 3.591756636771452e-30}\n",
      "{0.0, 8.046922435650789e-24, 1.9068979654545567e-47, 3.3904872614550026e-06, 1.2240153251838458e-20}\n",
      "{0.0, 1, 1.7094446752008738e-23, 8.483353791959128e-46}\n",
      "{0.0, 1.0438867993976132e-35}\n",
      "{0.0, 1.8821028866392848e-27}\n",
      "{0.0, 1, 3.7183000033703104e-28, 1.326356939636161e-38}\n",
      "{0.0, 6.126384576632592e-54, 4.085043984863776e-36}\n",
      "{0.0, 1.0560442272916686e-25, 6.110631658618853e-30, 1}\n",
      "{0.0, 1}\n",
      "{0.0, 5.2537485447181905e-77, 0.2, 1, 2.9437455065967685e-38}\n",
      "{0.0, 1, 4.053220522467827e-33}\n",
      "{0.0, 6.014509698466768e-43}\n",
      "{0.0}\n",
      "{0.0, 3.326897594704485e-27, 4.403675687070391e-22}\n",
      "{0.0, 1, 1.8226988411998352e-45}\n",
      "{0.0, 3.596607587491393e-37}\n",
      "{0.0, 1, 3.506340859489966e-24, 8.673927370327611e-17}\n",
      "{0.0, 1.3048156982748724e-39, 2.839332127979847e-25}\n",
      "{0.0, 1, 1.4755601734841626e-32}\n",
      "{0.0, 1, 1.841444200423388e-23, 4.349731647655729e-08, 3.7283018220280557e-32, 2.6547474606372184e-08}\n",
      "{0.0, 1, 1.782926958772994e-23}\n",
      "{0.0, 3.771084366107947e-42}\n",
      "{0.0, 1, 0.05}\n",
      "{0.0, 4.603094581070444e-33}\n",
      "{0.0, 1, 1.8963177117441596e-55}\n",
      "{0.0, 1.8068054808666446e-50, 6.027410731066952e-14}\n",
      "{0.0, 5.134539988144358e-17, 4.849513319980373e-68}\n",
      "{0.0, 1, 5.768906049638191e-21, 0.3333333333333333}\n",
      "{0.0}\n",
      "{0.0}\n",
      "{0.0, 2.8367025996991704e-32, 1.3576184573551107e-25}\n",
      "{0.0, 1, 7.539977955266517e-30}\n",
      "{0.0, 7.599009002440862e-29}\n",
      "{0.0, 1, 5.568124405773555e-31, 5.928286078441275e-43}\n",
      "{0.0, 2.5047773530824904e-32, 3.5148920061514526e-34, 4.201558131899141e-16}\n",
      "{0.0, 3.8046033667524426e-22, 1.1451680843749813e-30, 1.8872957534446066e-22}\n",
      "{0.0, 7.210230707000915e-30, 1.543662962445111e-42}\n",
      "{0.0, 3.1942393346593536e-40}\n",
      "{0.0, 3.6816361130881636e-15, 7.4804240286972615e-31, 5.9050547890425296e-12}\n",
      "{0.0, 1, 4.279304745075357e-37, 3.911713706036364e-29, 1.257176616057611e-33}\n",
      "{0.0, 1, 2.3976528915313295e-34, 1.6313639908666323e-31}\n",
      "{0.0, 1.203272451119403e-29, 2.0742738800058912e-32}\n",
      "{0.0, 1, 2.9210350885462694e-21, 2.0774111891764365e-33}\n",
      "{0.0, 2.6418843078361136e-28, 4.2998286122586186e-41}\n",
      "{0.0, 2.047087299594171e-35}\n",
      "{0.0, 1, 1.3218955322319222e-20, 2.2567906332895404e-26}\n",
      "{0.0, 2.5470263240699948e-23, 2.8511904766524037e-33}\n",
      "{0.0, 1.7546394282133075e-31, 1.0644079230927796e-05}\n",
      "{0.0, 6.919006095343235e-41, 8.491636129134914e-65, 0.0011741682974559685}\n",
      "{0.0, 3.3303095365969203e-41}\n",
      "{0.0, 2.245039206184029e-42, 5.403200300512779e-24}\n",
      "{0.0, 1, 1.4362017179530837e-18, 3.4299325372628233e-32}\n",
      "{0.0, 1.0869316483008551e-34, 1, 1.0280731023363039e-32, 7.929173830486438e-10}\n",
      "{0.0, 2.695654633960792e-32}\n",
      "{0.0, 1, 1.0539415180318714e-81}\n",
      "{0.0, 3.7311666577751156e-22, 4.958775648354802e-34}\n",
      "{0.0, 9.30150477337072e-42, 1.1764902115237407e-15}\n",
      "{0.0, 4.212423202360753e-09, 1.5138614629702183e-35, 7.829345619243253e-23, 1.5449961614916416e-39}\n",
      "{0.0, 3.581983793345688e-36, 0.0007048739285668215, 2.6285372704535e-43}\n",
      "{0.0, 1.582118599315973e-24, 3.8383839065286677e-25}\n",
      "{0.0, 4.416085841948066e-39}\n",
      "{1.4738170414415348e-39, 0.0, 6.424250450411723e-13, 3.1525934925872185e-14}\n",
      "{0.0, 1.0424685328012416e-45}\n",
      "{0.0, 9.592729402947352e-42}\n",
      "{0.0, 2.2799904233745006e-31, 2.3038660837301967e-32, 9.794303685567746e-43}\n",
      "{0.0, 1, 9.839523990086152e-17, 0.0469208211143695, 4.663637118671752e-16}\n",
      "{0.0, 9.660178276501245e-48, 7.548666009038855e-31}\n",
      "{0.0, 7.81685997912076e-45, 6.422595968216106e-62}\n",
      "{0.0, 1}\n",
      "{0.0, 3.213344409243054e-19, 4.058903509706054e-21, 1}\n",
      "{0.0, 2.2953559193101762e-35, 6.037532122526136e-44}\n",
      "{0.0, 1, 2.0678172900482805e-21, 1.4609313814422457e-26, 2.595053052752559e-31}\n",
      "{0.0, 5.852626590404906e-14, 1.5442807825285636e-26, 2.6144977076836552e-33}\n",
      "{0.0, 1, 7.888834240439088e-26}\n",
      "{0.0, 1.1665349136268869e-55}\n",
      "{0.0, 7.719751651257789e-25}\n",
      "{0.0, 4.064486636496263e-49}\n",
      "{0.0, 1, 5.367285936352893e-49, 5.822644847410909e-46, 8.04897495530409e-15}\n",
      "{0.0, 1.673383494220954e-23, 3.092040780595271e-54}\n",
      "{0.0, 1, 0.17391304347826086, 6.110575906905792e-44, 0.0001869158878504673}\n",
      "{0.0, 9.311929709844701e-40}\n",
      "{0.0, 6.668178165011771e-11, 2.1878857680803628e-17}\n",
      "{0.0}\n",
      "{0.0, 2.2119239610461743e-24, 6.340547565064919e-22, 1.648682271623749e-27}\n",
      "{0.0, 1.1269658438653834e-28}\n",
      "{0.0, 2.8816739598771805e-33, 7.475633600433938e-32}\n",
      "{0.0}\n",
      "{0.0}\n",
      "{0.0, 2.4998083979323587e-47}\n",
      "{0.0, 1}\n",
      "{0.0, 2.7215596435013193e-56}\n",
      "{0.0, 5.235298603281662e-28}\n",
      "{0.0, 1, 3.4657147817971507e-41}\n",
      "{0.0, 1.4823050272009798e-26, 1}\n",
      "{0.0, 3.414087065836355e-31}\n",
      "{0.0, 1, 2.2523159892753324e-35}\n",
      "{0.0, 6.320712924184003e-41, 1.988714732283848e-24, 1}\n",
      "{0.042735042735042736, 0.0, 7.96946148417341e-21, 1.0392238722905883e-16}\n",
      "{0.0, 8.264135930057277e-22}\n",
      "{0.0, 5.22741730023789e-36, 3.740817819849728e-21, 2.090167697147873e-34}\n",
      "{0.0, 1, 5.272947529324485e-23}\n",
      "{0.0, 6.303135772156277e-33, 1, 3.841218286152079e-67}\n",
      "{0.0, 2.631856863147355e-32}\n",
      "{0.0, 1.9374038325893885e-31, 2.0275151175289167e-30}\n",
      "{0.0, 1, 6.541433589812441e-26}\n",
      "{0.0, 6.880630489391461e-33, 6.862267888836796e-18}\n",
      "{0.0, 1.2756382433796416e-27}\n",
      "{0.0, 1.6644853999945205e-36, 1}\n",
      "{0.0, 6.817656091166871e-70, 1}\n",
      "{0.0, 5.888807199795138e-40, 2.1488348666588063e-44}\n",
      "{0.0, 1, 7.200665510086388e-16}\n",
      "{0.0, 3.224916998363715e-34, 5.6251852711694404e-24}\n",
      "{0.0, 1, 1.9112565774701696e-18, 5.717529783753831e-30, 4.1338194875798137e-16}\n",
      "{0.0, 8.727629295968249e-27}\n",
      "{0.0, 1, 5.751307522913622e-37, 3.753691040248239e-21, 9.938723352557316e-45}\n",
      "{0.0, 0.125, 6.1101367009800354e-30}\n",
      "{0.0, 9.353797784678939e-39}\n",
      "{0.0, 3.1502996687422273e-19}\n",
      "{0.0, 1.252514325622368e-23, 2.253863051398131e-44}\n",
      "{0.0, 3.646362450519353e-42}\n",
      "{0.0, 1, 1.4079051292925125e-26}\n",
      "{0.0}\n",
      "{0.0, 1.8631346665884445e-20}\n",
      "{0.0, 3.667062858877272e-20}\n",
      "{0.0, 2.457076807962594e-37, 2.449483128836293e-27, 1.9118382088406034e-22}\n",
      "{0.0, 1.545652679066915e-29}\n",
      "{0.0, 1, 1.9505378419572683e-41}\n",
      "{0.0, 3.5453968404521353e-22, 6.865011536085767e-20}\n",
      "{0.0, 2.057216385514002e-22, 3.243572909488594e-29, 1.38398487382114e-21}\n",
      "{0.0, 1, 1.496647509578544e-05, 3.147945832104675e-46}\n",
      "{0.0, 9.843193228995955e-32, 0.00546448087431694, 1.2853280338752936e-34}\n",
      "{0.0, 2.269631132946856e-43, 8.936392746306113e-71, 1}\n",
      "{0.0, 4.360880681646643e-75}\n"
     ]
    }
   ],
   "source": [
    "# Probabilidade de cada frase do paper, com base nos trigramas\n",
    "lista_paper_prob_tri=[]\n",
    "lista_paper_perp_tri=[]\n",
    "for paper in lista_trigramas2:\n",
    "    lista_prob_tri=[]\n",
    "    lista_perp_tri=[]\n",
    "    for frase in paper:\n",
    "        prob=1\n",
    "        for trigrama in frase:\n",
    "            prob = prob * (model_tri.score(trigrama[2], context=[trigrama[0],trigrama[1]])) #to na dúvida se é 1 e 0 ou 0 e 1\n",
    "        lista_prob_tri.append(prob)\n",
    "        try:\n",
    "            lista_perp_tri.append(model_tri.perplexity(frase))\n",
    "        except:\n",
    "            lista_perp_tri.append(math.inf)\n",
    "    lista_paper_prob_tri.append(lista_prob_tri)\n",
    "    lista_paper_perp_tri.append(lista_perp_tri)\n",
    "    \n",
    "for i in lista_paper_prob:\n",
    "    print(set(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8e42e2d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:41.789086Z",
     "start_time": "2023-03-03T02:42:41.774070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tri.score('presents', context=['paper','this'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c96ff595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:41.804728Z",
     "start_time": "2023-03-03T02:42:41.789086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16161616161616163"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tri.score('presents', context=['this','paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "89c475f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:41.836453Z",
     "start_time": "2023-03-03T02:42:41.804728Z"
    }
   },
   "outputs": [],
   "source": [
    "prob_frase_teste=[]\n",
    "for frase,prob in zip(list(frases_papers_teste.values()),lista_paper_prob_tri):\n",
    "    dicionario={}\n",
    "    for vl_frase, vl_prob in zip(frase,prob):\n",
    "        #if vl_prob!=1:\n",
    "            dicionario[vl_frase]= vl_prob\n",
    "    prob_frase_teste.append(dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "be2724aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:41.852403Z",
     "start_time": "2023-03-03T02:42:41.836453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prob_frase_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5b988ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:42.009541Z",
     "start_time": "2023-03-03T02:42:41.854356Z"
    }
   },
   "outputs": [],
   "source": [
    "selecao_frase_objetivo_teste =[]\n",
    "for h in prob_frase_teste:\n",
    "    x=[]\n",
    "    for i in set([i[0] for i in h]):\n",
    "        x.append(get_max(h,0,i))\n",
    "    selecao_frase_objetivo_teste.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "479c0a65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:42.025658Z",
     "start_time": "2023-03-03T02:42:42.009541Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo_teste_exp2=[]\n",
    "for i in selecao_frase_objetivo_teste:\n",
    "    try:\n",
    "        frase_objetivo_teste_exp2.append([i[0][0]])\n",
    "    except:\n",
    "        frase_objetivo_teste_exp2.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4f8ef6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:42.041322Z",
     "start_time": "2023-03-03T02:42:42.026545Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculo_rouge(frases,ouro):\n",
    "    rouge_total=[]\n",
    "    for i,j in zip(frases,ouro):\n",
    "        rouge_1={}\n",
    "        for h in range(len(i)):\n",
    "            for z in range(len(j)):\n",
    "                rouge_1[(h,z)] = (scorer.score(i[h],j[z])['rouge1'].fmeasure)\n",
    "        rouge_total.append(rouge_1)\n",
    "    return rouge_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "90e3e305",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:44.179178Z",
     "start_time": "2023-03-03T02:42:42.043310Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_total=[]\n",
    "for i,j in zip(frase_objetivo_teste_exp2,list(padrao_ouro_teste_frases.values())):\n",
    "    #print (len(i))\n",
    "    rouge_1={}\n",
    "    for h in range(len(i)):\n",
    "        for z in range(len(j)):\n",
    "            rouge_1[(h,z)] = (scorer.score(i[h],j[z])['rouge1'].fmeasure)\n",
    "    rouge_total.append(rouge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5326268d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:44.195129Z",
     "start_time": "2023-03-03T02:42:44.179178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.642424242424243"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Média de palavras do Padrão Ouro\n",
    "x=[]\n",
    "for i in lista_padrao_ouro_treino:\n",
    "    x.append(len(i))\n",
    "media_ouro_teste=np.mean(x)\n",
    "media_ouro_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "497f8983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:44.210764Z",
     "start_time": "2023-03-03T02:42:44.195129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['for example'],\n",
       " ['furthermore lexical constraints are not limited to the syntagmatic interlexical constraints discussed above'],\n",
       " ['where m lt n and k lt ki lt n for all i lt m for example a model pf ihi hh might be interpolated as follows where e ai hi hh for all histories hi hh'],\n",
       " ['word a in the english matrix is then the translation of word a in the german matrix'],\n",
       " ['for example in machine translation knowing the correct word sense helps to select the appropriate target words to use in order to translate into a target language'],\n",
       " ['for each leaf ie lexeme of a given syntax tree the lexicon specifies a lexical interpretation from the model'],\n",
       " ['finally we give experimental results in section using these two algorithms in appropriate domains and compare them to the labelled tree viterbi algorithm showing that each algorithm generally works best when evaluated on the criterion that it optimizes'],\n",
       " ['first the statistical model assigns a probability to every candidate parse tree for a sentence'],\n",
       " ['we examine the effects of speaking style read versus spontaneous and of discourse segmentation method textalone versus textandspeech on the nature of this relationship'],\n",
       " ['for example if we have that po takes two gtgt pit takes too then we know ceieris paribus to prefer the former transcription over the latter'],\n",
       " ['first we describe the simplest committeebased selection algorithm which has no parameters to tune'],\n",
       " ['we then extend the model to include a probabilistic treatment of both subcategorisation and whmovement'],\n",
       " ['for example'],\n",
       " ['we derive the rhetorical structures of texts by means of two new surfaceformbased algorithms one that identifies discourse usages of cue phrases and breaks sentences into clauses and one that produces valid rhetorical structure trees for unrestricted natural language texts'],\n",
       " ['for language pairs like spanishenglish this presents no great challenge a phrase like antonio gil usually gets translated as antonio gil'],\n",
       " ['we identify and validate from a large corpus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives'],\n",
       " ['for example while danieli and gerbino found that agent as dialogue strategy produced dialogues that were approximately twice as long as agent bs they had no way of determining whether agent as higher transaction success or agent bs efficiency was more critical to performance'],\n",
       " ['for a discussion of recent chinese segmentation work see sproat et al frequently mentioned in segmentation papers that native speakers of a language do not always agree about the correct segmentation and that the same text could be segmented into several very different and equally correct sets of words by different native speakers'],\n",
       " ['for these applications we have designed a fast algorithm for estimating a partial translation model which accounts for translational equivalence only at the word level'],\n",
       " ['we first describe the mechanism for scoring an individual candidate'],\n",
       " ['within a document there is a certain amount of consistency which cannot be expected across documents'],\n",
       " ['with the exception of the example sentence extraction component all the software modules are highly interactive and have substantial user interface requirements'],\n",
       " ['we know that the features used by the different algorithms are very similar typically the words and tags within a small window from the word being tagged'],\n",
       " ['while previous empirical methods for base np identification have been rather complex this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task'],\n",
       " ['w t neednt be a constituent but for the parses where it is there is no restriction on which of its words is the headword or what is the nonterminal label that accompanies the headword'],\n",
       " ['we do this by means of an experiment involving the task of morphosyntactic wordclass tagging'],\n",
       " ['we first define a word similarity measure based on the distributional pattern of words'],\n",
       " ['we present results for prepositional phrase attachment in both english and spanish'],\n",
       " ['we will also present some experimental results from two corpora and discuss criteria for judging the quality of the output'],\n",
       " ['figure'],\n",
       " ['we consider here the question of how to estimate the conditional cooccurrence probability pv in of an unseen word pair n v drawn from some finite set n x v two stateoftheart technologies are katzs backoff method and jelinek and mercers interpolation method'],\n",
       " ['we present a method for extracting parts of objects from wholes eg'],\n",
       " ['fig'],\n",
       " ['wordnet has been an important research tool but it is insufficient for domainspecific text such as that encountered in the mucs message understanding conferences'],\n",
       " ['from the coding manual subjective speechevent and privatestate sentences are used to communicate the speakers evaluations opinions emotions and speculations'],\n",
       " ['we present a method for automatic identification of noncompositional expressions using their statistical properties in a text corpus'],\n",
       " ['for an example see figure'],\n",
       " ['we have developed a corpusbased algorithm for automatically identifying definite noun phrases that are nonanaphoric which has the potential to improve the efficiency and accuracy of coreference resolution systems'],\n",
       " ['we describe our experience in building on the parsing model of collins'],\n",
       " ['whereas for parallel texts in some studies up to of the word alignments have been shown to be correct the accuracy for nonparallel texts has been around up to now'],\n",
       " ['for example diekema et al in a presentation at the trec conference voorhees and harman observed that the performance of their crosslanguage information retrieval was hurt by lexical gaps such as bosnial bosnie this illustrates a highly topical missing pair in their static lexical resource which was based on wordnet'],\n",
       " ['we describe two computationallytractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses and apply these to estimate a stochastic version of lexical'],\n",
       " ['we present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents'],\n",
       " ['which gets us in a position to present the list of task introduced by tempeval including some motivation as to why we feel that it is a good idea to split up temporal relation classification into sub tasks'],\n",
       " ['for each target word participants were provided with a training set in order to learn the senses of that word'],\n",
       " ['we report on the training and test data used for evaluation the process of their creation the participating systems teams runs the approaches adopted and the results achieved'],\n",
       " ['we present a multidocument summarizer called mead which generates summaries using cluster centroids produced by a topic detection and tracking system'],\n",
       " ['for the reasons expressed above we are interested in knowledgefree morphology induction'],\n",
       " ['whereas earlier methods all share the same basic intuition ie that similar words occur in similar contexts i formalise this in a slightly different way each word defines a probability distribution over all contexts namely the probability of the context given the word'],\n",
       " ['we have employed two models one extracted and adapted from bbns sift system miller et al and a tagbased parsing model adapted from chiang'],\n",
       " ['we thank dan klein and michael saunders for useful discussions and the anonymous reviewers for many helpful comments'],\n",
       " ['from a software engineering perspective modularisation is likely to reduce system development costs and increase system reliability'],\n",
       " ['while previous research summarized in section has investigated the theoretical basis of cotraining this study is motivated by practical concerns'],\n",
       " ['we are developing corpusbased techniques for identifying semantic relations at an intermediate level of description more specific than those used in case frames but more general than those used in traditional knowledge representation systems'],\n",
       " ['for example phonological noncompositionality has been observed finke amp weibel gregory et al where words like got gat and to tu change phonetically to gotta gave when combined'],\n",
       " ['first the input text is divided into elementary blocks'],\n",
       " ['while this has allowed for quantitative comparison of parsing techniques it has left open the question of how other types of text might affect parser performance and how portable parsing models are across corpora'],\n",
       " ['why'],\n",
       " ['for example a course might use prolog for parsing perl for corpus processing and a finitestate toolkit for morphological analysis'],\n",
       " ['fscore'],\n",
       " ['figure'],\n",
       " ['we expect the system to yield a large volume of highquality training data at a much lower cost than the traditional method of hiring lexicographers'],\n",
       " ['we combine various clues such as cognates similar context preservation of word similarity and word frequency'],\n",
       " ['figure shows some example attributes for idea'],\n",
       " ['for example the sentence how could anyone sit through this movie contains no single word that is obviously negative'],\n",
       " ['we evaluate basilisk on six semantic categories'],\n",
       " ['figure'],\n",
       " ['furthermore applications built on deep nlp technology should be extensible to multiple languages'],\n",
       " ['we have produced a preliminary version of the grammar matrix relying heavily on the lingo projects english resource grammar and to a lesser extent on the japanese grammar developed jointly between dfki saarbrucken germany and yy technologies mountain view ca'],\n",
       " ['finally we provide a summary and discussion'],\n",
       " ['first they estimate modification probabilities in other words how probable one segment tends to modify another'],\n",
       " ['while parameter estimation for me models is conceptually straightforward in practice me models for typical natural language tasks are very large and may well contain many thousands of free parameters'],\n",
       " ['we explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms'],\n",
       " ['while word senses and translation ambiguities may typically have alternative meanings that must be resolved through context a personal name such as jim clark may potentially refer to hundreds or thousands of distinct individuals'],\n",
       " ['we investigate selecting examples by directly maximising tagger agreement on unlabelled data a method which has been theoretically and empirically motivated in the cotraining literature'],\n",
       " ['we demonstrate this to be the case by improving on the best dutch results from conll using a maximum entropy me tagger'],\n",
       " ['when no gazetteer or other additional training resources are used the combined system attains a performance of f on the english development data integrating name location and person gazetteers and named entity systems trained on additional more general data reduces the fmeasure error by a factor of to on the english data'],\n",
       " ['we discuss two namedentity recognition models which use characters and character grams either exclusively or as an important part of their data representation'],\n",
       " ['we claim that the construction of informative abstracts requires access to deeper linguistic knowledge in order to make substantial improvements over purely statistical approaches'],\n",
       " ['we use deep linguistic features to predict semantic roles on syntactic arguments and show that these perform considerably better than surfaceoriented features'],\n",
       " ['we present a system for automatically identifying propbankstyle semantic roles based on the output of a statistical parser for combinatory categorial grammar'],\n",
       " ['we present a general framework for distributional similarity based on the concepts of precision and recall'],\n",
       " ['finally section summarizes our findings and conclusions'],\n",
       " ['we present a bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories and describe three unsupervised statistical techniques for the significantly harder task of detecting opinions at the sentence level'],\n",
       " ['when pos patterns are used to extract potential terms the problem lies in how to restrict the number of terms and only keep the ones that are relevant'],\n",
       " ['we give the motivation for having an international segmentation contest given that there have been two withinchina contests to date and we report on the results of this first international contest analyze these results and make some recommendations for the future'],\n",
       " ['we have taken six tracks academia sinica closed asc u penn chinese tree bank open and closedctboc hong kong cityu closed hkc peking university open and closedpkoc'],\n",
       " ['for example sentence can be said to entail that peter put the picture somewhere and so we can say that put up entails put'],\n",
       " ['for example if the verb eat is closer in meaning to a phrasal construction eat up compared to other simplex verbs with their phrasal constructions such as blowblow up then the lexicon should reflect that'],\n",
       " ['we use latent semantic analysis to determine the similarity between a multiword expression and its constituent words and claim that higher similarities indicate greater decomposability'],\n",
       " ['when restricted to sentences that are accepted by the parser the degree of incrementality increases to'],\n",
       " ['we describe in this paper the task definition resources participating systems and comparative results for the english lexical sample task which was organized as part of the senseval evaluation exercise'],\n",
       " ['for recalloriented understudy for gisting evaluation'],\n",
       " ['we provide a brief summary of the annotation system and labeling procedure interannotator reliability statistics overall distributional statistics a description of auxiliary files distributed with the corpus and information on how to obtain the data'],\n",
       " ['for example in partofspeech tagging a sentence must have at least one verb and cannot have three consecutive verbs'],\n",
       " ['word sense discrimination by clustering contexts in vector and similarity spaces'],\n",
       " ['figure shows a labeled dependency graph for asimple swedish sentence where each word of the sentence is labeled with its part of speech and each arc la beled with a grammatical functionformally we define dependency graphs in the follow ing way pp pa'],\n",
       " ['when complete nombank will provide argument structure for instances of about common nouns in the penn treebank ii corpus'],\n",
       " ['we explore the use of speculative language in medline abstracts'],\n",
       " ['for example wt is deleted in wilms tumor s such statements found in the literature represent individual genevariationmalignancy observables'],\n",
       " ['we present a novel discriminative approach to parsing inspired by the largemargin criterion underlying support vector machines'],\n",
       " ['for example it may be valuable to know that if someone has bought an item they may sell it at a later time'],\n",
       " ['following dagan and glickman we observe that a somewhat more general notion needed for applications is that of entailment relations eg'],\n",
       " ['first we discuss bilingual parsing and show how it can solve the problem of joint englishparse lparse and wordalignment inference'],\n",
       " ['we propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed'],\n",
       " ['for example the assignment to the first argument of sentence above may be based on experiencer subject'],\n",
       " ['figure shows an example of a monolingual alignment produced by giza'],\n",
       " ['first flexible feature designs for hierarchical tagsets become possible'],\n",
       " ['for chinese in particular words are not demarcated in a chinese sentence'],\n",
       " ['when evaluating on the mismatched outofdomain test data the gram baseline is outperformed by the improvement brought by the adaptation technique using a very small amount of matched bn data kwds is about relative'],\n",
       " ['for learning algorithms to identify these topics a text is usually represented as a bagofwords where a text is regarded as a multiset ie a bag of words and the word order or syntactic relations appearing in the original text is ignored'],\n",
       " ['we are now considering an approach for computing sentence importance based on the concept of eigenvector centrality prestige that we call lexpagerank'],\n",
       " ['figure'],\n",
       " ['we investigate and evaluate the application of textrank to two language processing tasks consisting of unsupervised keyword and sentence extraction and show that the results obtained with textrank are competitive with stateoftheart systems developed in these areas'],\n",
       " ['we describe the methods used to assign values to selected words and phrases and we introduce a method of bringing them together to create a model for the classification of texts'],\n",
       " ['we explore the use of syntactic information including constituent labels and headmodifier dependencies in computing similarity between output and reference'],\n",
       " ['while there is a large body of previous work focused on finding the semantic similarity of concepts and words the application of these wordoriented methods to text similarity has not been yet explored'],\n",
       " ['for i'],\n",
       " ['for example the treepath feature has been shown to be valuable in semantic role labeling gildea and palmer'],\n",
       " ['for a broader coverage we also employ a clustering technique to predict the most probable frame for a word which is not defined in framenet'],\n",
       " ['we introduce spmt a new class of statistical translation models that use syntactified target language phrases'],\n",
       " ['for blackbox smoothing we could have used a backoff scheme or an interpolation scheme'],\n",
       " ['we introduce learning automatically induce correspondences among features from different domains'],\n",
       " ['we present an approach which solves the problem incrementally thus we avoid creating intractable integer linear programs'],\n",
       " ['for example we may find textual evidence of a high likelihood of agreement bebecause we are most interested in techniques applicable across domains we restrict consideration to nlp aspects of the problem ignoring external problemspecific information'],\n",
       " ['fully automatic lexicon expansion for domainoriented sentiment analysis'],\n",
       " ['fortunately research in machine learning has produced methods for global inference and jointclassification that can help to address this defi ciency eg bunescu and mooney roth and yih'],\n",
       " ['we cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminativelytrained hidden markov model'],\n",
       " ['for example most would agree that feather is more related to bird than it is to tree'],\n",
       " ['which side are you on identifying perspectives at the document and sentence levels'],\n",
       " ['finally we try to draw general conclusions about multilingual parsing what makes a particular language treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser'],\n",
       " ['we report results on the conllx shared task buchholz et al data sets and present an error analysis'],\n",
       " ['we use svm classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion'],\n",
       " ['for example if one segmentation subsumes another they are not necessarily incompatible both may be equally valid'],\n",
       " ['we present discriminative reordering models for phrasebased statistical machine translation'],\n",
       " ['we present translation results on the shared task exploiting parallel texts for statistical machine translation generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories'],\n",
       " ['we model this conversion by an extended treetostring transducer that have multilevel trees on the sourceside which gives our system more expressive power and flexibility'],\n",
       " ['we introduce chinese whispers a randomized graphclustering algorithm which is timelinear in the number of edges'],\n",
       " ['finally we explore for the first time the utility of a joint phrasal translation model as a word alignment method'],\n",
       " ['factoredtranslation models allow the inclusion of supertags as a factor in the source or target language'],\n",
       " ['for both adaptive settings linear mixture weights were set as a function of the distance metrics described in section'],\n",
       " ['figure provides some statistics about the corpora used this year'],\n",
       " ['we present experimental results that demonstrate the improvements in correlations with human judgments that result from these parameter tunings'],\n",
       " ['we present an approach that combines machine learning with rulebased filters to detect preposition errors in a corpus of esl essays'],\n",
       " ['we reused the semeval english lexical sample subtask of task and set up both clusteringstyle unsuper vised evaluation using ontonotes senses as goldstandard and a supervised evaluation using the part of the dataset for mapping'],\n",
       " ['we describe our experience in producing acoarse version of the wordnet sense inven tory and preparing the sensetagged corpusfor the task'],\n",
       " ['word sense disambiguation wsd has been de scribed as a task in need of an application'],\n",
       " ['wikipedia'],\n",
       " ['what we need to compare is not individual judgements but two partial orders'],\n",
       " ['finally we converted the re sulting data to the senseval format'],\n",
       " ['figure gives an example dependency graph for the sentence mr tomash will remain as a director emeritus which has been extracted from the penn treebank marcus et al'],\n",
       " ['further metaevaluation of machine translation'],\n",
       " ['we find that other factors such as segmentation consistency and granularity of chinese words can be more important for machine translation'],\n",
       " ['with the rapid development of computing hardware multiprocessor servers and clusters become widely available'],\n",
       " ['we consider the underlying design principles of the stanford scheme from this perspective and compare it to the gr and parc representations'],\n",
       " ['for example the model in taskar et al is trained on only sentences of words or less reranking models collins charniak and johnson restrict yx to be a small set of parses from a firstpass parser see section for discussion of other related work'],\n",
       " ['furthermore we report and analyze the results and describe the approaches of the participating systems'],\n",
       " ['findings of the workshop on statistical machine translation'],\n",
       " ['for our submission we used k which resulted in million out of million sentence pairs being selected for use as training data'],\n",
       " ['first human language has intrinsically very sparse statistics at the surface level hence gaining complete knowledge on translation phrase pairs or target language ngrams is almost impractical'],\n",
       " ['we explore these differences through the use of a new tunable mt metric terplus which extends the translation edit rate evaluation metric with tunable parameters and the incorporation of morphology synonymy and paraphrases'],\n",
       " ['we are not aware of any research that has focused on learning the full scope of negation signals outside biomedical natural language processing'],\n",
       " ['figure illustrates the necessity of using prior knowledge and nonlocal decisions in ner'],\n",
       " ['we show that the same scope finding approach can be applied to both negation and hedging'],\n",
       " ['while the first two addressed bioir information retrieval and bioner named entity recognition respectively the last two focused on bioie information extraction seeking relations between biomolecules'],\n",
       " ['we perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech'],\n",
       " ['first they showed that a strong correlation between nonexpert and expertannotators can be achieved by combining the judgments of multiple nonexperts for instance by voting on each label using different turkers'],\n",
       " ['findings of the joint workshop on statistical machine translation and metrics for machine translation'],\n",
       " ['we then extrapolate three different models of compositionality a simple additive model a pointwisemultiplicative model and a partial least squares regression plsr model'],\n",
       " ['for example providing access to databases a question posed in natural language is converted into a formal database query that can be executed to retrieve information'],\n",
       " ['for example in the following sentence the subordinated clause starting with although contains factual information while uncertain information is included in the main clause and the embedded question'],\n",
       " ['for the feature based model we use some of the features proposed in past literature and propose new features'],\n",
       " ['while the basic task setup and entity definitions follow those of the ge task epi extends on the extraction targets by defining new event types relevant to task topics including major protein modification types and their reverse reactions'],\n",
       " ['when only primary arguments are considered the first five event types in table are classified as simple event types requiring only unary arguments'],\n",
       " ['for example adjectival forms of gpes such as chinese in the chinese leader would not be linked'],\n",
       " ['we participated in both the open and closed tracks and submitted results using both predicted and gold mentions'],\n",
       " ['findings of the workshop on statistical machine translation'],\n",
       " ['we include ranking and adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced tuning version shown to outperform bleu in minimum error rate training for a phrasebased urduenglish system'],\n",
       " ['further the special hash suffices to flag empty buckets'],\n",
       " ['findings of the workshop on statistical machine translation'],\n",
       " ['we have developed a new program called aligning parallel text text such as the canadian hansards that are available in two or more languages'],\n",
       " ['fuf is based on kays functional unification formalism kay'],\n",
       " ['we compare this algorithm to the baumwelch algorithm used for unsupervised training of stochastic taggers'],\n",
       " ['when evaluating an algorithm it is useful to have an idea of the lower and upper bounds on its performance'],\n",
       " ['for instance if c desert dessert and desert occurred more often than dessert in the training corpus then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to or left as desert'],\n",
       " ['for example one could construct vector representations of senses on the basis of their cooccurrence with words or with other senses'],\n",
       " ['we believe that the work reported here is the first study which has attempted to find np chunks subject only to the limitation that the structures recognized do not include recursively embedded nps and which has measured performance by automatic comparison with a preparsed corpus'],\n",
       " ['for some methods the corpora must also be aligned by sentence bro ga unfortunately such training corpora are available for only a handful of language pairs and the cost to create enough training data manually for new language pairs is very high'],\n",
       " ['for each test pattern its distance to all examples in memory is computed and the category of the least distant instances is used as the predicted category for the test pattern'],\n",
       " ['we also discuss the role of in machine learning and its importance in explaining performance differences observed on specific problems'],\n",
       " ['w a tag sequence candidate ti'],\n",
       " ['for instance bod b compares these results to schabes in which for short sentences of the sentences have no crossing brackets a much easier measure than exact match'],\n",
       " ['we present a statistical word feature the word relation matrix which can be used to find translated pairs of words and terms from nonparallel corpora across language groups'],\n",
       " ['what is most interesting here is the way in which strongly selecting word w is typically the head of a noun phrase which could lead the model astray for example toy soldiers behave differently from soldiers mccawley'],\n",
       " ['a linear observed time statistical parser based on maximum entropy models'],\n",
       " ['for instance if in a particular cell in the chart there is some nonterminal that generates the span with high probability and another that generates that span with low probability then we can remove the less likely nonterminal from the cell'],\n",
       " ['for example tax system is most often translated into french as regime fiscale each new batch of validated nccs raises the value of the objective function for the given application as demonstrated in section'],\n",
       " ['while these efforts may be useful for some applications we believe that they will never fully satisfy the need for semantic knowledge'],\n",
       " ['first we present introductions to wards and mcquittys methods section and the em algorithm section'],\n",
       " ['we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text we show how these concepts can be implemented and we discuss results that we obtained with a discoursebased summanzation program motivation the evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some consamples of the output in very few cases output of a summarization program with a humanmade summary or evaluated with the help of human subjects usually the results are modest unfortunately evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions the position that we take in this paper is that in order to build highquality summarization programs one needs to evaluate not only a representative set of automatically generated outputs a highly difficult problem by itself but also the adequacy of the assumptions that these programs use that way one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each with few exceptions automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text see paice for an excellent overview determining the salient parts is considered to be achievable because one or more of the following assumptions hold i important sentences in a text contain words that are used frequently lahn edmundson n important sentences contain words that are used in the tide and section headings edmundson in important sentences are located at the beginning or end of paragraphs baxendale iv important sentences are located at posilions in a text that are genre dependent these positions can be determined automatically through training techniques lin and hovy v important sentences use words as greatest and significant or indiphrases as the main aim of this paper and the purpose of this article while nonimportant senuse words as impossible edmundson rush salvador and zamora vi important sentences and concepts highest connected entities in elaborate semantic structures skorochodko lin barzilay and elhadad and vn imponant and nonimportant sentences are derivable from a discourse representation of the text sparck jones ono surmta and mike in determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections computers are accurate tools flowever in determining the concepts that are semantically related or the discourse structure of a text computers are no longer so accurate rather they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement although it is plausible that elaborate cohesionand coherencebased structures can be used effectively in summarization we believe that before building summarization programs we should determine the extent to which these assumptions hold in this paper we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text we show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program from discourse trees to summaries an empirical view'],\n",
       " ['we present the lexicalsemantic net for german germanet which integrates conceptual ontological information with lexical semantics within and across word classes'],\n",
       " ['for noun phrase anaphora gathering semantically possible antecedents amounts to running all the noun phrases in a text through various databases for number and gender and perhaps then a classifier that determines whether a noun phrase is a company person or place'],\n",
       " ['word sense disambiguation information retrieval'],\n",
       " ['furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing'],\n",
       " ['further experiments showed that when combined with handcoded systems from nyu the university of manitoba and isoquest inc mene was able to generate scores which exceeded the highest scores thusfar reported by any system on a muc evaluation'],\n",
       " ['we incorporate multiple anaphora resolution factors into a statistical framework specifically the distance between the pronoun and the proposed antecedent gendernumberanimaticity of the proposed antecedent governing head information and noun phrase repetition'],\n",
       " ['forming a set of facts about a piece of jewellery into a structure that yields a coherent text is a nontrivial problem'],\n",
       " ['we believe that it is the first such unsupervised technique developed for the general noun phrase coreference task'],\n",
       " ['for the message understanding conference muc a separate named entity recognition task was developed and the best systems achieved impressive accuracy with an fmeasure approaching'],\n",
       " ['fort t'],\n",
       " ['we used these three parsers to explore parser combination techniques'],\n",
       " ['we studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder']]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_objetivo_teste_exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ad552f31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:44.226642Z",
     "start_time": "2023-03-03T02:42:44.212596Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['statistical decisiontree models for parsing',\n",
       " 'syntactic natural language parsers have shown themselves to be inadequate for processing highlyambiguous largevocabulary text as is evidenced by their poor performance on domains like the wall street journal and by the movement away from parsingbased approaches to textprocessing in general',\n",
       " 'in this paper i describe spatter a statistical parser based on decisiontree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result',\n",
       " 'this work is based on the following premises grammars are too complex and detailed to develop manually for most interesting domains parsing models must rely heavily on lexical and contextual information to analyze sentences accurately and existing ngram modeling techniques are inadequate for parsing models',\n",
       " 'in experiments comparing spatter with ibms computer manuals parser spatter significantly outperforms the grammarbased parser',\n",
       " 'evaluating spatter against the penn treebank wall street journal corpus using the parseval measures spatter achieves precision recall and crossing brackets per sentence for sentences of words or less and pre cision recall and crossing brackets for sentences between and words in length',\n",
       " 'we create ftbucdep a depenency tree bank derived from ftbuc using the technique of head propagation rules',\n",
       " 'we find that lexicalization substantially improves performance compared to an unlexicalized baseline model such as a probabilistic contextfree grammar']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padrao_ouro_teste_frases[748]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4bcffdbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:44.305244Z",
     "start_time": "2023-03-03T02:42:44.229099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{(0, 0): 0.0,\n",
       "  (0, 1): 0.06666666666666667,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.13793103448275862,\n",
       "  (0, 2): 0.13793103448275862,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.11764705882352941,\n",
       "  (0, 5): 0.0,\n",
       "  (0, 6): 0.15,\n",
       "  (0, 7): 0.08695652173913043},\n",
       " {(0, 0): 0.09523809523809523,\n",
       "  (0, 1): 0.10256410256410257,\n",
       "  (0, 2): 0.11594202898550725,\n",
       "  (0, 3): 0.12195121951219512,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.075,\n",
       "  (0, 6): 0.03703703703703704,\n",
       "  (0, 7): 0.10714285714285714},\n",
       " {(0, 0): 0.2608695652173913,\n",
       "  (0, 1): 0.24242424242424243,\n",
       "  (0, 2): 0.2222222222222222,\n",
       "  (0, 3): 0.39024390243902435,\n",
       "  (0, 4): 0.22857142857142856,\n",
       "  (0, 5): 0.34146341463414637},\n",
       " {(0, 0): 0.15789473684210525,\n",
       "  (0, 1): 0.27272727272727276,\n",
       "  (0, 2): 0.19672131147540983,\n",
       "  (0, 3): 0.10714285714285715,\n",
       "  (0, 4): 0.19672131147540983,\n",
       "  (0, 5): 0.14814814814814814,\n",
       "  (0, 6): 0.05,\n",
       "  (0, 7): 0.09523809523809523},\n",
       " {(0, 0): 0.07692307692307693,\n",
       "  (0, 1): 0.05405405405405406,\n",
       "  (0, 2): 0.058823529411764705,\n",
       "  (0, 3): 0.17391304347826086,\n",
       "  (0, 4): 0.12244897959183673,\n",
       "  (0, 5): 0.12000000000000001},\n",
       " {(0, 0): 0.09523809523809525,\n",
       "  (0, 1): 0.13793103448275862,\n",
       "  (0, 2): 0.2950819672131147,\n",
       "  (0, 3): 0.1509433962264151,\n",
       "  (0, 4): 0.2769230769230769,\n",
       "  (0, 5): 0.40625000000000006,\n",
       "  (0, 6): 0.34782608695652173},\n",
       " {(0, 0): 0.16666666666666669,\n",
       "  (0, 1): 0.34285714285714286,\n",
       "  (0, 2): 0.12903225806451615,\n",
       "  (0, 3): 0.17777777777777776,\n",
       "  (0, 4): 0.20689655172413796,\n",
       "  (0, 5): 0.28571428571428575,\n",
       "  (0, 6): 0.14285714285714288},\n",
       " {(0, 0): 0.18181818181818182,\n",
       "  (0, 1): 0.3076923076923077,\n",
       "  (0, 2): 0.8571428571428571,\n",
       "  (0, 3): 0.3,\n",
       "  (0, 4): 0.2051282051282051,\n",
       "  (0, 5): 0.08695652173913043},\n",
       " {(0, 0): 0.05714285714285715,\n",
       "  (0, 1): 0.07407407407407408,\n",
       "  (0, 2): 0.1694915254237288,\n",
       "  (0, 3): 0.07407407407407408,\n",
       "  (0, 4): 0.11111111111111112},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.08,\n",
       "  (0, 3): 0.10256410256410256,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.14814814814814817,\n",
       "  (0, 6): 0.2857142857142857,\n",
       "  (0, 7): 0.06451612903225808,\n",
       "  (0, 8): 0.5714285714285714,\n",
       "  (0, 9): 0.1764705882352941,\n",
       "  (0, 10): 0.2,\n",
       "  (0, 11): 0.1395348837209302},\n",
       " {(0, 0): 0.09090909090909091,\n",
       "  (0, 1): 0.2285714285714286,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.11764705882352941,\n",
       "  (0, 4): 0.22222222222222224},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.1818181818181818,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0},\n",
       " {(0, 0): 0.29166666666666663,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.25925925925925924,\n",
       "  (0, 3): 0.17543859649122806},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.15789473684210528,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.12903225806451613,\n",
       "  (0, 4): 0.15384615384615383,\n",
       "  (0, 5): 0.12121212121212123,\n",
       "  (0, 6): 0.11428571428571427,\n",
       "  (0, 7): 0.046511627906976744},\n",
       " {(0, 0): 0.35714285714285715,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.3137254901960784,\n",
       "  (0, 3): 0.46808510638297873,\n",
       "  (0, 4): 0.30769230769230765,\n",
       "  (0, 5): 0.25},\n",
       " {(0, 0): 0.11538461538461538,\n",
       "  (0, 1): 0.10714285714285714,\n",
       "  (0, 2): 0.2150537634408602,\n",
       "  (0, 3): 0.13333333333333333,\n",
       "  (0, 4): 0.20588235294117646},\n",
       " {(0, 0): 0.13333333333333333,\n",
       "  (0, 1): 0.15625,\n",
       "  (0, 2): 0.1142857142857143,\n",
       "  (0, 3): 0.11940298507462686,\n",
       "  (0, 4): 0.25974025974025977,\n",
       "  (0, 5): 0.14492753623188406},\n",
       " {(0, 0): 0.25806451612903225,\n",
       "  (0, 1): 0.28,\n",
       "  (0, 2): 0.8064516129032258,\n",
       "  (0, 3): 0.20512820512820512,\n",
       "  (0, 4): 0.11111111111111112,\n",
       "  (0, 5): 0.18604651162790697,\n",
       "  (0, 6): 0.13636363636363635,\n",
       "  (0, 7): 0.3137254901960784,\n",
       "  (0, 8): 0.16666666666666666},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.1111111111111111,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.09523809523809525,\n",
       "  (0, 5): 0.06250000000000001,\n",
       "  (0, 6): 0.05405405405405406,\n",
       "  (0, 7): 0.16,\n",
       "  (0, 8): 0.15789473684210525,\n",
       "  (0, 9): 0.0930232558139535},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.05714285714285714,\n",
       "  (0, 2): 0.20408163265306123,\n",
       "  (0, 3): 0.09756097560975609,\n",
       "  (0, 4): 0.04081632653061225,\n",
       "  (0, 5): 0.18867924528301885},\n",
       " {(0, 0): 0.07692307692307693,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.21052631578947367,\n",
       "  (0, 3): 0.1388888888888889,\n",
       "  (0, 4): 0.1702127659574468,\n",
       "  (0, 5): 0.1904761904761905},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.19672131147540983,\n",
       "  (0, 2): 0.27450980392156865,\n",
       "  (0, 3): 0.15,\n",
       "  (0, 4): 0.25925925925925924,\n",
       "  (0, 5): 0.09999999999999999},\n",
       " {(0, 0): 0.2,\n",
       "  (0, 1): 0.1739130434782609,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.13043478260869568,\n",
       "  (0, 4): 0.24615384615384614,\n",
       "  (0, 5): 0.2181818181818182,\n",
       "  (0, 6): 0.08},\n",
       " {(0, 0): 0.048780487804878044,\n",
       "  (0, 1): 0.25,\n",
       "  (0, 2): 0.16666666666666666,\n",
       "  (0, 3): 0.16393442622950818,\n",
       "  (0, 4): 0.19672131147540986,\n",
       "  (0, 5): 0.15384615384615385},\n",
       " {(0, 0): 0.26086956521739135,\n",
       "  (0, 1): 0.16666666666666666,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.07692307692307691,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.05555555555555555,\n",
       "  (0, 6): 0.13333333333333333},\n",
       " {(0, 0): 0.2857142857142857,\n",
       "  (0, 1): 0.14285714285714285,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.29629629629629634,\n",
       "  (0, 4): 0.23076923076923075,\n",
       "  (0, 5): 0.06666666666666667,\n",
       "  (0, 6): 0.2857142857142857},\n",
       " {(0, 0): 0.4210526315789474,\n",
       "  (0, 1): 0.3333333333333333,\n",
       "  (0, 2): 0.09302325581395349,\n",
       "  (0, 3): 0.14285714285714288,\n",
       "  (0, 4): 1.0,\n",
       "  (0, 5): 0.26666666666666666},\n",
       " {(0, 0): 0.07142857142857144,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.22222222222222224,\n",
       "  (0, 3): 0.05555555555555556,\n",
       "  (0, 4): 0.13953488372093023,\n",
       "  (0, 5): 0.10000000000000002,\n",
       "  (0, 6): 0.06451612903225806,\n",
       "  (0, 7): 0.125,\n",
       "  (0, 8): 0.20408163265306123,\n",
       "  (0, 9): 0.17142857142857143},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0,\n",
       "  (0, 6): 0.0,\n",
       "  (0, 7): 0.0},\n",
       " {(0, 0): 0.0425531914893617,\n",
       "  (0, 1): 0.24137931034482765,\n",
       "  (0, 2): 0.11111111111111109,\n",
       "  (0, 3): 0.2153846153846154},\n",
       " {(0, 0): 0.1111111111111111,\n",
       "  (0, 1): 0.9565217391304348,\n",
       "  (0, 2): 0.24242424242424246,\n",
       "  (0, 3): 0.16216216216216217,\n",
       "  (0, 4): 0.11428571428571427},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0,\n",
       "  (0, 6): 0.0},\n",
       " {(0, 0): 0.06060606060606061,\n",
       "  (0, 1): 0.15789473684210525,\n",
       "  (0, 2): 0.14814814814814814,\n",
       "  (0, 3): 0.14285714285714288},\n",
       " {(0, 0): 0.19354838709677416,\n",
       "  (0, 1): 0.10526315789473685,\n",
       "  (0, 2): 0.4000000000000001,\n",
       "  (0, 3): 0.1904761904761905,\n",
       "  (0, 4): 0.4242424242424242},\n",
       " {(0, 0): 0.3478260869565218,\n",
       "  (0, 1): 0.2962962962962963,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.21428571428571427,\n",
       "  (0, 4): 0.11764705882352941,\n",
       "  (0, 5): 0.2692307692307692},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.07142857142857142,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0},\n",
       " {(0, 0): 0.28571428571428575,\n",
       "  (0, 1): 0.2790697674418604,\n",
       "  (0, 2): 0.1702127659574468,\n",
       "  (0, 3): 1.0,\n",
       "  (0, 4): 0.2857142857142857,\n",
       "  (0, 5): 0.19230769230769232,\n",
       "  (0, 6): 0.2686567164179104,\n",
       "  (0, 7): 0.2692307692307692},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.14285714285714288,\n",
       "  (0, 2): 0.07142857142857144,\n",
       "  (0, 3): 0.9565217391304348,\n",
       "  (0, 4): 0.27586206896551724,\n",
       "  (0, 5): 0.28571428571428575},\n",
       " {(0, 0): 0.0975609756097561,\n",
       "  (0, 1): 0.33333333333333337,\n",
       "  (0, 2): 0.25,\n",
       "  (0, 3): 0.34615384615384615,\n",
       "  (0, 4): 1.0,\n",
       "  (0, 5): 0.21212121212121213,\n",
       "  (0, 6): 0.13559322033898305,\n",
       "  (0, 7): 0.0,\n",
       "  (0, 8): 0.16},\n",
       " {(0, 0): 0.07142857142857142,\n",
       "  (0, 1): 0.1764705882352941,\n",
       "  (0, 2): 0.16666666666666666,\n",
       "  (0, 3): 0.057971014492753624,\n",
       "  (0, 4): 0.08955223880597014},\n",
       " {(0, 0): 0.17647058823529413,\n",
       "  (0, 1): 0.2916666666666667,\n",
       "  (0, 2): 0.9491525423728815,\n",
       "  (0, 3): 0.10526315789473684,\n",
       "  (0, 4): 0.18604651162790697},\n",
       " {(0, 0): 0.06060606060606061,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.19047619047619052,\n",
       "  (0, 3): 0.1090909090909091},\n",
       " {(0, 0): 0.09756097560975609,\n",
       "  (0, 1): 0.34210526315789475,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.2903225806451613},\n",
       " {(0, 0): 0.15384615384615383,\n",
       "  (0, 1): 0.22727272727272727,\n",
       "  (0, 2): 0.5199999999999999,\n",
       "  (0, 3): 0.14285714285714282,\n",
       "  (0, 4): 0.3529411764705882},\n",
       " {(0, 0): 0.05555555555555556,\n",
       "  (0, 1): 0.2127659574468085,\n",
       "  (0, 2): 0.1132075471698113,\n",
       "  (0, 3): 0.08888888888888889,\n",
       "  (0, 4): 0.962962962962963},\n",
       " {(0, 0): 0.12121212121212123,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.1702127659574468,\n",
       "  (0, 3): 0.17647058823529413,\n",
       "  (0, 4): 0.22727272727272724},\n",
       " {(0, 0): 0.3,\n",
       "  (0, 1): 0.13793103448275862,\n",
       "  (0, 2): 0.12121212121212123,\n",
       "  (0, 3): 0.05882352941176471,\n",
       "  (0, 4): 0.30303030303030304,\n",
       "  (0, 5): 0.16666666666666666,\n",
       "  (0, 6): 0.12121212121212123,\n",
       "  (0, 7): 0.11111111111111112},\n",
       " {(0, 0): 0.08,\n",
       "  (0, 1): 0.13793103448275862,\n",
       "  (0, 2): 0.034482758620689655,\n",
       "  (0, 3): 0.06779661016949153,\n",
       "  (0, 4): 0.06896551724137931,\n",
       "  (0, 5): 0.273972602739726},\n",
       " {(0, 0): 0.18181818181818182,\n",
       "  (0, 1): 0.1,\n",
       "  (0, 2): 0.9090909090909091,\n",
       "  (0, 3): 0.14285714285714288,\n",
       "  (0, 4): 0.0},\n",
       " {(0, 0): 0.13793103448275862,\n",
       "  (0, 1): 0.1904761904761905,\n",
       "  (0, 2): 0.1851851851851852,\n",
       "  (0, 3): 0.16666666666666666,\n",
       "  (0, 4): 0.13793103448275862},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.21052631578947367,\n",
       "  (0, 2): 0.07692307692307693,\n",
       "  (0, 3): 0.14814814814814817},\n",
       " {(0, 0): 0.13333333333333333,\n",
       "  (0, 1): 0.25,\n",
       "  (0, 2): 0.10526315789473685,\n",
       "  (0, 3): 0.2173913043478261,\n",
       "  (0, 4): 0.19607843137254902,\n",
       "  (0, 5): 0.16666666666666663,\n",
       "  (0, 6): 0.23809523809523808,\n",
       "  (0, 7): 0.04878048780487805,\n",
       "  (0, 8): 0.1568627450980392},\n",
       " {(0, 0): 0.13043478260869565,\n",
       "  (0, 1): 0.6122448979591837,\n",
       "  (0, 2): 0.163265306122449,\n",
       "  (0, 3): 0.13888888888888887,\n",
       "  (0, 4): 0.1935483870967742,\n",
       "  (0, 5): 0.15686274509803924},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.043478260869565216,\n",
       "  (0, 2): 0.09090909090909091,\n",
       "  (0, 3): 0.04,\n",
       "  (0, 4): 0.07547169811320756,\n",
       "  (0, 5): 0.06153846153846154},\n",
       " {(0, 0): 0.13333333333333333,\n",
       "  (0, 1): 0.13333333333333333,\n",
       "  (0, 2): 0.11764705882352941,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.10526315789473685,\n",
       "  (0, 5): 0.0},\n",
       " {(0, 0): 0.15384615384615385,\n",
       "  (0, 1): 0.14814814814814817,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.29411764705882354,\n",
       "  (0, 4): 0.2745098039215686,\n",
       "  (0, 5): 0.14814814814814817,\n",
       "  (0, 6): 0.125},\n",
       " {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.0, (0, 3): 0.0, (0, 4): 0.0},\n",
       " {(0, 0): 0.08000000000000002,\n",
       "  (0, 1): 0.14285714285714282,\n",
       "  (0, 2): 0.11764705882352941,\n",
       "  (0, 3): 0.10526315789473685,\n",
       "  (0, 4): 0.2},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0,\n",
       "  (0, 6): 0.0,\n",
       "  (0, 7): 0.125,\n",
       "  (0, 8): 0.0},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0,\n",
       "  (0, 6): 0.0,\n",
       "  (0, 7): 0.0},\n",
       " {(0, 0): 0.05714285714285714,\n",
       "  (0, 1): 0.1276595744680851,\n",
       "  (0, 2): 0.06666666666666667,\n",
       "  (0, 3): 1.0,\n",
       "  (0, 4): 0.22727272727272727,\n",
       "  (0, 5): 0.19047619047619052,\n",
       "  (0, 6): 0.1886792452830189,\n",
       "  (0, 7): 0.10256410256410256},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.058823529411764705,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.07142857142857144,\n",
       "  (0, 4): 0.06896551724137931,\n",
       "  (0, 5): 0.39999999999999997},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.05714285714285715,\n",
       "  (0, 2): 0.07407407407407408,\n",
       "  (0, 3): 0.052631578947368425,\n",
       "  (0, 4): 0.12903225806451613,\n",
       "  (0, 5): 0.0,\n",
       "  (0, 6): 0.0},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.14285714285714288,\n",
       "  (0, 2): 0.11428571428571428,\n",
       "  (0, 3): 0.05,\n",
       "  (0, 4): 0.125,\n",
       "  (0, 5): 0.14285714285714288,\n",
       "  (0, 6): 0.13333333333333333},\n",
       " {(0, 0): 0.1111111111111111,\n",
       "  (0, 1): 0.26086956521739124,\n",
       "  (0, 2): 0.19354838709677416,\n",
       "  (0, 3): 0.2222222222222222,\n",
       "  (0, 4): 1.0,\n",
       "  (0, 5): 0.21428571428571427,\n",
       "  (0, 6): 0.2857142857142857},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0,\n",
       "  (0, 6): 0.0},\n",
       " {(0, 0): 0.1111111111111111,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.060606060606060615,\n",
       "  (0, 3): 0.08333333333333334,\n",
       "  (0, 4): 0.12903225806451615,\n",
       "  (0, 5): 0.0},\n",
       " {(0, 0): 0.24999999999999994,\n",
       "  (0, 1): 0.22641509433962265,\n",
       "  (0, 2): 0.14736842105263157,\n",
       "  (0, 3): 0.3055555555555556},\n",
       " {(0, 0): 0.0, (0, 1): 0.12121212121212123, (0, 2): 0.09090909090909091},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0625,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0851063829787234,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0625},\n",
       " {(0, 0): 0.20512820512820512,\n",
       "  (0, 1): 0.2285714285714286,\n",
       "  (0, 2): 0.2,\n",
       "  (0, 3): 1.0,\n",
       "  (0, 4): 0.28070175438596495,\n",
       "  (0, 5): 0.13114754098360656,\n",
       "  (0, 6): 0.08888888888888888},\n",
       " {(0, 0): 0.3846153846153846,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.27027027027027023,\n",
       "  (0, 3): 0.5142857142857142,\n",
       "  (0, 4): 0.41025641025641024,\n",
       "  (0, 5): 0.4444444444444444,\n",
       "  (0, 6): 0.3870967741935484,\n",
       "  (0, 7): 0.358974358974359},\n",
       " {(0, 0): 0.10526315789473684,\n",
       "  (0, 1): 0.2105263157894737,\n",
       "  (0, 2): 0.06896551724137931,\n",
       "  (0, 3): 0.18181818181818185,\n",
       "  (0, 4): 0.18867924528301885,\n",
       "  (0, 5): 0.2857142857142857},\n",
       " {(0, 0): 0.13333333333333333,\n",
       "  (0, 1): 0.2727272727272727,\n",
       "  (0, 2): 0.20833333333333331,\n",
       "  (0, 3): 1.0,\n",
       "  (0, 4): 0.05,\n",
       "  (0, 5): 0.05555555555555555,\n",
       "  (0, 6): 0.2545454545454545,\n",
       "  (0, 7): 0.1333333333333333,\n",
       "  (0, 8): 0.2666666666666666},\n",
       " {(0, 0): 0.3333333333333333,\n",
       "  (0, 1): 0.10810810810810811,\n",
       "  (0, 2): 0.2790697674418604,\n",
       "  (0, 3): 0.23076923076923075,\n",
       "  (0, 4): 0.18518518518518515,\n",
       "  (0, 5): 0.1333333333333333},\n",
       " {(0, 0): 0.10344827586206896,\n",
       "  (0, 1): 0.136986301369863,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.14084507042253522},\n",
       " {(0, 0): 0.13793103448275862,\n",
       "  (0, 1): 0.9565217391304348,\n",
       "  (0, 2): 0.08163265306122448,\n",
       "  (0, 3): 0.2222222222222222,\n",
       "  (0, 4): 0.05555555555555555,\n",
       "  (0, 5): 0.15384615384615383},\n",
       " {(0, 0): 0.125,\n",
       "  (0, 1): 0.15384615384615383,\n",
       "  (0, 2): 0.375,\n",
       "  (0, 3): 0.2592592592592593,\n",
       "  (0, 4): 0.27999999999999997},\n",
       " {(0, 0): 0.39999999999999997,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.44,\n",
       "  (0, 3): 0.3333333333333333,\n",
       "  (0, 4): 0.1290322580645161,\n",
       "  (0, 5): 0.20408163265306123},\n",
       " {(0, 0): 0.41379310344827586,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.31111111111111117,\n",
       "  (0, 3): 0.2926829268292683},\n",
       " {(0, 0): 0.5454545454545454,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.10526315789473685,\n",
       "  (0, 3): 0.2631578947368421,\n",
       "  (0, 4): 0.6875},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.09523809523809523,\n",
       "  (0, 5): 0.06666666666666667,\n",
       "  (0, 6): 0.060606060606060615},\n",
       " {(0, 0): 0.25925925925925924,\n",
       "  (0, 1): 0.16,\n",
       "  (0, 2): 0.37500000000000006,\n",
       "  (0, 3): 1.0,\n",
       "  (0, 4): 0.37500000000000006,\n",
       "  (0, 5): 0.4337349397590362,\n",
       "  (0, 6): 0.31746031746031744},\n",
       " {(0, 0): 0.05555555555555556,\n",
       "  (0, 1): 0.2127659574468085,\n",
       "  (0, 2): 0.19047619047619047,\n",
       "  (0, 3): 0.32835820895522383,\n",
       "  (0, 4): 0.18181818181818182},\n",
       " {(0, 0): 0.16666666666666669,\n",
       "  (0, 1): 0.3055555555555556,\n",
       "  (0, 2): 0.8611111111111112},\n",
       " {(0, 0): 0.0625,\n",
       "  (0, 1): 0.041666666666666664,\n",
       "  (0, 2): 0.16666666666666666,\n",
       "  (0, 3): 0.05555555555555555,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.14285714285714285,\n",
       "  (0, 6): 0.11428571428571428,\n",
       "  (0, 7): 0.08333333333333333,\n",
       "  (0, 8): 0.12,\n",
       "  (0, 9): 0.05128205128205128,\n",
       "  (0, 10): 0.06060606060606061},\n",
       " {(0, 0): 0.125,\n",
       "  (0, 1): 0.1111111111111111,\n",
       "  (0, 2): 0.16666666666666666,\n",
       "  (0, 3): 0.15686274509803924},\n",
       " {(0, 0): 0.18604651162790695,\n",
       "  (0, 1): 0.24615384615384614,\n",
       "  (0, 2): 0.2162162162162162,\n",
       "  (0, 3): 0.24615384615384614},\n",
       " {(0, 0): 0.1875,\n",
       "  (0, 1): 0.35000000000000003,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.2608695652173913,\n",
       "  (0, 4): 0.1923076923076923,\n",
       "  (0, 5): 0.2380952380952381,\n",
       "  (0, 6): 0.49056603773584906},\n",
       " {(0, 0): 0.09523809523809523,\n",
       "  (0, 1): 0.12121212121212122,\n",
       "  (0, 2): 0.1081081081081081,\n",
       "  (0, 3): 0.25,\n",
       "  (0, 4): 0.27906976744186046},\n",
       " {(0, 0): 0.33333333333333337,\n",
       "  (0, 1): 0.9310344827586207,\n",
       "  (0, 2): 0.3043478260869565},\n",
       " {(0, 0): 0.28571428571428575,\n",
       "  (0, 1): 0.8571428571428571,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.05555555555555555,\n",
       "  (0, 4): 0.07142857142857144,\n",
       "  (0, 5): 0.08333333333333333},\n",
       " {(0, 0): 0.09523809523809522, (0, 1): 0.23728813559322035, (0, 2): 1.0},\n",
       " {(0, 0): 0.19999999999999998,\n",
       "  (0, 1): 0.1090909090909091,\n",
       "  (0, 2): 0.15,\n",
       "  (0, 3): 0.1951219512195122,\n",
       "  (0, 4): 0.125,\n",
       "  (0, 5): 0.05714285714285714,\n",
       "  (0, 6): 0.163265306122449,\n",
       "  (0, 7): 0.17142857142857143},\n",
       " {(0, 0): 1.0,\n",
       "  (0, 1): 0.4736842105263157,\n",
       "  (0, 2): 0.2962962962962963,\n",
       "  (0, 3): 0.4736842105263157,\n",
       "  (0, 4): 0.23999999999999996,\n",
       "  (0, 5): 0.2285714285714286,\n",
       "  (0, 6): 0.21621621621621623,\n",
       "  (0, 7): 0.3157894736842105},\n",
       " {(0, 0): 0.04347826086956522,\n",
       "  (0, 1): 0.15625,\n",
       "  (0, 2): 0.22222222222222227,\n",
       "  (0, 3): 0.23684210526315788,\n",
       "  (0, 4): 0.1842105263157895},\n",
       " {(0, 0): 0.16,\n",
       "  (0, 1): 0.7804878048780488,\n",
       "  (0, 2): 0.3684210526315789,\n",
       "  (0, 3): 0.17391304347826086,\n",
       "  (0, 4): 0.15789473684210525,\n",
       "  (0, 5): 0.3333333333333333},\n",
       " {(0, 0): 0.5,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.20689655172413793,\n",
       "  (0, 3): 0.07407407407407408,\n",
       "  (0, 4): 0.25806451612903225,\n",
       "  (0, 5): 0.2641509433962264,\n",
       "  (0, 6): 0.4347826086956522,\n",
       "  (0, 7): 0.20000000000000004},\n",
       " {(0, 0): 0.08,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.0392156862745098,\n",
       "  (0, 3): 0.07272727272727272,\n",
       "  (0, 4): 0.15},\n",
       " {(0, 0): 0.10526315789473684,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.26666666666666666,\n",
       "  (0, 3): 0.22727272727272727,\n",
       "  (0, 4): 0.04651162790697675,\n",
       "  (0, 5): 0.12765957446808512,\n",
       "  (0, 6): 0.3243243243243243},\n",
       " {(0, 0): 0.0625,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.11428571428571427,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.05555555555555555,\n",
       "  (0, 5): 0.05555555555555555,\n",
       "  (0, 6): 0.12121212121212122,\n",
       "  (0, 7): 0.11428571428571427,\n",
       "  (0, 8): 0.09999999999999999},\n",
       " {(0, 0): 0.22222222222222224,\n",
       "  (0, 1): 0.19354838709677416,\n",
       "  (0, 2): 0.11764705882352941,\n",
       "  (0, 3): 0.125,\n",
       "  (0, 4): 0.30769230769230765,\n",
       "  (0, 5): 0.1951219512195122,\n",
       "  (0, 6): 0.2608695652173913,\n",
       "  (0, 7): 0.0,\n",
       "  (0, 8): 0.23529411764705882},\n",
       " {(0, 0): 0.13333333333333333,\n",
       "  (0, 1): 0.22857142857142856,\n",
       "  (0, 2): 0.15384615384615385,\n",
       "  (0, 3): 0.3111111111111111,\n",
       "  (0, 4): 0.1568627450980392},\n",
       " {(0, 0): 0.06666666666666667,\n",
       "  (0, 1): 0.23728813559322035,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.2285714285714286,\n",
       "  (0, 4): 0.27450980392156865,\n",
       "  (0, 5): 0.27999999999999997},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.25,\n",
       "  (0, 2): 0.21052631578947367,\n",
       "  (0, 3): 0.186046511627907,\n",
       "  (0, 4): 0.20408163265306123,\n",
       "  (0, 5): 0.25,\n",
       "  (0, 6): 0.05714285714285714},\n",
       " {(0, 0): 0.11764705882352942,\n",
       "  (0, 1): 0.07142857142857142,\n",
       "  (0, 2): 0.060606060606060615,\n",
       "  (0, 3): 0.16666666666666669,\n",
       "  (0, 4): 0.1111111111111111,\n",
       "  (0, 5): 0.1,\n",
       "  (0, 6): 0.14285714285714285,\n",
       "  (0, 7): 0.1142857142857143},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.07407407407407407,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0,\n",
       "  (0, 6): 1.0,\n",
       "  (0, 7): 0.0,\n",
       "  (0, 8): 0.04878048780487805,\n",
       "  (0, 9): 0.0,\n",
       "  (0, 10): 0.0},\n",
       " {(0, 0): 0.09523809523809525,\n",
       "  (0, 1): 0.4615384615384615,\n",
       "  (0, 2): 0.5806451612903226,\n",
       "  (0, 3): 0.2222222222222222,\n",
       "  (0, 4): 0.14634146341463414,\n",
       "  (0, 5): 0.16666666666666666,\n",
       "  (0, 6): 0.0851063829787234,\n",
       "  (0, 7): 0.1951219512195122},\n",
       " {(0, 0): 0.17777777777777776,\n",
       "  (0, 1): 0.1923076923076923,\n",
       "  (0, 2): 0.21276595744680854,\n",
       "  (0, 3): 0.06060606060606061,\n",
       "  (0, 4): 0.07999999999999999,\n",
       "  (0, 5): 0.19718309859154928,\n",
       "  (0, 6): 0.7096774193548386,\n",
       "  (0, 7): 0.22222222222222224,\n",
       "  (0, 8): 0.38961038961038963,\n",
       "  (0, 9): 0.07692307692307691,\n",
       "  (0, 10): 0.3278688524590164},\n",
       " {(0, 0): 0.19999999999999998,\n",
       "  (0, 1): 0.2222222222222222,\n",
       "  (0, 2): 0.2222222222222222,\n",
       "  (0, 3): 0.17857142857142858,\n",
       "  (0, 4): 0.14285714285714285,\n",
       "  (0, 5): 0.20588235294117643,\n",
       "  (0, 6): 0.14545454545454545,\n",
       "  (0, 7): 0.0,\n",
       "  (0, 8): 0.19672131147540983},\n",
       " {(0, 0): 0.14285714285714288,\n",
       "  (0, 1): 0.3414634146341463,\n",
       "  (0, 2): 0.18181818181818182,\n",
       "  (0, 3): 0.9767441860465117,\n",
       "  (0, 4): 0.1714285714285714,\n",
       "  (0, 5): 0.09302325581395349,\n",
       "  (0, 6): 0.25000000000000006,\n",
       "  (0, 7): 0.15,\n",
       "  (0, 8): 0.6829268292682926},\n",
       " {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.0, (0, 3): 0.0, (0, 4): 0.0},\n",
       " {(0, 0): 0.047619047619047616,\n",
       "  (0, 1): 0.25396825396825395,\n",
       "  (0, 2): 0.46153846153846156,\n",
       "  (0, 3): 0.22222222222222227,\n",
       "  (0, 4): 0.18867924528301885},\n",
       " {(0, 0): 0.04878048780487805,\n",
       "  (0, 1): 0.4225352112676056,\n",
       "  (0, 2): 0.2711864406779661,\n",
       "  (0, 3): 0.08695652173913043,\n",
       "  (0, 4): 0.2285714285714286},\n",
       " {(0, 0): 0.14814814814814817,\n",
       "  (0, 1): 0.3636363636363636,\n",
       "  (0, 2): 0.9500000000000001,\n",
       "  (0, 3): 0.20000000000000004,\n",
       "  (0, 4): 0.5161290322580645,\n",
       "  (0, 5): 0.4117647058823529},\n",
       " {(0, 0): 0.25641025641025644,\n",
       "  (0, 1): 0.2222222222222222,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.24615384615384617,\n",
       "  (0, 4): 0.09090909090909091},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.06896551724137931,\n",
       "  (0, 2): 0.05714285714285715,\n",
       "  (0, 3): 0.0},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.06666666666666667,\n",
       "  (0, 2): 0.10526315789473685,\n",
       "  (0, 3): 0.2105263157894737,\n",
       "  (0, 4): 0.1276595744680851,\n",
       "  (0, 5): 0.09999999999999999,\n",
       "  (0, 6): 0.06666666666666667},\n",
       " {(0, 0): 0.05405405405405406,\n",
       "  (0, 1): 0.1276595744680851,\n",
       "  (0, 2): 0.17777777777777778,\n",
       "  (0, 3): 0.08695652173913043,\n",
       "  (0, 4): 0.1587301587301587,\n",
       "  (0, 5): 1.0,\n",
       "  (0, 6): 0.052631578947368425,\n",
       "  (0, 7): 0.05714285714285714},\n",
       " {(0, 0): 0.56, (0, 1): 1.0, (0, 2): 0.23076923076923078},\n",
       " {(0, 0): 0.2,\n",
       "  (0, 1): 0.23529411764705882,\n",
       "  (0, 2): 0.24242424242424243,\n",
       "  (0, 3): 0.07142857142857142},\n",
       " {(0, 0): 0.3529411764705882,\n",
       "  (0, 1): 0.09523809523809525,\n",
       "  (0, 2): 0.07407407407407407,\n",
       "  (0, 3): 0.13333333333333333,\n",
       "  (0, 4): 0.19999999999999998,\n",
       "  (0, 5): 0.88,\n",
       "  (0, 6): 0.08695652173913043,\n",
       "  (0, 7): 0.24999999999999994,\n",
       "  (0, 8): 0.0588235294117647,\n",
       "  (0, 9): 0.17142857142857143,\n",
       "  (0, 10): 0.17391304347826086,\n",
       "  (0, 11): 0.13333333333333333},\n",
       " {(0, 0): 0.31999999999999995,\n",
       "  (0, 1): 0.15789473684210528,\n",
       "  (0, 2): 0.3636363636363636,\n",
       "  (0, 3): 1.0,\n",
       "  (0, 4): 0.14285714285714282,\n",
       "  (0, 5): 0.29411764705882354,\n",
       "  (0, 6): 0.13636363636363638,\n",
       "  (0, 7): 0.15789473684210528,\n",
       "  (0, 8): 0.29166666666666663},\n",
       " {(0, 0): 0.041666666666666664,\n",
       "  (0, 1): 0.1639344262295082,\n",
       "  (0, 2): 0.225,\n",
       "  (0, 3): 0.23076923076923078,\n",
       "  (0, 4): 0.14925373134328357},\n",
       " {(0, 0): 1.0,\n",
       "  (0, 1): 0.125,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.06666666666666667,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.13636363636363638,\n",
       "  (0, 6): 0.0,\n",
       "  (0, 7): 0.18181818181818182,\n",
       "  (0, 8): 0.0,\n",
       "  (0, 9): 0.1},\n",
       " {(0, 0): 0.10810810810810811,\n",
       "  (0, 1): 0.16666666666666666,\n",
       "  (0, 2): 0.0784313725490196,\n",
       "  (0, 3): 0.2941176470588235,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.16326530612244897,\n",
       "  (0, 6): 0.15384615384615383,\n",
       "  (0, 7): 0.12244897959183672},\n",
       " {(0, 0): 0.24242424242424246,\n",
       "  (0, 1): 0.3157894736842105,\n",
       "  (0, 2): 0.2926829268292683,\n",
       "  (0, 3): 0.052631578947368425,\n",
       "  (0, 4): 0.16326530612244897,\n",
       "  (0, 5): 1.0,\n",
       "  (0, 6): 0.09302325581395349,\n",
       "  (0, 7): 0.3428571428571428},\n",
       " {(0, 0): 0.07142857142857142,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.10526315789473684,\n",
       "  (0, 3): 0.049999999999999996,\n",
       "  (0, 4): 0.1509433962264151,\n",
       "  (0, 5): 0.21052631578947367,\n",
       "  (0, 6): 0.15,\n",
       "  (0, 7): 0.05263157894736842,\n",
       "  (0, 8): 0.06666666666666667},\n",
       " {(0, 0): 1.0,\n",
       "  (0, 1): 0.3225806451612903,\n",
       "  (0, 2): 0.13333333333333333,\n",
       "  (0, 3): 0.34782608695652173,\n",
       "  (0, 4): 0.18181818181818185,\n",
       "  (0, 5): 0.32,\n",
       "  (0, 6): 0.45,\n",
       "  (0, 7): 0.3,\n",
       "  (0, 8): 0.0,\n",
       "  (0, 9): 0.0,\n",
       "  (0, 10): 0.07692307692307693,\n",
       "  (0, 11): 0.14285714285714288},\n",
       " {(0, 0): 0.15000000000000002,\n",
       "  (0, 1): 0.15151515151515152,\n",
       "  (0, 2): 0.18181818181818182,\n",
       "  (0, 3): 0.2545454545454546,\n",
       "  (0, 4): 0.11764705882352942,\n",
       "  (0, 5): 1.0,\n",
       "  (0, 6): 0.14285714285714288},\n",
       " {(0, 0): 0.07692307692307691,\n",
       "  (0, 1): 0.2580645161290323,\n",
       "  (0, 2): 0.13636363636363638,\n",
       "  (0, 3): 0.0784313725490196,\n",
       "  (0, 4): 0.9090909090909091,\n",
       "  (0, 5): 0.14814814814814814,\n",
       "  (0, 6): 0.13793103448275862,\n",
       "  (0, 7): 0.08163265306122448},\n",
       " {(0, 0): 0.12903225806451613,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.2978723404255319,\n",
       "  (0, 3): 0.1111111111111111,\n",
       "  (0, 4): 0.16666666666666666},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.05263157894736842,\n",
       "  (0, 3): 0.09523809523809526,\n",
       "  (0, 4): 0.12765957446808512,\n",
       "  (0, 5): 0.05128205128205128,\n",
       "  (0, 6): 0.0,\n",
       "  (0, 7): 0.0,\n",
       "  (0, 8): 0.0,\n",
       "  (0, 9): 0.04761904761904763},\n",
       " {(0, 0): 0.8235294117647058,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.10526315789473685,\n",
       "  (0, 3): 0.07142857142857142,\n",
       "  (0, 4): 0.18181818181818182,\n",
       "  (0, 5): 0.3076923076923077,\n",
       "  (0, 6): 0.18181818181818182,\n",
       "  (0, 7): 0.3157894736842105,\n",
       "  (0, 8): 0.27586206896551724},\n",
       " {(0, 0): 0.25,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.35820895522388063,\n",
       "  (0, 3): 0.3728813559322034,\n",
       "  (0, 4): 0.2692307692307693,\n",
       "  (0, 5): 0.13333333333333333,\n",
       "  (0, 6): 0.0,\n",
       "  (0, 7): 0.24657534246575344,\n",
       "  (0, 8): 0.16216216216216217},\n",
       " {(0, 0): 0.06060606060606061,\n",
       "  (0, 1): 0.12244897959183673,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.17777777777777778,\n",
       "  (0, 4): 0.1276595744680851,\n",
       "  (0, 5): 0.052631578947368425,\n",
       "  (0, 6): 0.06060606060606061,\n",
       "  (0, 7): 0.1875,\n",
       "  (0, 8): 0.3225806451612903},\n",
       " {(0, 0): 0.19354838709677422,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.2456140350877193,\n",
       "  (0, 3): 0.1875,\n",
       "  (0, 4): 0.25},\n",
       " {(0, 0): 0.35714285714285715,\n",
       "  (0, 1): 0.4,\n",
       "  (0, 2): 0.20000000000000004,\n",
       "  (0, 3): 0.23255813953488372,\n",
       "  (0, 4): 0.3529411764705882,\n",
       "  (0, 5): 1.0,\n",
       "  (0, 6): 0.12903225806451613},\n",
       " {(0, 0): 0.26086956521739124,\n",
       "  (0, 1): 0.11111111111111112,\n",
       "  (0, 2): 0.15384615384615385,\n",
       "  (0, 3): 0.9090909090909091,\n",
       "  (0, 4): 0.25,\n",
       "  (0, 5): 0.2926829268292683},\n",
       " {(0, 0): 0.17391304347826086,\n",
       "  (0, 1): 0.3181818181818182,\n",
       "  (0, 2): 0.2545454545454545,\n",
       "  (0, 3): 0.16216216216216214,\n",
       "  (0, 4): 0.09523809523809525,\n",
       "  (0, 5): 0.163265306122449},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.12121212121212122,\n",
       "  (0, 2): 0.05263157894736841,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.09523809523809525,\n",
       "  (0, 5): 0.380952380952381,\n",
       "  (0, 6): 0.0},\n",
       " {(0, 0): 0.23529411764705882,\n",
       "  (0, 1): 0.23076923076923078,\n",
       "  (0, 2): 0.10526315789473684,\n",
       "  (0, 3): 0.17142857142857143,\n",
       "  (0, 4): 0.17777777777777778,\n",
       "  (0, 5): 0.23076923076923078},\n",
       " {(0, 0): 0.2962962962962963,\n",
       "  (0, 1): 0.27027027027027023,\n",
       "  (0, 2): 0.3571428571428571,\n",
       "  (0, 3): 0.6222222222222222,\n",
       "  (0, 4): 0.22857142857142854,\n",
       "  (0, 5): 0.17391304347826086,\n",
       "  (0, 6): 0.11267605633802817},\n",
       " {(0, 0): 0.22727272727272727,\n",
       "  (0, 1): 0.22580645161290322,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.7142857142857142,\n",
       "  (0, 4): 0.41509433962264153,\n",
       "  (0, 5): 0.21818181818181817,\n",
       "  (0, 6): 0.0909090909090909},\n",
       " {(0, 0): 0.07407407407407407,\n",
       "  (0, 1): 0.12903225806451613,\n",
       "  (0, 2): 0.7906976744186046,\n",
       "  (0, 3): 0.25,\n",
       "  (0, 4): 0.1967213114754098},\n",
       " {(0, 0): 0.09090909090909091,\n",
       "  (0, 1): 0.14285714285714288,\n",
       "  (0, 2): 0.2857142857142857,\n",
       "  (0, 3): 0.14814814814814814,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.2325581395348837,\n",
       "  (0, 6): 0.2380952380952381,\n",
       "  (0, 7): 0.08},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.07407407407407408,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.06896551724137931,\n",
       "  (0, 4): 0.06896551724137931,\n",
       "  (0, 5): 0.0},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.15789473684210525,\n",
       "  (0, 2): 0.19999999999999998,\n",
       "  (0, 3): 0.13793103448275862},\n",
       " {(0, 0): 0.11111111111111112,\n",
       "  (0, 1): 0.12698412698412698,\n",
       "  (0, 2): 0.044444444444444446,\n",
       "  (0, 3): 0.12244897959183672,\n",
       "  (0, 4): 0.046511627906976744},\n",
       " {(0, 0): 1.0,\n",
       "  (0, 1): 0.22222222222222218,\n",
       "  (0, 2): 0.14814814814814814,\n",
       "  (0, 3): 0.09999999999999999,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.06896551724137931,\n",
       "  (0, 6): 0.05128205128205128},\n",
       " {(0, 0): 0.4137931034482759,\n",
       "  (0, 1): 0.2916666666666667,\n",
       "  (0, 2): 0.1951219512195122,\n",
       "  (0, 3): 1.0,\n",
       "  (0, 4): 0.24000000000000002,\n",
       "  (0, 5): 0.25641025641025644,\n",
       "  (0, 6): 0.35,\n",
       "  (0, 7): 0.2926829268292683},\n",
       " {(0, 0): 0.1,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.07142857142857142,\n",
       "  (0, 3): 0.3225806451612903,\n",
       "  (0, 4): 0.1935483870967742,\n",
       "  (0, 5): 0.2},\n",
       " {(0, 0): 0.22222222222222218,\n",
       "  (0, 1): 0.3137254901960784,\n",
       "  (0, 2): 0.23529411764705882,\n",
       "  (0, 3): 1.0,\n",
       "  (0, 4): 0.3783783783783784,\n",
       "  (0, 5): 0.10810810810810811},\n",
       " {(0, 0): 0.14814814814814814,\n",
       "  (0, 1): 0.1764705882352941,\n",
       "  (0, 2): 0.15625,\n",
       "  (0, 3): 0.21875,\n",
       "  (0, 4): 0.13559322033898305,\n",
       "  (0, 5): 0.28125000000000006,\n",
       "  (0, 6): 0.15873015873015872,\n",
       "  (0, 7): 0.22950819672131148},\n",
       " {(0, 0): 0.22222222222222224,\n",
       "  (0, 1): 0.08695652173913043,\n",
       "  (0, 2): 0.26666666666666666,\n",
       "  (0, 3): 0.16393442622950818,\n",
       "  (0, 4): 0.3225806451612903,\n",
       "  (0, 5): 1.0,\n",
       "  (0, 6): 0.19354838709677422,\n",
       "  (0, 7): 0.2424242424242424},\n",
       " {(0, 0): 1.0,\n",
       "  (0, 1): 0.19354838709677416,\n",
       "  (0, 2): 0.2727272727272727,\n",
       "  (0, 3): 0.1875,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.18181818181818182,\n",
       "  (0, 6): 0.3846153846153846},\n",
       " {(0, 0): 0.06451612903225805,\n",
       "  (0, 1): 0.12121212121212123,\n",
       "  (0, 2): 0.08888888888888888,\n",
       "  (0, 3): 0.05882352941176471,\n",
       "  (0, 4): 0.1290322580645161,\n",
       "  (0, 5): 0.09999999999999999,\n",
       "  (0, 6): 0.1702127659574468},\n",
       " {(0, 0): 0.1111111111111111,\n",
       "  (0, 1): 0.19607843137254902,\n",
       "  (0, 2): 0.0851063829787234,\n",
       "  (0, 3): 0.05,\n",
       "  (0, 4): 0.23076923076923075,\n",
       "  (0, 5): 0.13636363636363638,\n",
       "  (0, 6): 0.07692307692307691,\n",
       "  (0, 7): 0.14634146341463417,\n",
       "  (0, 8): 0.08163265306122448,\n",
       "  (0, 9): 0.10344827586206896,\n",
       "  (0, 10): 0.12244897959183673},\n",
       " {(0, 0): 0.30434782608695654,\n",
       "  (0, 1): 0.3050847457627119,\n",
       "  (0, 2): 0.20689655172413793,\n",
       "  (0, 3): 1.0,\n",
       "  (0, 4): 0.26666666666666666,\n",
       "  (0, 5): 0.24615384615384617,\n",
       "  (0, 6): 0.3692307692307692},\n",
       " {(0, 0): 0.3125,\n",
       "  (0, 1): 0.15789473684210528,\n",
       "  (0, 2): 0.3902439024390244,\n",
       "  (0, 3): 0.06060606060606061,\n",
       "  (0, 4): 0.17391304347826086,\n",
       "  (0, 5): 0.13043478260869565,\n",
       "  (0, 6): 0.27906976744186046,\n",
       "  (0, 7): 0.35714285714285715},\n",
       " {(0, 0): 0.1904761904761905,\n",
       "  (0, 1): 0.23529411764705882,\n",
       "  (0, 2): 0.37499999999999994,\n",
       "  (0, 3): 0.15384615384615385,\n",
       "  (0, 4): 0.17777777777777778,\n",
       "  (0, 5): 0.20408163265306123,\n",
       "  (0, 6): 0.0975609756097561},\n",
       " {(0, 0): 0.24000000000000005,\n",
       "  (0, 1): 0.14634146341463414,\n",
       "  (0, 2): 0.34285714285714286,\n",
       "  (0, 3): 0.3225806451612903,\n",
       "  (0, 4): 1.0,\n",
       "  (0, 5): 0.2,\n",
       "  (0, 6): 0.2222222222222222,\n",
       "  (0, 7): 0.38709677419354843,\n",
       "  (0, 8): 0.09523809523809523},\n",
       " {(0, 0): 0.11764705882352941,\n",
       "  (0, 1): 0.1016949152542373,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.08450704225352113,\n",
       "  (0, 4): 0.08163265306122448,\n",
       "  (0, 5): 0.08163265306122448,\n",
       "  (0, 6): 0.21052631578947367,\n",
       "  (0, 7): 0.24000000000000002},\n",
       " {(0, 0): 0.26315789473684204,\n",
       "  (0, 1): 0.21428571428571427,\n",
       "  (0, 2): 0.10256410256410256,\n",
       "  (0, 3): 0.29411764705882354,\n",
       "  (0, 4): 1.0,\n",
       "  (0, 5): 0.30434782608695654,\n",
       "  (0, 6): 0.24242424242424243,\n",
       "  (0, 7): 0.26315789473684204,\n",
       "  (0, 8): 0.23728813559322037},\n",
       " {(0, 0): 0.049999999999999996,\n",
       "  (0, 1): 0.14285714285714285,\n",
       "  (0, 2): 0.049999999999999996,\n",
       "  (0, 3): 0.125,\n",
       "  (0, 4): 0.1702127659574468,\n",
       "  (0, 5): 0.1818181818181818},\n",
       " {(0, 0): 1.0,\n",
       "  (0, 1): 0.25,\n",
       "  (0, 2): 0.2857142857142857,\n",
       "  (0, 3): 0.2777777777777778,\n",
       "  (0, 4): 0.12121212121212122,\n",
       "  (0, 5): 0.4444444444444445},\n",
       " {(0, 0): 0.3125,\n",
       "  (0, 1): 0.26315789473684204,\n",
       "  (0, 2): 0.18604651162790697,\n",
       "  (0, 3): 0.16666666666666666,\n",
       "  (0, 4): 0.9777777777777777,\n",
       "  (0, 5): 0.125,\n",
       "  (0, 6): 0.21818181818181817,\n",
       "  (0, 7): 0.0857142857142857,\n",
       "  (0, 8): 0.14634146341463414,\n",
       "  (0, 9): 0.049999999999999996},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.19607843137254902,\n",
       "  (0, 2): 0.21621621621621623,\n",
       "  (0, 3): 0.05128205128205129,\n",
       "  (0, 4): 0.03773584905660377,\n",
       "  (0, 5): 0.11111111111111112,\n",
       "  (0, 6): 0.14285714285714288,\n",
       "  (0, 7): 0.08333333333333334},\n",
       " {(0, 0): 0.13953488372093023,\n",
       "  (0, 1): 0.16666666666666666,\n",
       "  (0, 2): 0.3673469387755102,\n",
       "  (0, 3): 0.2692307692307692,\n",
       "  (0, 4): 0.13953488372093023},\n",
       " {(0, 0): 0.08333333333333333,\n",
       "  (0, 1): 0.07692307692307693,\n",
       "  (0, 2): 0.3404255319148936,\n",
       "  (0, 3): 0.27027027027027023,\n",
       "  (0, 4): 0.11764705882352941},\n",
       " {(0, 0): 0.09302325581395349,\n",
       "  (0, 1): 0.16393442622950818,\n",
       "  (0, 2): 0.23999999999999996,\n",
       "  (0, 3): 0.3389830508474576},\n",
       " {(0, 0): 0.125,\n",
       "  (0, 1): 0.1818181818181818,\n",
       "  (0, 2): 0.07272727272727272,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0975609756097561},\n",
       " {(0, 0): 0.07999999999999999,\n",
       "  (0, 1): 0.07407407407407408,\n",
       "  (0, 2): 0.24390243902439027,\n",
       "  (0, 3): 0.10256410256410256,\n",
       "  (0, 4): 0.06666666666666667,\n",
       "  (0, 5): 0.07692307692307693,\n",
       "  (0, 6): 0.10714285714285716,\n",
       "  (0, 7): 0.17647058823529413},\n",
       " {(0, 0): 0.06666666666666667,\n",
       "  (0, 1): 0.1212121212121212,\n",
       "  (0, 2): 0.05555555555555555,\n",
       "  (0, 3): 0.25,\n",
       "  (0, 4): 1.0,\n",
       "  (0, 5): 0.358974358974359,\n",
       "  (0, 6): 0.10126582278481013},\n",
       " {(0, 0): 1.0,\n",
       "  (0, 1): 0.22857142857142856,\n",
       "  (0, 2): 0.2727272727272727,\n",
       "  (0, 3): 0.19354838709677416,\n",
       "  (0, 4): 0.18181818181818182,\n",
       "  (0, 5): 0.06060606060606061,\n",
       "  (0, 6): 0.16666666666666669},\n",
       " {(0, 0): 0.22222222222222224,\n",
       "  (0, 1): 0.13333333333333333,\n",
       "  (0, 2): 0.10169491525423728,\n",
       "  (0, 3): 1.0},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.17391304347826086,\n",
       "  (0, 3): 0.07692307692307691,\n",
       "  (0, 4): 0.06451612903225805,\n",
       "  (0, 5): 0.07692307692307691,\n",
       "  (0, 6): 0.08333333333333334,\n",
       "  (0, 7): 0.08695652173913043,\n",
       "  (0, 8): 0.0},\n",
       " {(0, 0): 1.0,\n",
       "  (0, 1): 0.2,\n",
       "  (0, 2): 0.2857142857142857,\n",
       "  (0, 3): 0.19354838709677416,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.2},\n",
       " {(0, 0): 0.0625,\n",
       "  (0, 1): 0.9600000000000001,\n",
       "  (0, 2): 0.13793103448275862,\n",
       "  (0, 3): 0.16666666666666666,\n",
       "  (0, 4): 0.1,\n",
       "  (0, 5): 0.2571428571428572,\n",
       "  (0, 6): 0.15789473684210525},\n",
       " {(0, 0): 0.09523809523809525,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.028985507246376812,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0},\n",
       " {(0, 0): 0.24,\n",
       "  (0, 1): 0.4615384615384615,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.41860465116279066,\n",
       "  (0, 4): 0.4,\n",
       "  (0, 5): 0.18604651162790697},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.16666666666666663,\n",
       "  (0, 2): 0.0909090909090909,\n",
       "  (0, 3): 0.19607843137254902,\n",
       "  (0, 4): 0.18181818181818185,\n",
       "  (0, 5): 0.20512820512820512,\n",
       "  (0, 6): 0.1818181818181818},\n",
       " {(0, 0): 0.0816326530612245,\n",
       "  (0, 1): 0.18181818181818182,\n",
       "  (0, 2): 0.1388888888888889,\n",
       "  (0, 3): 0.13793103448275862,\n",
       "  (0, 4): 0.07407407407407408,\n",
       "  (0, 5): 0.24324324324324323,\n",
       "  (0, 6): 0.13333333333333333,\n",
       "  (0, 7): 0.11320754716981131,\n",
       "  (0, 8): 0.14705882352941174,\n",
       "  (0, 9): 0.09999999999999999,\n",
       "  (0, 10): 0.037735849056603765},\n",
       " {(0, 0): 0.13793103448275862,\n",
       "  (0, 1): 0.09523809523809523,\n",
       "  (0, 2): 0.22857142857142854,\n",
       "  (0, 3): 0.2545454545454545,\n",
       "  (0, 4): 0.20833333333333334,\n",
       "  (0, 5): 0.15789473684210528,\n",
       "  (0, 6): 0.17777777777777778,\n",
       "  (0, 7): 0.12765957446808512,\n",
       "  (0, 8): 0.0},\n",
       " {(0, 0): 0.039999999999999994,\n",
       "  (0, 1): 0.12903225806451613,\n",
       "  (0, 2): 0.14492753623188406,\n",
       "  (0, 3): 0.2285714285714286,\n",
       "  (0, 4): 0.13698630136986303,\n",
       "  (0, 5): 0.10169491525423728,\n",
       "  (0, 6): 0.17910447761194032,\n",
       "  (0, 7): 0.14814814814814814},\n",
       " {(0, 0): 0.07547169811320754,\n",
       "  (0, 1): 0.11594202898550725,\n",
       "  (0, 2): 0.16129032258064516,\n",
       "  (0, 3): 0.2,\n",
       "  (0, 4): 0.19354838709677416,\n",
       "  (0, 5): 0.18749999999999997,\n",
       "  (0, 6): 0.26229508196721313,\n",
       "  (0, 7): 0.14035087719298245,\n",
       "  (0, 8): 0.07692307692307693},\n",
       " {(0, 0): 0.052631578947368425,\n",
       "  (0, 1): 0.0975609756097561,\n",
       "  (0, 2): 0.09302325581395349,\n",
       "  (0, 3): 0.22641509433962262,\n",
       "  (0, 4): 0.2692307692307692,\n",
       "  (0, 5): 0.22580645161290322,\n",
       "  (0, 6): 0.0,\n",
       "  (0, 7): 0.18421052631578946,\n",
       "  (0, 8): 0.1927710843373494,\n",
       "  (0, 9): 0.29166666666666663,\n",
       "  (0, 10): 0.05128205128205128},\n",
       " {(0, 0): 0.38888888888888884,\n",
       "  (0, 1): 0.2666666666666666,\n",
       "  (0, 2): 0.125,\n",
       "  (0, 3): 0.27906976744186046,\n",
       "  (0, 4): 0.4285714285714286,\n",
       "  (0, 5): 0.975609756097561,\n",
       "  (0, 6): 0.14035087719298245,\n",
       "  (0, 7): 0.13333333333333333},\n",
       " {(0, 0): 0.30769230769230765,\n",
       "  (0, 1): 0.12903225806451613,\n",
       "  (0, 2): 0.14814814814814814,\n",
       "  (0, 3): 0.09756097560975609,\n",
       "  (0, 4): 0.12903225806451613,\n",
       "  (0, 5): 0.14285714285714288},\n",
       " {(0, 0): 0.1142857142857143,\n",
       "  (0, 1): 0.2,\n",
       "  (0, 2): 0.05128205128205127,\n",
       "  (0, 3): 0.14814814814814817,\n",
       "  (0, 4): 0.19607843137254902,\n",
       "  (0, 5): 0.14285714285714285,\n",
       "  (0, 6): 0.19047619047619047,\n",
       "  (0, 7): 0.2592592592592593,\n",
       "  (0, 8): 0.17857142857142858,\n",
       "  (0, 9): 0.15094339622641512},\n",
       " {(0, 0): 0.29411764705882354,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.28571428571428575,\n",
       "  (0, 3): 0.4390243902439025,\n",
       "  (0, 4): 0.10256410256410255,\n",
       "  (0, 5): 0.09523809523809523,\n",
       "  (0, 6): 0.28125000000000006},\n",
       " {(0, 0): 0.047619047619047616,\n",
       "  (0, 1): 0.2,\n",
       "  (0, 2): 0.16901408450704225,\n",
       "  (0, 3): 0.14814814814814817,\n",
       "  (0, 4): 0.19444444444444445,\n",
       "  (0, 5): 0.18181818181818182},\n",
       " {(0, 0): 1.0,\n",
       "  (0, 1): 0.2,\n",
       "  (0, 2): 0.16326530612244897,\n",
       "  (0, 3): 0.39999999999999997,\n",
       "  (0, 4): 0.08333333333333334,\n",
       "  (0, 5): 0.045454545454545456,\n",
       "  (0, 6): 0.14634146341463417},\n",
       " {(0, 0): 0.04347826086956522,\n",
       "  (0, 1): 0.1764705882352941,\n",
       "  (0, 2): 0.24096385542168675,\n",
       "  (0, 3): 0.14035087719298248,\n",
       "  (0, 4): 0.10344827586206896},\n",
       " {(0, 0): 0.09523809523809523,\n",
       "  (0, 1): 0.15686274509803924,\n",
       "  (0, 2): 0.10000000000000002,\n",
       "  (0, 3): 0.15384615384615383,\n",
       "  (0, 4): 0.15686274509803924,\n",
       "  (0, 5): 0.11538461538461539,\n",
       "  (0, 6): 0.18518518518518517,\n",
       "  (0, 7): 0.10000000000000002,\n",
       "  (0, 8): 0.30508474576271183,\n",
       "  (0, 9): 0.13333333333333333},\n",
       " {(0, 0): 0.13793103448275862,\n",
       "  (0, 1): 0.23809523809523808,\n",
       "  (0, 2): 0.23529411764705882,\n",
       "  (0, 3): 0.2926829268292683,\n",
       "  (0, 4): 0.09523809523809525,\n",
       "  (0, 5): 0.1111111111111111,\n",
       "  (0, 6): 0.15,\n",
       "  (0, 7): 0.04761904761904762,\n",
       "  (0, 8): 0.2285714285714286,\n",
       "  (0, 9): 0.12244897959183673,\n",
       "  (0, 10): 0.10810810810810811},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.10810810810810811,\n",
       "  (0, 2): 0.2962962962962963,\n",
       "  (0, 3): 0.17647058823529413,\n",
       "  (0, 4): 0.11764705882352941,\n",
       "  (0, 5): 0.11764705882352941},\n",
       " {(0, 0): 0.021390374331550804,\n",
       "  (0, 1): 0.08261617900172118,\n",
       "  (0, 2): 0.06956521739130435},\n",
       " {(0, 0): 0.37037037037037035,\n",
       "  (0, 1): 1.0,\n",
       "  (0, 2): 0.24489795918367344,\n",
       "  (0, 3): 0.11428571428571427,\n",
       "  (0, 4): 0.1951219512195122,\n",
       "  (0, 5): 0.16326530612244897,\n",
       "  (0, 6): 0.3111111111111111},\n",
       " {(0, 0): 0.038461538461538464,\n",
       "  (0, 1): 0.15384615384615383,\n",
       "  (0, 2): 0.17647058823529413,\n",
       "  (0, 3): 0.09374999999999999,\n",
       "  (0, 4): 0.20253164556962025,\n",
       "  (0, 5): 0.1388888888888889,\n",
       "  (0, 6): 0.06779661016949151,\n",
       "  (0, 7): 0.17142857142857143},\n",
       " {(0, 0): 0.15384615384615385,\n",
       "  (0, 1): 0.125,\n",
       "  (0, 2): 0.09523809523809523,\n",
       "  (0, 3): 0.21052631578947364,\n",
       "  (0, 4): 0.15384615384615385,\n",
       "  (0, 5): 0.16666666666666666},\n",
       " {(0, 0): 0.08,\n",
       "  (0, 1): 0.23255813953488372,\n",
       "  (0, 2): 0.08888888888888889,\n",
       "  (0, 3): 0.1395348837209302,\n",
       "  (0, 4): 0.14634146341463414,\n",
       "  (0, 5): 0.19999999999999998,\n",
       "  (0, 6): 1.0,\n",
       "  (0, 7): 0.3333333333333333},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.07547169811320754,\n",
       "  (0, 2): 0.2465753424657534,\n",
       "  (0, 3): 0.09999999999999999,\n",
       "  (0, 4): 0.25,\n",
       "  (0, 5): 0.49122807017543857},\n",
       " {(0, 0): 0.21052631578947367,\n",
       "  (0, 1): 0.08333333333333333,\n",
       "  (0, 2): 1.0,\n",
       "  (0, 3): 0.17391304347826086,\n",
       "  (0, 4): 0.18518518518518515,\n",
       "  (0, 5): 0.22727272727272727,\n",
       "  (0, 6): 0.2909090909090909,\n",
       "  (0, 7): 0.2325581395348837,\n",
       "  (0, 8): 0.2456140350877193,\n",
       "  (0, 9): 0.20833333333333334,\n",
       "  (0, 10): 0.2571428571428571,\n",
       "  (0, 11): 0.2127659574468085},\n",
       " {(0, 0): 0.06896551724137931,\n",
       "  (0, 1): 0.29411764705882354,\n",
       "  (0, 2): 0.1142857142857143,\n",
       "  (0, 3): 0.2666666666666666,\n",
       "  (0, 4): 0.0975609756097561},\n",
       " {(0, 0): 0.2608695652173913,\n",
       "  (0, 1): 0.33333333333333337,\n",
       "  (0, 2): 0.2424242424242424,\n",
       "  (0, 3): 0.17391304347826086,\n",
       "  (0, 4): 0.16666666666666666,\n",
       "  (0, 5): 0.25,\n",
       "  (0, 6): 0.13953488372093023,\n",
       "  (0, 7): 0.12903225806451615,\n",
       "  (0, 8): 0.07407407407407407,\n",
       "  (0, 9): 0.15789473684210525,\n",
       "  (0, 10): 0.2295081967213115},\n",
       " {(0, 0): 0.22857142857142856,\n",
       "  (0, 1): 0.2727272727272727,\n",
       "  (0, 2): 0.0784313725490196,\n",
       "  (0, 3): 0.2264150943396226,\n",
       "  (0, 4): 0.08888888888888889,\n",
       "  (0, 5): 0.30769230769230765,\n",
       "  (0, 6): 0.14285714285714285},\n",
       " {(0, 0): 0.0,\n",
       "  (0, 1): 0.0,\n",
       "  (0, 2): 0.0,\n",
       "  (0, 3): 0.0,\n",
       "  (0, 4): 0.0,\n",
       "  (0, 5): 0.0,\n",
       "  (0, 6): 0.0,\n",
       "  (0, 7): 0.0,\n",
       "  (0, 8): 0.0},\n",
       " {(0, 0): 0.22222222222222224,\n",
       "  (0, 1): 0.25806451612903225,\n",
       "  (0, 2): 0.16666666666666666,\n",
       "  (0, 3): 0.11764705882352941,\n",
       "  (0, 4): 0.08333333333333333,\n",
       "  (0, 5): 0.2857142857142857,\n",
       "  (0, 6): 0.1702127659574468,\n",
       "  (0, 7): 0.18749999999999997},\n",
       " {(0, 0): 0.125,\n",
       "  (0, 1): 0.21052631578947364,\n",
       "  (0, 2): 0.3333333333333333,\n",
       "  (0, 3): 0.25,\n",
       "  (0, 4): 1.0,\n",
       "  (0, 5): 0.12244897959183672}]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7a564330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:44.321242Z",
     "start_time": "2023-03-03T02:42:44.306334Z"
    }
   },
   "outputs": [],
   "source": [
    "maximo_rouge_paper = max_rouge(rouge_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "416a9ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:44.336904Z",
     "start_time": "2023-03-03T02:42:44.321242Z"
    }
   },
   "outputs": [],
   "source": [
    "precisao_exp3_pt2, revocacao_exp3_pt2 = rouge_precisao_revocacao(maximo_rouge_paper, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f2589b45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:44.352560Z",
     "start_time": "2023-03-03T02:42:44.336904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5408221127312582"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precisao_exp3_pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0ea2d7f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:44.368550Z",
     "start_time": "2023-03-03T02:42:44.352560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08636530958097381"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(revocacao_exp3_pt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "edabfd9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:44.384643Z",
     "start_time": "2023-03-03T02:42:44.369170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ revocação ---------\n",
      "qtd de papers com revocação>=(1/media_ouro): 57\n",
      "% do total: 0.2688679245283019\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 8\n",
      "% do total: 0.03773584905660377\n"
     ]
    }
   ],
   "source": [
    "print('------ revocação ---------')\n",
    "qtd_revocacao_exp3_pt2, media_revocacao_exp3_pt2  = analise_revocacao(revocacao_exp3_pt2, lista_padrao_ouro_teste, media_ouro_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "95978798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:45.570673Z",
     "start_time": "2023-03-03T02:42:44.384643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ precisao ---------\n",
      "qtd de papers com precisao>=0.8: 84\n",
      "% do total: 0.39622641509433965\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 8\n",
      "% do total: 0.03773584905660377\n",
      "media precisao: 0.5408221127312582\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_38324\\322150317.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'------ precisao ---------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprecisao_cbow_exp3_pt2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevocacao_cbow_exp3_pt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalise_precisao\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecisao_exp3_pt2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlista_padrao_ouro_teste\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "print('------ precisao ---------')\n",
    "precisao_cbow_exp3_pt2, revocacao_cbow_exp3_pt2 = analise_precisao(precisao_exp3_pt2, lista_padrao_ouro_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec09a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-03T02:42:45.680281Z",
     "start_time": "2023-03-03T02:42:45.680281Z"
    }
   },
   "outputs": [],
   "source": [
    "len(lista_padrao_ouro_teste)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.062px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
