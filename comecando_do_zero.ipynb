{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "416cdc6c",
   "metadata": {},
   "source": [
    "# Código Mestrado\n",
    "\n",
    "Terminar tudo sem o GloVe e dps voltar pro GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38442482",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12293b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:49.521764Z",
     "start_time": "2023-02-23T00:49:45.829128Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "import collections\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import numpy\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import itertools\n",
    "import string\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a993e5b",
   "metadata": {},
   "source": [
    "## Leitura da Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fd6220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:49.587549Z",
     "start_time": "2023-02-23T00:49:49.522829Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('lista_papers'):\n",
    "    lista_papers = joblib.load('lista_papers')\n",
    "else:\n",
    "    list_of_files = os.listdir('scisummnet_release1.1__20190413/top1000_complete')\n",
    "\n",
    "    list_of_files2= []\n",
    "    for i in list_of_files:\n",
    "        nome = 'scisummnet_release1.1__20190413/top1000_complete/' + i + '/' + 'Documents_xml/' +os.listdir(f'scisummnet_release1.1__20190413/top1000_complete/{i}/Documents_xml')[0]\n",
    "        list_of_files2.append(nome)\n",
    "\n",
    "\n",
    "    lines = []\n",
    "    for file in list_of_files2:\n",
    "        f= open(file,'r')\n",
    "        lines.append(f.readlines())\n",
    "        f.close()\n",
    "\n",
    "    lista_papers=[]\n",
    "    for j in lines:\n",
    "        texto=''\n",
    "        for i in j:\n",
    "            texto = texto + str(i)\n",
    "        lista_papers.append(texto)\n",
    "\n",
    "        joblib.dump(lista_papers, 'lista_papers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7664d250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:49.605638Z",
     "start_time": "2023-02-23T00:49:49.588548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1009"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5941c0c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:49.621182Z",
     "start_time": "2023-02-23T00:49:49.607956Z"
    }
   },
   "outputs": [],
   "source": [
    "#Aplicando indices aos papers\n",
    "indices           = range(0,len(lista_papers))\n",
    "papers_com_indice = dict(zip(indices, lista_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67feb662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:49.639906Z",
     "start_time": "2023-02-23T00:49:49.623183Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_intro(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*[Ii][nN][tT][rR][oO][dD][Uu][Cc][Tt][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(\"\\\\t\",\"\",txt)\n",
    "        txt = re.sub('\\/','', txt)\n",
    "        txt = re.sub(\"\\-\",\"\",txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = re.sub('e.g.','',txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        txt = re.sub('&amp;quot;seed&amp;quot;', '', txt)\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "        txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "        txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "        lista.append(txt)\n",
    "    return txt\n",
    "\n",
    "def preprocessa_abstract(corpus: str):\n",
    "    regIter = re.finditer(\"(<[Aa][Bb][sS][tT][Rr][aA][cC][tT]>)(.+?)(</[aA][bB][sS][tT][rR][aA][cC][tT]>)\", corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub('\\/','', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = re.sub(\"\\-\",\"\",txt)\n",
    "        txt = re.sub(\"\\\\t\",\"\",txt)\n",
    "        txt = re.sub('e.g.','',txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        txt = re.sub('&amp;quot;seed&amp;quot;', '', txt)\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub(r'^(\\d)*(\\.)?([0-9]{1})?', '', txt)         #removendo decimal\n",
    "        txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "        txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "        lista.append(txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def preprocessa_conclusion(corpus: str):\n",
    "    regIter1 = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos1 = [ t.group(2) for t in regIter1]\n",
    "    regIter2 = re.finditer('(<SECTION.*[Aa][Cc][Kk][Nn][Oo][Ww][Ll][Ee][Dd][Gg][Mm][Ee][Nn][Tt][Ss].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos2 = [ t.group(2) for t in regIter2]\n",
    "    regIter3 = re.finditer('(<SECTION.*[Aa][Cc][Kk][Nn][Oo][Ww][Ll][Ee][Dd][Gg][Ee][Mm][Ee][Nn][Tt][Ss].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos3 = [ t.group(2) for t in regIter3]\n",
    "    textos = textos1\n",
    "    for i in textos2:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    for i in textos3:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = re.sub('\\/','', txt)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(\"\\-\",\"\",txt)\n",
    "        txt = txt.strip()\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\\\t\",\"\",txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = re.sub('e.g.','',txt)\n",
    "        txt = re.sub('&amp;quot;', '', txt)\n",
    "        txt = re.sub('&amp;quot;seed&amp;quot;', '', txt)\n",
    "        txt = re.sub('\\.+', '', txt)\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "        txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "        txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a977585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:49.655072Z",
     "start_time": "2023-02-23T00:49:49.641407Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_indices_preprocessamento(x, funcao_preprocessamento):\n",
    "    \"\"\"Função para obter os índices dos papers que contém a introdução, abstract ou conclusão\n",
    "    obs:alterar a o parâmetro funcao_preprocessamento com:\n",
    "    preprocessa_intro OU preprocessa_abstract OU preprocessa_conclusion\"\"\"\n",
    "    papers_processados=[]\n",
    "    indices=[]\n",
    "    for i,j in zip(list(x.keys()),list(x.values())):\n",
    "        try:\n",
    "            papers_processados.append(funcao_preprocessamento(j))\n",
    "            indices.append(i)\n",
    "        except:\n",
    "            None\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0441dce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.619902Z",
     "start_time": "2023-02-23T00:49:49.656074Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_papers_com_abstract   = get_indices_preprocessamento(papers_com_indice, preprocessa_abstract)\n",
    "indices_papers_com_introducao = get_indices_preprocessamento(papers_com_indice, preprocessa_intro)\n",
    "indices_papers_com_conclusao  = get_indices_preprocessamento(papers_com_indice, preprocessa_conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8cc24f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.634848Z",
     "start_time": "2023-02-23T00:49:55.622852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de papers com abstract 924\n",
      "qtd de papers com introdução 802\n",
      "qtd de papers com conclusão 831\n",
      "---------------------------------\n",
      "qtd total de papers 1009\n"
     ]
    }
   ],
   "source": [
    "print('qtd de papers com abstract', len(indices_papers_com_abstract))\n",
    "print('qtd de papers com introdução', len(indices_papers_com_introducao))\n",
    "print('qtd de papers com conclusão', len(indices_papers_com_conclusao))\n",
    "\n",
    "print('---------------------------------')\n",
    "\n",
    "print('qtd total de papers', len(lista_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b50e5cbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.672213Z",
     "start_time": "2023-02-23T00:49:55.636847Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_sem_abstract = dict((key,value) for key, value in papers_com_indice.items() if key  not in  indices_papers_com_abstract)\n",
    "papers_sem_introducao = dict((key,value) for key, value in papers_com_indice.items() if key  not in  indices_papers_com_introducao)\n",
    "papers_sem_conclusao = dict((key,value) for key, value in papers_com_indice.items() if key  not in  indices_papers_com_conclusao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99449edd",
   "metadata": {},
   "source": [
    "Contagem dos títulos das sessões para avaliar os títulos dos papers que não possuiam `abstract` ou `introdução`ou `conclusão`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba54376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.703239Z",
     "start_time": "2023-02-23T00:49:55.676667Z"
    }
   },
   "outputs": [],
   "source": [
    "def contador_sessoes(x):\n",
    "    \"\"\"input: dicionário de papers\n",
    "       output: lista contendo o título das sessões\"\"\"\n",
    "    titulos_sessoes=[]\n",
    "    for i in x.values():\n",
    "        regIter = re.finditer('(<SECTION(.*?)>)', i, re.DOTALL|re.MULTILINE)\n",
    "        textos = [ t.group(2) for t in regIter]\n",
    "        titulos_sessoes.append(textos)\n",
    "\n",
    "    x=[]\n",
    "    for i in titulos_sessoes:\n",
    "        for j in i:\n",
    "            pattern = r'\"(.*?)\"'\n",
    "            matches = re.findall(pattern,str(j))\n",
    "            pattern2 = r'[0-9]'\n",
    "            new_string = re.sub(pattern2, '', matches[0])\n",
    "            x.append(new_string.strip())\n",
    "\n",
    "    frequency = collections.Counter(x)\n",
    "    return sorted(frequency.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f76df89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.718987Z",
     "start_time": "2023-02-23T00:49:55.704240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 20),\n",
       " ('Introduction', 12),\n",
       " ('Acknowledgements', 6),\n",
       " ('Results', 5),\n",
       " ('', 4),\n",
       " ('Related Work', 4),\n",
       " ('Acknowledgments', 3),\n",
       " ('Abstract  Introduction', 3),\n",
       " ('Data and Evaluation', 3),\n",
       " ('Concluding Remarks', 3),\n",
       " ('Conclusion', 2),\n",
       " ('Experimental Results', 2),\n",
       " ('Conclusions and Future Work', 2),\n",
       " ('Conditional Random Fields', 2),\n",
       " ('The Joyce System in the Ulysses User Interface', 1),\n",
       " ('The Structure of Joyce', 1),\n",
       " ('The Text Planner', 1),\n",
       " ('The Sentence Planner', 1),\n",
       " ('The Linguistic Realizer', 1),\n",
       " ('An Example', 1),\n",
       " ('ENTER AGENT #&lt;information&gt; OBJECT INCOMPONENT Black Box&gt; LOCATION #&lt;PORT P&gt;',\n",
       "  1),\n",
       " ('Porting the System', 1),\n",
       " ('TRANSLATION DELIVERY SYSTEM', 1),\n",
       " ('TESTING AND EVALUATING MULTI-ENGINE PERFORMANCE', 1),\n",
       " ('CURRENT AND FUTURE WORK', 1),\n",
       " ('Input Structure', 1),\n",
       " ('System Architecture', 1),\n",
       " ('Linguistic Knowledge Bases', 1),\n",
       " ('Interfaces', 1),\n",
       " ('System Performance', 1),\n",
       " ('Status', 1),\n",
       " ('. Corpus-based Methods', 1),\n",
       " ('Arabic Language and Data', 1),\n",
       " ('SVM Based Approach', 1),\n",
       " ('Evaluation', 1),\n",
       " ('Conclusions &amp; Future Directions', 1),\n",
       " ('Previous Work', 1),\n",
       " ('Open IE in TEXTRUNNER', 1),\n",
       " ('Morpheme Segmentation', 1),\n",
       " ('Unsupervised Acquisition of New Stems', 1),\n",
       " ('Performance Evaluations', 1),\n",
       " ('Summary and Future Work', 1),\n",
       " ('Acknowledgment', 1),\n",
       " ('The Nature of Relations in English', 1),\n",
       " ('Relation Extraction', 1),\n",
       " ('Hybrid Relation Extraction', 1),\n",
       " ('Artificial Intelligence Center SRI International', 1),\n",
       " ('Traditional Solutions and an Alternative Approach', 1),\n",
       " ('Technical Preliminaries', 1),\n",
       " (\"Using Restriction to Extend Ear-. ley's Algorithm for PATR-II\", 1),\n",
       " ('Applications', 1),\n",
       " ('Encountering new words', 1),\n",
       " ('Bilingual lexicon as seed words', 1),\n",
       " ('Ranking translation candidates', 1),\n",
       " ('Confidence on seed word pairs', 1),\n",
       " ('Related work', 1),\n",
       " ('Discussions', 1),\n",
       " ('Conclusions', 1),\n",
       " ('Linear and Syntactic Order of a Sentence', 1),\n",
       " ('Projective Dependency Grammars Revisited', 1),\n",
       " ('A Formalization of', 1),\n",
       " ('A Polynomial Parser for PP-GDG', 1),\n",
       " ('Task description', 1),\n",
       " ('Chunk Types', 1),\n",
       " ('Support Vector Machines', 1),\n",
       " ('Approach for Chunk Identification', 1),\n",
       " ('Discussion', 1),\n",
       " ('Participating Systems', 1),\n",
       " ('Efficient Feature Induction for CRFs', 1),\n",
       " ('Web-augmented Lexicons', 1),\n",
       " ('Feature Set', 1),\n",
       " ('Results and Discussion', 1),\n",
       " ('METEOR: Metric for Evaluation of Translation with Explicit ORdering', 1),\n",
       " ('The METEOR Metric . Weaknesses in BLEU Addressed in METEOR', 1),\n",
       " ('Evaluation of the METEOR Metric', 1),\n",
       " ('Future Work', 1),\n",
       " ('Features', 1),\n",
       " ('Inductive Deterministic Parsing', 1),\n",
       " ('Non-Projective Relations', 1),\n",
       " ('Performance', 1),\n",
       " ('Experiments', 1),\n",
       " ('Error Analysis', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contador_sessoes(papers_sem_abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09df8b8",
   "metadata": {},
   "source": [
    "contador_sessoes(papers_sem_introducao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "405e0875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.753157Z",
     "start_time": "2023-02-23T00:49:55.719988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 176),\n",
       " ('Introduction', 69),\n",
       " ('Discussion', 27),\n",
       " ('', 18),\n",
       " ('Experiments', 11),\n",
       " ('Related Work', 8),\n",
       " ('Results', 8),\n",
       " ('Acknowledgment', 7),\n",
       " ('Experimental Results', 4),\n",
       " ('Evaluation.', 3),\n",
       " ('Discussion.', 3),\n",
       " ('Summary', 3),\n",
       " ('Evaluation', 3),\n",
       " ('Acknowledgement', 3),\n",
       " ('Support Vector Machines', 3),\n",
       " ('Previous Work', 3),\n",
       " ('Motivation', 3),\n",
       " ('Experiments.', 2),\n",
       " ('Summary.', 2),\n",
       " ('Related work', 2),\n",
       " ('. Results', 2),\n",
       " ('Method', 2),\n",
       " ('REFERENCES', 2),\n",
       " ('INTRODUCTION', 2),\n",
       " ('TRANSLATION DELIVERY SYSTEM', 1),\n",
       " ('TESTING AND EVALUATING MULTI-ENGINE PERFORMANCE', 1),\n",
       " ('CURRENT AND FUTURE WORK', 1),\n",
       " ('.. The Amount of Training Data Required', 1),\n",
       " ('Grammar Transformation.', 1),\n",
       " ('LM Adaptation via Sentence Retrieval.', 1),\n",
       " ('Structured Query Models.', 1),\n",
       " ('Decision Trees.', 1),\n",
       " ('Splitting of the POS Tags.', 1),\n",
       " ('Our Tagger.', 1),\n",
       " ('Motivation and Theoretical implications.', 1),\n",
       " ('A CUG Grammar  Model that Aecomodates Word', 1),\n",
       " ('Previous work.', 1),\n",
       " ('Word sense disambiguation with VLNNs.', 1),\n",
       " ('Conclus ion.', 1),\n",
       " ('Difficulties in the Identification of Words.', 1),\n",
       " ('Mot ivat ions.', 1),\n",
       " ('SLTAG.', 1),\n",
       " ('If b\\\\[..\\\\] ~ t\\\\[..Talt\\\\[$~\\\\]: l(b, , i,j,k,I)=.', 1),\n",
       " ('N V P is al', 1),\n",
       " ('Overview', 1),\n",
       " ('General outline of the algorithm.', 1),\n",
       " ('Example output.', 1),\n",
       " ('Conc lus ion.', 1),\n",
       " ('Related Work.', 1),\n",
       " ('Chinese Syntactic Reordering Rules.', 1),\n",
       " ('N -gram Language Models', 1),\n",
       " ('Related Work on Distributed Language.', 1),\n",
       " ('Stupid Backoff.', 1),\n",
       " ('Distributed Training.', 1),\n",
       " ('Distributed Application.', 1),\n",
       " ('Opinion extraction: Task design.', 1),\n",
       " ('Minimum Bayes Risk Decoding', 1),\n",
       " ('Lattice MBR Decoding', 1),\n",
       " ('WFSA MBR Computations', 1),\n",
       " ('Linear Corpus BLEU', 1),\n",
       " ('An Example', 1),\n",
       " ('Minimum Error Rate Training on N-best Lists', 1),\n",
       " ('Minimum Error Rate Training on Lattices', 1),\n",
       " ('Upper Bound for Size of Envelopes', 1),\n",
       " ('Practical Aspects', 1),\n",
       " ('Form of the Model', 1),\n",
       " ('Adaptation', 1),\n",
       " ('Cross-Lingual Projection: Background', 1),\n",
       " ('Unsupervised Cross-Lingual Projection', 1),\n",
       " ('Supervised Cross-Lingual Projection', 1),\n",
       " ('Two Orthogonal Semantic Models', 1),\n",
       " ('A Hybrid Logico-Distributional Model', 1),\n",
       " ('Building Matrices for Relational Words', 1),\n",
       " ('Computing Sentence Vectors', 1),\n",
       " ('Future Work', 1),\n",
       " ('Methods and experiments', 1),\n",
       " ('Concluding remarks', 1),\n",
       " ('The WFST Reordering Model.', 1),\n",
       " ('Estimation of the Reordering Models.', 1),\n",
       " ('Translation Experiments.', 1),\n",
       " ('. Model Estimation', 1),\n",
       " ('. Discussion', 1),\n",
       " ('Cambridge, MA', 1),\n",
       " ('IBM', 1),\n",
       " ('I INTRODUCTION', 1),\n",
       " ('THE LANGUAGE MODEL', 1),\n",
       " ('THE TRANSLATION MODEL', 1),\n",
       " ('SEARCHING', 1),\n",
       " ('PARAMETER ESTIMATION', 1),\n",
       " ('Two PILOT EXPERIMENTS', 1),\n",
       " ('PLANS', 1),\n",
       " ('QXX  XXX  XXX', 1),\n",
       " ('. Corpus-based Methods', 1),\n",
       " ('Definition: M-OPTIMIZE', 1),\n",
       " ('Chunking', 1),\n",
       " ('Integration of Logic Prover into a QA System', 1),\n",
       " ('Logic Representation of Text', 1),\n",
       " ('World Knowledge Axioms', 1),\n",
       " ('NLP Axioms', 1),\n",
       " ('Appositions', 1),\n",
       " ('Control Strategy', 1),\n",
       " ('An example', 1),\n",
       " ('Factored Language Models', 1),\n",
       " ('Generalized Parallel Backoff', 1),\n",
       " ('SRILM-FLM extensions', 1),\n",
       " ('Word Clustering', 1),\n",
       " ('Discriminative Name Tagger', 1),\n",
       " ('Active Learning', 1),\n",
       " ('Orientation Unigram Model', 1),\n",
       " ('Abstract*', 1),\n",
       " ('Treebanking', 1),\n",
       " ('PropBanking', 1),\n",
       " ('Word Sense', 1),\n",
       " ('Ontology', 1),\n",
       " ('Coreference', 1),\n",
       " ('Related and Future Work', 1),\n",
       " ('Dependency Reparsing', 1),\n",
       " ('Constituent Reparsing', 1),\n",
       " ('. Metadata-based methods', 1),\n",
       " ('. Future work', 1),\n",
       " ('Easy-first parsing', 1),\n",
       " ('Parsing algorithm', 1),\n",
       " ('Learning Algorithm', 1),\n",
       " ('return false  return true', 1),\n",
       " ('Feature Representation', 1),\n",
       " ('Computational Complexity and Efficient Implementation', 1),\n",
       " ('Experiments and Results', 1),\n",
       " ('Models', 1),\n",
       " (\"Training'\", 1),\n",
       " ('Background and Motivation', 1),\n",
       " ('Transducers and Parameters', 1),\n",
       " ('Estimation in Parameterized FSTs', 1),\n",
       " ('The E Step: Expectation Semirings', 1),\n",
       " ('Removing Inefficiencies', 1),\n",
       " ('Feature?Vector Representations of Parse.', 1),\n",
       " ('Robust Parsing using LFG', 1),\n",
       " ('Discriminative Statistical Estimation from Partially Labeled Data', 1),\n",
       " ('Experimental Evaluation', 1),\n",
       " ('Evaluating a CCG parser.', 1),\n",
       " ('CCGbank?a CCG treebank.', 1),\n",
       " ('Generative models of CCG derivations.', 1),\n",
       " ('Extending the baseline model.', 1),\n",
       " ('Morpheme Segmentation', 1),\n",
       " ('Unsupervised Acquisition of New Stems', 1),\n",
       " ('Performance Evaluations', 1),\n",
       " ('Summary and Future Work', 1),\n",
       " ('Introduction: Tree-to-Tree Mappings', 1),\n",
       " ('A Natural Proposal: Synchronous TSG', 1),\n",
       " ('Past Work', 1),\n",
       " ('A Probabilistic TSG Formalism', 1),\n",
       " ('Tree Parsing Algorithms for TSG', 1),\n",
       " ('Extending to Synchronous TSG', 1),\n",
       " ('Probabilistic model', 1),\n",
       " ('Parsing with PCFG-LA', 1),\n",
       " ('Disambiguation models for HPSG', 1),\n",
       " ('Packed representation of HPSG parse trees', 1),\n",
       " ('Features', 1),\n",
       " ('Discussion and related work', 1),\n",
       " ('..', 1),\n",
       " ('MSR-MT', 1),\n",
       " ('Kernel Methods', 1),\n",
       " ('Kernel Relation Detection', 1),\n",
       " ('PHYS PHYS EMP-ORG', 1),\n",
       " (') Dependency path kernel', 1),\n",
       " ('Further Work', 1),\n",
       " ('Discovery of Patterns', 1),\n",
       " ('Discovery of Categories', 1),\n",
       " ('Block Sequence Model', 1),\n",
       " ('Approximate Relevant Set Method', 1),\n",
       " ('Discussion and Future Work', 1),\n",
       " ('Pitman-Yor Process', 1),\n",
       " ('Hierarchical Pitman-Yor Language Models', 1),\n",
       " ('Hierarchical Chinese Restaurant Processes', 1),\n",
       " ('Function DrawWord(u):', 1),\n",
       " ('Inference Schemes', 1),\n",
       " ('Baseline MT System', 1),\n",
       " ('. Results EuroParl', 1),\n",
       " ('NIST', 1),\n",
       " ('(NP (NNP Air) (NNP Force) (NN contract))', 1),\n",
       " ('Background', 1),\n",
       " ('Corpus Creation', 1),\n",
       " ('Corpus Analysis', 1),\n",
       " ('Composition Models', 1),\n",
       " ('Evaluation Set-up', 1),\n",
       " ('The Narrative Chain Model', 1),\n",
       " ('Learning Narrative Relations', 1),\n",
       " ('.. Baseline', 1),\n",
       " ('.. Results', 1),\n",
       " ('Ordering Narrative Events', 1),\n",
       " ('Discrete Narrative Event Chains', 1),\n",
       " ('Phrase Alignment Problems', 1),\n",
       " ('Complexity of Inference in A', 1),\n",
       " ('COUNTING PERFECT MATCHINGS, CPM', 1),\n",
       " ('Solving the Optimization Problem', 1),\n",
       " ('Translation Hypergraphs', 1),\n",
       " ('Minimum Error Rate Training', 1),\n",
       " ('Minimum Bayes-Risk Decoding', 1),\n",
       " ('MERT for MBR Parameter Optimization', 1),\n",
       " ('Problem Definition and Approach', 1),\n",
       " ('System Architecture', 1),\n",
       " ('Data sets and Evaluation Methodology', 1),\n",
       " ('The Transition-based Parsing Algorithm', 1),\n",
       " ('Feature Templates', 1),\n",
       " ('. agreement', 1),\n",
       " ('Mary = [hair: blond]', 1),\n",
       " ('[', 1),\n",
       " ('speaker:', 1),\n",
       " ('I Overview', 1),\n",
       " ('II The Formalism', 1),\n",
       " ('III Translation', 1),\n",
       " ('The Logical Notation', 1),\n",
       " ('Opaque Adverbials', 1),\n",
       " ('De Re and De Dicto Belief Reports', 1),\n",
       " ('Identity in Belief Contexts', 1),\n",
       " ('Ezist(Qt) A Exiat(Q)', 1),\n",
       " ('y)Evening-Star(z) A Evening-Star(y) rel-identieal(z, y)', 1),\n",
       " ('The Role of Semantics', 1),\n",
       " ('Acknowledgement.', 1),\n",
       " ('FocusList: Heventl],[drivel],[/-]]', 1),\n",
       " ('Tree Sets of Various Formalisms', 1),\n",
       " ('Dependencies between Paths', 1),\n",
       " ('Linear Context-Free Rewriting Systems', 1),\n",
       " ('References', 1),\n",
       " ('save X from Y ( concordance lines)  save PERSON from Y ( concordance lines)',\n",
       "  1),\n",
       " ('. save INST(ITUTION) from (ECON) BAD ( concordance lines)', 1),\n",
       " ('. save ANIMAL from DESTRUCT(ION) ( concordance lines)', 1),\n",
       " ('I.', 1),\n",
       " ('THE HANSARD CORPORA', 1),\n",
       " ('ALIGNING ANCHOR POINTS', 1),\n",
       " ('ALIGNING SENTENCES AND PARAGRAPH BOUNDARIES', 1),\n",
       " ('BEAD', 1),\n",
       " ('STOP', 1),\n",
       " ('RESULTS', 1),\n",
       " ('STATISTICAL TRANSLATION', 1),\n",
       " ('SENSES BASED ON BINARY QUESTIONS', 1),\n",
       " ('A PILOT EXPERIMENT', 1),\n",
       " ('ACKNOWLEGMENTS', 1),\n",
       " ('The Alignment Model', 1),\n",
       " ('Attribute grammar interpretation', 1),\n",
       " ('Actor-based GB parsing', 1),\n",
       " ('* Vifk fie T Itiffi', 1),\n",
       " ('Methodology and experimental results', 1),\n",
       " ('Charts', 1),\n",
       " ('Generation', 1),\n",
       " ('The Algorithm Schema', 1),\n",
       " ('Internal and External Indices', 1),\n",
       " ('Indexing', 1),\n",
       " ('Identifying Genres: Generic Cues', 1),\n",
       " ('Classifying verbs', 1),\n",
       " ('Intersective Levin classes', 1),\n",
       " ('Cross-linguistic verb classes', 1),\n",
       " ('Linear and Syntactic Order of a Sentence', 1),\n",
       " ('Projective Dependency Grammars Revisited', 1),\n",
       " ('A Formalization of', 1),\n",
       " ('A Polynomial Parser for PP-GDG', 1),\n",
       " ('Significance of WYSIWYM editing', 1),\n",
       " ('Full automation', 1),\n",
       " ('Broad-coverage parsing', 1),\n",
       " ('Labeled, semantic relations', 1),\n",
       " ('Semantic relation structures', 1),\n",
       " ('Full inversion of structures', 1),\n",
       " ('Weighted paths', 1),\n",
       " ('Similarity and inference', 1),\n",
       " ('Disambiguating MindNet', 1),\n",
       " ('Notation for context-free grammars', 1),\n",
       " ('Bilexical context-free grammars', 1),\n",
       " ('Bilexical CFG in time (n)', 1),\n",
       " ('A more efficient variant', 1),\n",
       " ('Multiple word senses', 1),\n",
       " ('Final remarks', 1),\n",
       " ('Approach for Chunk Identification', 1),\n",
       " ('Dependency Analysis using SVMs', 1),\n",
       " ('Experiments and Discussion', 1),\n",
       " ('System Overview', 1),\n",
       " ('Baseline Quantitative Metrics', 1),\n",
       " ('Qualitative Evaluation of the Quantitative Metrics', 1),\n",
       " ('Framework', 1),\n",
       " ('Discourse Annotation Task', 1),\n",
       " ('Quality Assurance', 1),\n",
       " ('Corpus Details', 1),\n",
       " ('Evaluation Data', 1),\n",
       " ('Spelling-Based Model', 1),\n",
       " ('Combining The Phonetic-Based and Spelling-Based Models', 1),\n",
       " ('Improving Transliterations', 1),\n",
       " ('Evaluation and Experimental Results', 1),\n",
       " ('Parameter Estimation.', 1),\n",
       " ('Knowledge Sources', 1),\n",
       " ('Learning Algorithms', 1),\n",
       " ('Evaluation Data Sets', 1),\n",
       " ('Empirical Results', 1),\n",
       " ('Discussions', 1),\n",
       " ('Resources', 1),\n",
       " ('Translation System Description', 1),\n",
       " ('Spoken Document Retrieval System', 1),\n",
       " ('A Large Chinese-English Translation', 1),\n",
       " ('Concluding Remarks', 1),\n",
       " ('The Senseval- Task', 1),\n",
       " ('Target MRL?s.', 1),\n",
       " ('Semantic Parsing Framework.', 1),\n",
       " ('Corpus Annotation.', 1),\n",
       " ('Integrated Parsing Model.', 1),\n",
       " ('Previous work', 1),\n",
       " ('Proceedure', 1),\n",
       " ('Appendix I.', 1),\n",
       " ('A Graph for Sentiment Categorization', 1),\n",
       " ('Graph-Based Semi-Supervised Learning', 1),\n",
       " ('Frame semantics and FrameNet.', 1),\n",
       " ('Definition of the task.', 1),\n",
       " ('Participants.', 1),\n",
       " ('Results.', 1),\n",
       " ('SEMANTIC HIERARCHY', 1),\n",
       " ('SEMANTIC DISTANCE', 1),\n",
       " ('DI D', 1),\n",
       " ('TRAINING AND TESTING DATA', 1),\n",
       " ('Word sense disambiguation of gloss concepts', 1),\n",
       " ('Logical form transformation', 1),\n",
       " ('Semantic form transformation', 1),\n",
       " ('Include more derivational morphology', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Results, Discussion, Concluding remarks, Experiments and Results, Conclustion and Future Research\n",
    "contador_sessoes(papers_sem_conclusao)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a437010",
   "metadata": {},
   "source": [
    "Seleção dos papers que possuaim as 3 sessões simultâneamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e974a43e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.768111Z",
     "start_time": "2023-02-23T00:49:55.754159Z"
    }
   },
   "outputs": [],
   "source": [
    "indice_papers=[]\n",
    "for idx in list(indices_papers_com_abstract):\n",
    "    if idx in list(indices_papers_com_introducao):\n",
    "        if idx in list(indices_papers_com_conclusao):\n",
    "            indice_papers.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c67cff55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.783592Z",
     "start_time": "2023-02-23T00:49:55.769103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de papers com as três sessões simulataneamente: 707\n"
     ]
    }
   ],
   "source": [
    "print('Quantidade de papers com as três sessões simulataneamente:',len(indice_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d05e9487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.802447Z",
     "start_time": "2023-02-23T00:49:55.785566Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_faltantes=[]\n",
    "for i in indices:\n",
    "    if i not in indice_papers:\n",
    "        indices_faltantes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52127e6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.817701Z",
     "start_time": "2023-02-23T00:49:55.804936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de papers sem as três sessões: 302\n"
     ]
    }
   ],
   "source": [
    "print('Quantidade de papers sem as três sessões:',len(indices_faltantes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392f086",
   "metadata": {},
   "source": [
    "Portanto, dos 1009 papers utilizaremos 707 para o estudo do objetivo, pois somente estes possuem simultaneamente as sessões `abstract`, `introdução` e `conclusão`.\n",
    "\n",
    "Separado 70% em treino e 30% em teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e30f3952",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.832847Z",
     "start_time": "2023-02-23T00:49:55.819236Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_treino = indice_papers[0:495]\n",
    "indices_teste  = indice_papers[495:708]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baf2161e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:49:55.859929Z",
     "start_time": "2023-02-23T00:49:55.835843Z"
    }
   },
   "outputs": [],
   "source": [
    "papers_de_fora = dict((key,value) for key, value in papers_com_indice.items() if key in indices_faltantes)\n",
    "\n",
    "papers        = dict((key,value) for key, value in papers_com_indice.items() if key in indice_papers)\n",
    "papers_treino = dict((key,value) for key, value in papers_com_indice.items() if key in indices_treino)\n",
    "papers_teste  = dict((key,value) for key, value in papers_com_indice.items() if key in indices_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7962c",
   "metadata": {},
   "source": [
    "Seguir o estudo com os artigos armazenados na variável `papers`. \\\n",
    "Os armazenados na variável `papers_de_fora` não entrarão no estudo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a2fb69",
   "metadata": {},
   "source": [
    "# Separação das Seções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afd47834",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:00.056462Z",
     "start_time": "2023-02-23T00:49:55.862036Z"
    }
   },
   "outputs": [],
   "source": [
    "#Definição do Abstract\n",
    "abstract=[]\n",
    "for i in papers_treino.values():\n",
    "    abstract.append(preprocessa_abstract(i))\n",
    "    \n",
    "#Definição da Introdução\n",
    "intro=[]\n",
    "for i in papers_treino.values():\n",
    "    intro.append(preprocessa_intro(i))\n",
    "    \n",
    "#Definição da Conclusão\n",
    "conclusion=[]\n",
    "for i in papers_treino.values():\n",
    "    conclusion.append(preprocessa_conclusion(i))\n",
    "    \n",
    "#Tokens\n",
    "token_abstract   = []\n",
    "for texto in abstract:\n",
    "    token_abstract.append(texto.split())\n",
    "    \n",
    "token_intro   = []\n",
    "for texto in intro:\n",
    "    token_intro.append(texto.split())\n",
    "    \n",
    "token_conclusion   = []\n",
    "for texto in conclusion:\n",
    "    token_conclusion.append(texto.split())\n",
    "\n",
    "tokens = token_abstract+token_intro+token_conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b2034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:02.308603Z",
     "start_time": "2023-02-23T00:50:00.058484Z"
    }
   },
   "source": [
    "#Criação de arquivo para treinar o GloVe em C\n",
    "adiciona = ''\n",
    "for i,j,k in zip(abstract, intro, conclusion):\n",
    "    adiciona = adiciona + i + ''+ j + ''+ k +' \\n '\n",
    "    \n",
    "#open text file\n",
    "text_file = open(\"treino_glove.txt\", \"w\")\n",
    "\n",
    "#write string to file\n",
    "text_file.write(adiciona)\n",
    "\n",
    "#close file\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a1311",
   "metadata": {},
   "source": [
    "# Leitura da Sumarização Padrão Ouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d40dca5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:02.342205Z",
     "start_time": "2023-02-23T00:50:02.311095Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('lista_padrao_ouro'):\n",
    "    lista_padrao_ouro = joblib.load('lista_padrao_ouro')\n",
    "else:\n",
    "    list_of_files = os.listdir('scisummnet_release1.1__20190413/top1000_complete')\n",
    "\n",
    "    list_of_files2= []\n",
    "    for i in list_of_files:\n",
    "        nome = 'scisummnet_release1.1__20190413/top1000_complete/' + i + '/' + 'summary/' +os.listdir(f'scisummnet_release1.1__20190413/top1000_complete/{i}/summary')[0]\n",
    "        list_of_files2.append(nome)\n",
    "\n",
    "\n",
    "    lines = []\n",
    "    for file in list_of_files2:\n",
    "        f= open(file,'r', encoding='utf-8' )\n",
    "        lines.append(f.readlines())\n",
    "        f.close()\n",
    "\n",
    "    lista_padrao_ouro=[]\n",
    "    for j in lines:\n",
    "        texto=''\n",
    "        for i in j:\n",
    "            texto = texto + str(i)\n",
    "        lista_padrao_ouro.append(texto)\n",
    "\n",
    "        joblib.dump(lista_padrao_ouro, 'lista_padrao_ouro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50bc10a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:02.357408Z",
     "start_time": "2023-02-23T00:50:02.343519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1009"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_padrao_ouro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55d4b14e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:02.378829Z",
     "start_time": "2023-02-23T00:50:02.359747Z"
    }
   },
   "outputs": [],
   "source": [
    "#Aplicando indices aos papers\n",
    "indices           = range(0,len(lista_papers))\n",
    "padrao_ouro = dict(zip(indices, lista_padrao_ouro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47474ce6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:02.409339Z",
     "start_time": "2023-02-23T00:50:02.381124Z"
    }
   },
   "outputs": [],
   "source": [
    "selecao_padrao_ouro = dict((key,value) for key, value in padrao_ouro.items() if key in  indice_papers)\n",
    "selecao_padrao_ouro_treino = dict((key,value) for key, value in padrao_ouro.items() if key in  indices_treino)\n",
    "selecao_padrao_ouro_teste = dict((key,value) for key, value in padrao_ouro.items() if key in  indices_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f3cce19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:02.421861Z",
     "start_time": "2023-02-23T00:50:02.411580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selecao_padrao_ouro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81c1d78d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:02.437373Z",
     "start_time": "2023-02-23T00:50:02.424214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selecao_padrao_ouro_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e7f4c",
   "metadata": {},
   "source": [
    "# Experimento 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71732596",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480bc742",
   "metadata": {},
   "source": [
    "#### Word 2 Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1949294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-19T19:49:22.559383Z",
     "start_time": "2023-02-19T19:34:05.876639Z"
    }
   },
   "source": [
    "model_cbow = Word2Vec(tokens, min_count=1, epochs=100, sg=0)\n",
    "joblib.dump(model_cbow,'model_cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d9bfbcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:02.747006Z",
     "start_time": "2023-02-23T00:50:02.439436Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_cbow'):\n",
    "    model_cbow = joblib.load('model_cbow')\n",
    "else:\n",
    "    model_cbow = Word2Vec(tokens, min_count=1, epochs=100, sg=0)\n",
    "    joblib.dump(model_cbow,'model_cbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87103908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-19T19:55:09.064758Z",
     "start_time": "2023-02-19T19:49:22.563901Z"
    }
   },
   "source": [
    "model_sg = Word2Vec(tokens, min_count=1, epochs=100, sg=1)\n",
    "joblib.dump(model_sg,'model_sg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1d66dd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:03.009768Z",
     "start_time": "2023-02-23T00:50:02.758009Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('model_sg'):\n",
    "    model_sg = joblib.load('model_sg')\n",
    "else:\n",
    "    model_sg = Word2Vec(tokens, min_count=1, epochs=100, sg=1)\n",
    "    joblib.dump(model_sg,'model_sg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbf05b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:03.027786Z",
     "start_time": "2023-02-23T00:50:03.011860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38854"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_sg.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b37329a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:03.043218Z",
     "start_time": "2023-02-23T00:50:03.029678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38854"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_cbow.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ffcafa",
   "metadata": {},
   "source": [
    "#### Fast Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32a53e",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/fasttext.html\n",
    "\n",
    "https://fasttext.cc/docs/en/python-module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fc6ce0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:04.491112Z",
     "start_time": "2023-02-23T00:50:03.046114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['string.txt']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s= abstract + intro + conclusion\n",
    "lista_papers2=''\n",
    "for i in s:\n",
    "    lista_papers2= lista_papers2+ i\n",
    "    \n",
    "joblib.dump(lista_papers2, 'string.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d849beaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:57.539577Z",
     "start_time": "2023-02-23T00:50:04.492773Z"
    }
   },
   "outputs": [],
   "source": [
    "model_ft = fasttext.train_unsupervised('string.txt', model='skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b1e35e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:57.571039Z",
     "start_time": "2023-02-23T00:50:57.539577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11391"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_ft.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "937587f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:57.587127Z",
     "start_time": "2023-02-23T00:50:57.571039Z"
    }
   },
   "outputs": [],
   "source": [
    "def vetor_frases_fasttext(palavras_frases):\n",
    "    \"\"\"input: lista de palavras de cada frase\n",
    "        output: lista de vetores de cada frase\"\"\"\n",
    "    x=[]\n",
    "    for i in palavras_frases:\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            try:\n",
    "                soma = soma +model_ft.get_word_vector(j)\n",
    "            except:\n",
    "                soma=soma+0\n",
    "        x.append(soma)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac43d12",
   "metadata": {},
   "source": [
    "## Medida de Similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b11d4606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:57.618737Z",
     "start_time": "2023-02-23T00:50:57.588990Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_intro(corpus: str):\n",
    "    regIter = re.finditer('(<SECTION.*[Ii][nN][tT][rR][oO][dD][Uu][Cc][Tt][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = re.sub('e.g.','',txt)\n",
    "        txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "        txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "        txt = txt.strip()\n",
    "        lista.append(txt)\n",
    "    return txt\n",
    "\n",
    "def preprocessa_abstract(corpus: str):\n",
    "    regIter = re.finditer(\"(<[Aa][Bb][sS][tT][Rr][aA][cC][tT]>)(.+?)(</[aA][bB][sS][tT][rR][aA][cC][tT]>)\", corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = re.sub('e.g.','',txt)\n",
    "        txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "        txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "        txt = txt.strip()\n",
    "        lista.append(txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def preprocessa_conclusion(corpus: str):\n",
    "    regIter1 = re.finditer('(<SECTION.*[Cc][Oo][Nn][Cc][Ll][Uu][Ss][Ii][Oo][Nn].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos1 = [ t.group(2) for t in regIter1]\n",
    "    regIter2 = re.finditer('(<SECTION.*[Aa][Cc][Kk][Nn][Oo][Ww][Ll][Ee][Dd][Gg][Mm][Ee][Nn][Tt][Ss].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos2 = [ t.group(2) for t in regIter2]\n",
    "    regIter3 = re.finditer('(<SECTION.*[Aa][Cc][Kk][Nn][Oo][Ww][Ll][Ee][Dd][Gg][Ee][Mm][Ee][Nn][Tt][Ss].*?>(.*)<\\/SECTION>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos3 = [ t.group(2) for t in regIter3]\n",
    "    textos = textos1\n",
    "    for i in textos2:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    for i in textos3:\n",
    "        if i not in textos:\n",
    "            textos.append(i)\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = re.sub('e.g.','',txt)\n",
    "        txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "        txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "        txt = txt.strip()\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43a94a4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:57.634047Z",
     "start_time": "2023-02-23T00:50:57.620049Z"
    }
   },
   "outputs": [],
   "source": [
    "def funcao_frase_palavra(frases_abstract):\n",
    "    \"\"\"input: lista de frases\n",
    "        output: lista de palavras de cada frase\"\"\"\n",
    "    palavras_frases=[]\n",
    "    for i in frases_abstract:\n",
    "        palavras_frases.append(i.split(' '))\n",
    "    try:\n",
    "        palavras_frases.remove('')\n",
    "    except:\n",
    "        return palavras_frases\n",
    "    return palavras_frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0149325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:57.652225Z",
     "start_time": "2023-02-23T00:50:57.636052Z"
    }
   },
   "outputs": [],
   "source": [
    "def vetor_frases_sg(palavras_frases):\n",
    "    \"\"\"input: lista de palavras de cada frase\n",
    "        output: lista de vetores de cada frase\"\"\"\n",
    "    x=[]\n",
    "    for i in palavras_frases:\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            try:\n",
    "                soma = soma +model_sg.wv[j]\n",
    "            except:\n",
    "                soma=soma+0\n",
    "        x.append(soma)\n",
    "    return x\n",
    "\n",
    "def vetor_frases_cbow(palavras_frases):\n",
    "    \"\"\"input: lista de palavras de cada frase\n",
    "        output: lista de vetores de cada frase\"\"\"\n",
    "    x=[]\n",
    "    for i in palavras_frases:\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            try:\n",
    "                soma = soma +model_sg.wv[j]\n",
    "            except:\n",
    "                soma=soma+0\n",
    "        x.append(soma)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "144bceb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:50:57.668276Z",
     "start_time": "2023-02-23T00:50:57.655275Z"
    }
   },
   "outputs": [],
   "source": [
    "def funcao_sim_coseno(a,b):\n",
    "    if ((numpy.linalg.norm(a)==0) |(numpy.linalg.norm(b)==0)):\n",
    "        return 0\n",
    "    else:\n",
    "        valor_coseno = (numpy.dot(a, b))/(numpy.linalg.norm(a)* numpy.linalg.norm(b))\n",
    "        return valor_coseno\n",
    "    \n",
    "def dicionario_similaridade(vetor_a,vetor_b):\n",
    "    \"\"\"Input: vetor correspondente às frases das seções a e b\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    dicionario={}\n",
    "    for i in range(len(vetor_a)):\n",
    "        for j in range(len(vetor_b)):\n",
    "            dicionario[i,j]= funcao_sim_coseno(vetor_a[i],vetor_b[j])\n",
    "    return dicionario\n",
    "\n",
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d79a01db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:00.961862Z",
     "start_time": "2023-02-23T00:50:57.670380Z"
    }
   },
   "outputs": [],
   "source": [
    "#Definição do Abstract\n",
    "abstract=[]\n",
    "for i in papers_treino.values():\n",
    "    abstract.append(preprocessa_abstract(i))\n",
    "    \n",
    "#Definição da Introdução\n",
    "intro=[]\n",
    "for i in papers_treino.values():\n",
    "    intro.append(preprocessa_intro(i))\n",
    "    \n",
    "#Definição da Conclusão\n",
    "conclusion=[]\n",
    "for i in papers_treino.values():\n",
    "    conclusion.append(preprocessa_conclusion(i))\n",
    "    \n",
    "#Tokens\n",
    "token_abstract   = []\n",
    "for texto in abstract:\n",
    "    token_abstract.append(texto.split())\n",
    "    \n",
    "token_intro   = []\n",
    "for texto in intro:\n",
    "    token_intro.append(texto.split())\n",
    "    \n",
    "token_conclusion   = []\n",
    "for texto in conclusion:\n",
    "    token_conclusion.append(texto.split())\n",
    "\n",
    "tokens = token_abstract+token_intro+token_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c0866e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:00.977777Z",
     "start_time": "2023-02-23T00:51:00.963041Z"
    }
   },
   "outputs": [],
   "source": [
    "def frases_palavras_vetores(secao, funcao_conversao_vetor):\n",
    "    frases_intro=[]\n",
    "    for i in secao:\n",
    "        frases_intro.append(remove_values_from_list(i.split('.'),''))\n",
    "        \n",
    "    frases_intro3=[]\n",
    "    for i in frases_intro:\n",
    "        frases_intro2=[]\n",
    "        for j in i:\n",
    "            frases_intro2.append(j.strip())\n",
    "        frases_intro3.append(frases_intro2)\n",
    "\n",
    "    frases_intro = frases_intro3 \n",
    "    \n",
    "    \n",
    "    palavras_intro=[]\n",
    "    for i in frases_intro:\n",
    "        palavras_intro.append(funcao_frase_palavra(i))\n",
    "\n",
    "    vetor_intro=[]\n",
    "    for i in palavras_intro:\n",
    "        vetor_intro.append(funcao_conversao_vetor(i))\n",
    "        \n",
    "    return frases_intro,palavras_intro,vetor_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e038baf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:00.992771Z",
     "start_time": "2023-02-23T00:51:00.978789Z"
    }
   },
   "outputs": [],
   "source": [
    "def aplicando_dicionario_similaridade(v1,v2):\n",
    "    x = []\n",
    "    for i,j in zip(range(len(v1)),range(len(v2))):\n",
    "        x.append(dicionario_similaridade(v1[i], v2[j]))\n",
    "        \n",
    "    top_3_indices = []\n",
    "    for i in range(len(x)):\n",
    "        top_3_indices.append(sorted(x[i], key=x[i].get, reverse=True)[:3])\n",
    "        \n",
    "    lista_1=[]\n",
    "    for i in top_3_indices:\n",
    "        top3=[]\n",
    "        for j in i:\n",
    "            top3.append(j[0])\n",
    "        lista_1.append(list(set(top3)))\n",
    "        \n",
    "    return x,top_3_indices,lista_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac716162",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:01.010454Z",
     "start_time": "2023-02-23T00:51:00.994774Z"
    }
   },
   "outputs": [],
   "source": [
    "def frases_interseccao(lista_1,lista_2):\n",
    "    indices_objetivo=[]\n",
    "    for i,j in zip(lista_1,lista_2):\n",
    "        indices_objetivo.append(list(set.intersection(*map(set,[i,j]))))\n",
    "    return indices_objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c6fcd31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:01.026503Z",
     "start_time": "2023-02-23T00:51:01.011775Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frases_objetivo(indices_objetivo):\n",
    "    \"\"\"retorna a frase objetivo a partir dos indices\"\"\"\n",
    "    frase_objetivo=[]\n",
    "    for idx_f,idx_ind in zip(range(len(frases_abstract)),indices_objetivo):\n",
    "        lista_one=[]\n",
    "        for j in idx_ind:\n",
    "            lista_one.append(frases_abstract[idx_f][j])\n",
    "            #print(idx_f)\n",
    "            #print(j)\n",
    "            #print(frases_abstract[idx_f][j])\n",
    "        frase_objetivo.append(lista_one)\n",
    "    return frase_objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b24de",
   "metadata": {},
   "source": [
    "## Similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "992b76e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:01.043465Z",
     "start_time": "2023-02-23T00:51:01.027491Z"
    }
   },
   "outputs": [],
   "source": [
    "def funcao_frase_palavra(frases_abstract):\n",
    "    \"\"\"input: lista de frases\n",
    "        output: lista de palavras de cada frase\"\"\"\n",
    "    palavras_frases=[]\n",
    "    for i in frases_abstract:\n",
    "        palavras_frases.append(i.split(' '))\n",
    "    try:\n",
    "        palavras_frases.remove('')\n",
    "    except:\n",
    "        return palavras_frases\n",
    "    return palavras_frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "226dc495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:01.059337Z",
     "start_time": "2023-02-23T00:51:01.044876Z"
    }
   },
   "outputs": [],
   "source": [
    "def vetor_frases_sg(palavras_frases):\n",
    "    \"\"\"input: lista de palavras de cada frase\n",
    "        output: lista de vetores de cada frase\"\"\"\n",
    "    x=[]\n",
    "    for i in palavras_frases:\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            try:\n",
    "                soma = soma +model_sg.wv[j]\n",
    "            except:\n",
    "                soma=soma+0\n",
    "        x.append(soma)\n",
    "    return x\n",
    "\n",
    "def vetor_frases_cbow(palavras_frases):\n",
    "    \"\"\"input: lista de palavras de cada frase\n",
    "        output: lista de vetores de cada frase\"\"\"\n",
    "    x=[]\n",
    "    for i in palavras_frases:\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            try:\n",
    "                soma = soma +model_cbow.wv[j]\n",
    "            except:\n",
    "                soma=soma+0\n",
    "        x.append(soma)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f54863e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:01.075980Z",
     "start_time": "2023-02-23T00:51:01.060319Z"
    }
   },
   "outputs": [],
   "source": [
    "def funcao_sim_coseno(a,b):\n",
    "    if ((numpy.linalg.norm(a)==0) |(numpy.linalg.norm(b)==0)):\n",
    "        return 0\n",
    "    else:\n",
    "        valor_coseno = (numpy.dot(a, b))/(numpy.linalg.norm(a)* numpy.linalg.norm(b))\n",
    "        return valor_coseno\n",
    "    \n",
    "def dicionario_similaridade(vetor_a,vetor_b):\n",
    "    \"\"\"Input: vetor correspondente às frases das seções a e b\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    dicionario={}\n",
    "    for i in range(len(vetor_a)):\n",
    "        for j in range(len(vetor_b)):\n",
    "            dicionario[i,j]= funcao_sim_coseno(vetor_a[i],vetor_b[j])\n",
    "    return dicionario\n",
    "\n",
    "def remove_values_from_list(the_list, val):\n",
    "    return [value for value in the_list if value != val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dda2a3e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:04.447785Z",
     "start_time": "2023-02-23T00:51:01.078388Z"
    }
   },
   "outputs": [],
   "source": [
    "#Definição do Abstract\n",
    "abstract=[]\n",
    "for i in papers_treino.values():\n",
    "    abstract.append(preprocessa_abstract(i))\n",
    "    \n",
    "#Definição da Introdução\n",
    "intro=[]\n",
    "for i in papers_treino.values():\n",
    "    intro.append(preprocessa_intro(i))\n",
    "    \n",
    "#Definição da Conclusão\n",
    "conclusion=[]\n",
    "for i in papers_treino.values():\n",
    "    conclusion.append(preprocessa_conclusion(i))\n",
    "    \n",
    "#Tokens\n",
    "token_abstract   = []\n",
    "for texto in abstract:\n",
    "    token_abstract.append(texto.split())\n",
    "    \n",
    "token_intro   = []\n",
    "for texto in intro:\n",
    "    token_intro.append(texto.split())\n",
    "    \n",
    "token_conclusion   = []\n",
    "for texto in conclusion:\n",
    "    token_conclusion.append(texto.split())\n",
    "\n",
    "tokens = token_abstract+token_intro+token_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0bc11c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:04.463730Z",
     "start_time": "2023-02-23T00:51:04.449903Z"
    }
   },
   "outputs": [],
   "source": [
    "def frases_palavras_vetores(secao, funcao_conversao_vetor):\n",
    "    \"\"\"input: nome da seção (abstract, introdução, conclusão) e função que converte as frases em vetores\n",
    "    output: frases, palavras e vetor correspondentes à seção de entrada\"\"\"\n",
    "    frases_intro=[]\n",
    "    for i in secao:\n",
    "        frases_intro.append(remove_values_from_list(i.split('.'),''))\n",
    "        \n",
    "    frases_intro3=[]\n",
    "    for i in frases_intro:\n",
    "        frases_intro2=[]\n",
    "        for j in i:\n",
    "            frases_intro2.append(j.strip())\n",
    "        frases_intro3.append(frases_intro2)\n",
    "\n",
    "    frases_intro = frases_intro3 \n",
    "    \n",
    "    \n",
    "    palavras_intro=[]\n",
    "    for i in frases_intro:\n",
    "        palavras_intro.append(funcao_frase_palavra(i))\n",
    "\n",
    "    vetor_intro=[]\n",
    "    for i in palavras_intro:\n",
    "        vetor_intro.append(funcao_conversao_vetor(i))\n",
    "        \n",
    "    return frases_intro,palavras_intro,vetor_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c30b2775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:04.479367Z",
     "start_time": "2023-02-23T00:51:04.463730Z"
    }
   },
   "outputs": [],
   "source": [
    "def aplicando_dicionario_similaridade(v1,v2):\n",
    "    \"\"\"cálculo da similaridade, pegando os 3 vetores mais similares\"\"\"\n",
    "    x = []\n",
    "    for i,j in zip(range(len(v1)),range(len(v2))):\n",
    "        x.append(dicionario_similaridade(v1[i], v2[j]))\n",
    "        \n",
    "    top_3_indices = []\n",
    "    for i in range(len(x)):\n",
    "        top_3_indices.append(sorted(x[i], key=x[i].get, reverse=True)[:3])\n",
    "        \n",
    "    lista_1=[]\n",
    "    for i in top_3_indices:\n",
    "        top3=[]\n",
    "        for j in i:\n",
    "            top3.append(j[0])\n",
    "        lista_1.append(list(set(top3)))\n",
    "        \n",
    "    return x,top_3_indices,lista_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e850296b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:04.495200Z",
     "start_time": "2023-02-23T00:51:04.479367Z"
    }
   },
   "outputs": [],
   "source": [
    "def frases_interseccao(lista_1,lista_2):\n",
    "    \"\"\"comparando se os índices similares pertencem às duas listas (abstract e conclusão) e (abstract e intro) \n",
    "    e retorna os índices em comum\"\"\"\n",
    "    indices_objetivo=[]\n",
    "    for i,j in zip(lista_1,lista_2):\n",
    "        indices_objetivo.append(list(set.intersection(*map(set,[i,j]))))\n",
    "    return indices_objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63264462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:04.510839Z",
     "start_time": "2023-02-23T00:51:04.495200Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frases_objetivo(indices_objetivo):\n",
    "    \"\"\"retorna a frase objetivo a partir dos indices\"\"\"\n",
    "    frase_objetivo=[]\n",
    "    for idx_f,idx_ind in zip(range(len(frases_abstract)),indices_objetivo):\n",
    "        lista_one=[]\n",
    "        for j in idx_ind:\n",
    "            lista_one.append(frases_abstract[idx_f][j])\n",
    "            #print(idx_f)\n",
    "            #print(j)\n",
    "            #print(frases_abstract[idx_f][j])\n",
    "        frase_objetivo.append(lista_one)\n",
    "    return frase_objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ba74a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:26.530732Z",
     "start_time": "2023-02-23T00:51:04.510839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frase_objetivo_sg']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec - SkipGram\n",
    "\n",
    "#transforma em vetor\n",
    "frases_intro,palavras_intro,vetor_intro                = frases_palavras_vetores(intro, vetor_frases_sg)\n",
    "frases_abstract,palavras_abstract,vetor_abstract       = frases_palavras_vetores(abstract, vetor_frases_sg)\n",
    "frases_conclusion,palavras_conclusion,vetor_conclusion = frases_palavras_vetores(conclusion, vetor_frases_sg)\n",
    "\n",
    "        #Abtract e Introdução\n",
    "sim_ab_intro_sg,indices_ab_intro_sg,lista_idx_ab_intro_sg = aplicando_dicionario_similaridade(vetor_abstract,vetor_intro)\n",
    "\n",
    "        #Abtract e Conclusão\n",
    "sim_ab_conc_sg,indices_ab_conc_sg,lista_idx_ab_c_sg = aplicando_dicionario_similaridade(vetor_abstract,vetor_conclusion)\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_sg = frases_interseccao(lista_idx_ab_intro_sg,lista_idx_ab_c_sg)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_sg = get_frases_objetivo(indices_objetivo_sg)\n",
    "\n",
    "joblib.dump(frase_objetivo_sg,'frase_objetivo_sg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b55308b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:51:53.088007Z",
     "start_time": "2023-02-23T00:51:26.533903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frase_objetivo_cbow']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word2Vec - CBOW\n",
    "\n",
    "#transforma em vetor\n",
    "frases_intro,palavras_intro,vetor_intro_cbow                = frases_palavras_vetores(intro, vetor_frases_cbow)\n",
    "frases_abstract,palavras_abstract,vetor_abstract_cbow       = frases_palavras_vetores(abstract, vetor_frases_cbow)\n",
    "frases_conclusion,palavras_conclusion,vetor_conclusion_cbow = frases_palavras_vetores(conclusion, vetor_frases_cbow)\n",
    "\n",
    "    #Abtract e Introdução\n",
    "sim_ab_intro_cbow, indices_ab_intro_cbow, lista_idx_ab_intro_cbow  = aplicando_dicionario_similaridade(vetor_abstract_cbow,vetor_intro_cbow)\n",
    "\n",
    "    #Abtract e Conclusão\n",
    "sim_ab_conc_cbow, indices_ab_conc_cbow, lista_idx_ab_c_cbow = aplicando_dicionario_similaridade(vetor_abstract_cbow,vetor_conclusion_cbow)\n",
    "\n",
    "#Pega os Indices em comum\n",
    "indices_objetivo_cbow = frases_interseccao(lista_idx_ab_intro_cbow,lista_idx_ab_c_cbow)\n",
    "\n",
    "#Retorna as Frases correspondentes\n",
    "frase_objetivo_cbow = get_frases_objetivo(indices_objetivo_cbow)\n",
    "\n",
    "joblib.dump(frase_objetivo_cbow,'frase_objetivo_cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "feae8024",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:37.889338Z",
     "start_time": "2023-02-23T00:51:53.090005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['frase_objetivo_ft']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FastText\n",
    "frases_intro_ft,palavras_intro_ft,vetor_intro_ft                = frases_palavras_vetores(intro, vetor_frases_fasttext)\n",
    "frases_abstract_ft,palavras_abstract_ft,vetor_abstract_ft       = frases_palavras_vetores(abstract, vetor_frases_fasttext)\n",
    "frases_conclusion_ft,palavras_conclusion_ft,vetor_conclusion_ft = frases_palavras_vetores(conclusion, vetor_frases_fasttext)\n",
    "\n",
    "\n",
    "#Abtract e Introdução\n",
    "sim_ab_intro_ft,indices_ab_intro_ft,lista_idx_ab_intro_ft = aplicando_dicionario_similaridade(vetor_abstract_ft,vetor_intro_ft)\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_ft,indices_ab_conc_ft,lista_idx_ab_c_ft = aplicando_dicionario_similaridade(vetor_abstract_ft,vetor_conclusion_ft)\n",
    "\n",
    "#Indices\n",
    "indices_objetivo_ft = frases_interseccao(lista_idx_ab_intro_ft,lista_idx_ab_c_ft)\n",
    "\n",
    "#Frases\n",
    "frase_objetivo_ft = get_frases_objetivo(indices_objetivo_ft)\n",
    "\n",
    "joblib.dump(frase_objetivo_ft,'frase_objetivo_ft')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5809c7",
   "metadata": {},
   "source": [
    "# ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcc13a64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:37.904468Z",
     "start_time": "2023-02-23T00:52:37.891009Z"
    }
   },
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "12affe87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T03:02:05.275773Z",
     "start_time": "2023-02-23T03:02:05.243463Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_49212\\20641873.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mselecao_padrao_ouro\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselecao_padrao_ouro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mselecao_padrao_ouro_treino\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselecao_padrao_ouro_treino\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mselecao_padrao_ouro_teste\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselecao_padrao_ouro_teste\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "selecao_padrao_ouro        = list(selecao_padrao_ouro.values())\n",
    "selecao_padrao_ouro_treino = list(selecao_padrao_ouro_treino.values())\n",
    "selecao_padrao_ouro_teste  = list(selecao_padrao_ouro_teste.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7537416c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:37.937754Z",
     "start_time": "2023-02-23T00:52:37.924734Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_processamento_ouro(txt):\n",
    "    txt = re.sub(r'\\n',' ', txt)\n",
    "    txt = re.sub(r'<.*?>','', txt)\n",
    "    txt = txt.strip()\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    #txt = re.sub(' . ', '.', txt).strip()\n",
    "    #txt = '<sos> ' + txt.strip() + ' <eos>'\n",
    "    txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "    txt = re.sub('&amp;quot;', '', txt)\n",
    "    txt = re.sub('  ', ' ', txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    txt = re.sub('e.g.','',txt)\n",
    "    txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "    txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "    txt = txt.lower()\n",
    "    txt = txt.strip()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2931eef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:37.955912Z",
     "start_time": "2023-02-23T00:52:37.940185Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_processamento_ouro2(txt):\n",
    "    txt = txt.strip()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e39f9d27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:37.971923Z",
     "start_time": "2023-02-23T00:52:37.957416Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculo_rouge(frase_objetivo,lista_frases_padrao_ouro):\n",
    "    rouge_total=[]\n",
    "    for i,j in zip(frase_objetivo,lista_frases_padrao_ouro):\n",
    "        rouge_1={}\n",
    "        for h in range(len(i)):\n",
    "            for z in range(len(j)):\n",
    "                rouge_1[(h,z)] = (scorer.score(i[h],j[z])['rouge1'].fmeasure)\n",
    "        rouge_total.append(rouge_1)\n",
    "    return rouge_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28feee84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:37.987718Z",
     "start_time": "2023-02-23T00:52:37.972596Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_max(dic, coord, val):\n",
    "    return max(filter(lambda item: item[0][coord] == val, dic.items()), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "519298a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:38.006738Z",
     "start_time": "2023-02-23T00:52:37.989302Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_rouge(rouge_total):\n",
    "    \"\"\"pega o máximo rouge score para cada frase selecionada como pertencente ao objetivo, em comparação com cada frase\n",
    "    do padrão ouro\"\"\"\n",
    "    maximo_rouge_por_paper =[]\n",
    "    for h in rouge_total:\n",
    "        x=[]\n",
    "        for i in set([i[0] for i in h]):\n",
    "            x.append(get_max(h,0,i))\n",
    "        maximo_rouge_por_paper.append(x)\n",
    "    return maximo_rouge_por_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5072130b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:38.088011Z",
     "start_time": "2023-02-23T00:52:38.008720Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_frases_padrao_ouro=[]\n",
    "for i in selecao_padrao_ouro_treino:\n",
    "    texto = pre_processamento_ouro(i).strip().split('.')\n",
    "    texto = remove_values_from_list(texto,'')\n",
    "    lista_frases_padrao_ouro.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a4129e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:38.105693Z",
     "start_time": "2023-02-23T00:52:38.090021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.864646464646465"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[]\n",
    "for i in lista_frases_padrao_ouro:\n",
    "    x.append(len(i))\n",
    "media_ouro=np.mean(x)\n",
    "media_ouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ba8b3ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.782813Z",
     "start_time": "2023-02-23T00:52:38.107837Z"
    }
   },
   "outputs": [],
   "source": [
    "rouge_ft = calculo_rouge(frase_objetivo_ft,lista_frases_padrao_ouro)\n",
    "rouge_sg = calculo_rouge(frase_objetivo_sg,lista_frases_padrao_ouro)\n",
    "rouge_cbow  = calculo_rouge(frase_objetivo_cbow,lista_frases_padrao_ouro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6ff5508",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.799633Z",
     "start_time": "2023-02-23T00:52:54.784935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_cbow == rouge_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "72faa673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.815342Z",
     "start_time": "2023-02-23T00:52:54.801058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_cbow == rouge_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63884004",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.832503Z",
     "start_time": "2023-02-23T00:52:54.817400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_sg == rouge_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb9710e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.848412Z",
     "start_time": "2023-02-23T00:52:54.834315Z"
    }
   },
   "outputs": [],
   "source": [
    "maximo_rouge_por_paper_ft   = max_rouge(rouge_ft)\n",
    "maximo_rouge_por_paper_sg   = max_rouge(rouge_sg)\n",
    "maximo_rouge_por_paper_cbow = max_rouge(rouge_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1891b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.865108Z",
     "start_time": "2023-02-23T00:52:54.849410Z"
    }
   },
   "outputs": [],
   "source": [
    "def rouge_precisao_revocacao(maximo_rouge_por_paper):\n",
    "    precisao=[]\n",
    "    for i in maximo_rouge_por_paper:\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            soma = soma + j[1]\n",
    "        #print(len(i))\n",
    "        try:\n",
    "            precisao.append(soma/len(i))\n",
    "        except:\n",
    "            precisao.append(0)\n",
    "\n",
    "    revocacao=[]\n",
    "    for i,h in zip(maximo_rouge_por_paper,lista_frases_padrao_ouro):\n",
    "        soma=0\n",
    "        for j in i:\n",
    "            soma = soma + j[1]\n",
    "        #print(len(i))\n",
    "        try:\n",
    "            revocacao.append(soma/len(h))\n",
    "        except:\n",
    "            revocacao.append(0)\n",
    "    return precisao, revocacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee24e4a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.880479Z",
     "start_time": "2023-02-23T00:52:54.866633Z"
    }
   },
   "outputs": [],
   "source": [
    "precisao_sg, revocacao_sg = rouge_precisao_revocacao(maximo_rouge_por_paper_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7219fcd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.898362Z",
     "start_time": "2023-02-23T00:52:54.882591Z"
    }
   },
   "outputs": [],
   "source": [
    "precisao_cbow, revocacao_cbow = rouge_precisao_revocacao(maximo_rouge_por_paper_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc92feed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.913389Z",
     "start_time": "2023-02-23T00:52:54.901392Z"
    }
   },
   "outputs": [],
   "source": [
    "precisao_ft, revocacao_ft = rouge_precisao_revocacao(maximo_rouge_por_paper_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b36014",
   "metadata": {},
   "source": [
    "# Experimento 1 - Conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b1428afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.933381Z",
     "start_time": "2023-02-23T00:52:54.915391Z"
    }
   },
   "outputs": [],
   "source": [
    "def analise_precisao(precisao):\n",
    "    x=[]\n",
    "    qtd_zero=[]\n",
    "    for i in precisao:\n",
    "        if i>=0.8:\n",
    "            x.append(i)\n",
    "        if i==0:\n",
    "            qtd_zero.append(i)\n",
    "    print('qtd de papers com precisao>=0.8:',len(x))\n",
    "    print('% do total:',len(x)/len(lista_frases_padrao_ouro))\n",
    "    print('qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções:',len(qtd_zero))\n",
    "    print('% do total:',len(qtd_zero)/len(lista_frases_padrao_ouro))\n",
    "    print('media precisao:', np.mean(precisao))\n",
    "    return len(x), len(qtd_zero),np.mean(precisao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8dec553e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.949189Z",
     "start_time": "2023-02-23T00:52:54.933381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7993235051807926\n",
      "0.7570617628982128\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(precisao_sg))\n",
    "print(np.mean(precisao_cbow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9817d5da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.964907Z",
     "start_time": "2023-02-23T00:52:54.949189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de papers com precisao>=0.8: 372\n",
      "% do total: 0.7515151515151515\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 85\n",
      "% do total: 0.1717171717171717\n",
      "media precisao: 0.7570617628982128\n"
     ]
    }
   ],
   "source": [
    "qtd_precisao_08_cbow, qtd_zero_cbow, media_cbow = analise_precisao(precisao_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d3eff74e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.980506Z",
     "start_time": "2023-02-23T00:52:54.964907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de papers com precisao>=0.8: 394\n",
      "% do total: 0.795959595959596\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 67\n",
      "% do total: 0.13535353535353536\n",
      "media precisao: 0.7993235051807926\n"
     ]
    }
   ],
   "source": [
    "qtd_precisao_08_sg, qtd_zero_sg, media_sg = analise_precisao(precisao_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ff5498d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:54.996551Z",
     "start_time": "2023-02-23T00:52:54.982439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtd de papers com precisao>=0.8: 392\n",
      "% do total: 0.7919191919191919\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 61\n",
      "% do total: 0.12323232323232323\n",
      "media precisao: 0.805909926956016\n"
     ]
    }
   ],
   "source": [
    "qtd_precisao_08_ft, qtd_zero_ft,  media_ft = analise_precisao(precisao_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "de8405c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.013359Z",
     "start_time": "2023-02-23T00:52:54.997698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'media_ft'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisao_mean =  {'media_ft':media_ft,\n",
    "                  'media_sg':media_sg,\n",
    "                  'media_cbow':media_cbow}\n",
    "max(precisao_mean, key=precisao_mean.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cab97ae0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.028520Z",
     "start_time": "2023-02-23T00:52:55.015012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qtd_zero_ft'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtd_papers_hipotese_invalida = {'qtd_zero_sg':qtd_zero_sg,\n",
    "                              'qtd_zero_ft':qtd_zero_ft,\n",
    "                              'qtd_zero_cbow':qtd_zero_cbow}\n",
    "min(qtd_papers_hipotese_invalida, key=qtd_papers_hipotese_invalida.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0f642567",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.046766Z",
     "start_time": "2023-02-23T00:52:55.029511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qtd_precisao_08_sg'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtd_precisao_alta = {'qtd_precisao_08_sg':qtd_precisao_08_sg,\n",
    "                     'qtd_precisao_08_ft':qtd_precisao_08_ft,\n",
    "                     'qtd_precisao_08_cbow':qtd_precisao_08_cbow}\n",
    "max(qtd_precisao_alta, key=qtd_precisao_alta.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "41ebf8b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.060386Z",
     "start_time": "2023-02-23T00:52:55.049208Z"
    }
   },
   "outputs": [],
   "source": [
    "def analise_revocacao(revocacao):\n",
    "    x=[]\n",
    "    qtd_zero=[]\n",
    "    for i in revocacao:\n",
    "        if i>=(1/media_ouro): #valor de 1/média qtd frases padrao ouro\n",
    "            x.append(i)\n",
    "        if i==0:\n",
    "            qtd_zero.append(i)\n",
    "    print('qtd de papers com revocação>=(1/media_ouro):',len(x))\n",
    "    print('% do total:',len(x)/len(lista_frases_padrao_ouro))\n",
    "    print('qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções:',len(qtd_zero))\n",
    "    print('% do total:',len(qtd_zero)/len(lista_frases_padrao_ouro))\n",
    "    return len(x), np.mean(revocacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a474700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.085360Z",
     "start_time": "2023-02-23T00:52:55.077186Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip-gram\n",
      "qtd de papers com revocação>=(1/media_ouro): 299\n",
      "% do total: 0.604040404040404\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 67\n",
      "% do total: 0.13535353535353536\n",
      "cbow\n",
      "qtd de papers com revocação>=(1/media_ouro): 291\n",
      "% do total: 0.5878787878787879\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 85\n",
      "% do total: 0.1717171717171717\n",
      "fasttext\n",
      "qtd de papers com revocação>=(1/media_ouro): 312\n",
      "% do total: 0.6303030303030303\n",
      "qtd de papers onde não há frases similares presentes simultaneamente nas 3 seções: 61\n",
      "% do total: 0.12323232323232323\n"
     ]
    }
   ],
   "source": [
    "print('skip-gram')\n",
    "qtd_revocacao_sg, media_revocacao_sg      = analise_revocacao(revocacao_sg)\n",
    "\n",
    "print('cbow')\n",
    "qtd_revocacao_cbow,  media_revocacao_cbow   = analise_revocacao(revocacao_cbow)\n",
    "\n",
    "print('fasttext')\n",
    "qtd_revocacao_ft,  media_revocacao_ft       = analise_revocacao(revocacao_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dac3e177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.100085Z",
     "start_time": "2023-02-23T00:52:55.087729Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qtd_revocacao_ft'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtd_revocacao_alta = {'qtd_revocacao_sg':qtd_revocacao_sg,\n",
    "                     'qtd_revocacao_ft':qtd_revocacao_ft,\n",
    "                     'qtd_revocacao_cbow':qtd_revocacao_cbow}\n",
    "max(qtd_revocacao_alta, key=qtd_revocacao_alta.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "45e7bead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.122243Z",
     "start_time": "2023-02-23T00:52:55.101090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'media_revocacao_ft'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_revocacao   = {'media_revocacao_sg':media_revocacao_sg,\n",
    "                     'media_revocacao_ft':media_revocacao_ft,\n",
    "                     'media_revocacao_cbow':media_revocacao_cbow}\n",
    "max(media_revocacao, key=media_revocacao.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9be5bbee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.137924Z",
     "start_time": "2023-02-23T00:52:55.124573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20777584647115963"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_revocacao_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "126537cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.156976Z",
     "start_time": "2023-02-23T00:52:55.138898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21296846725291213"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_revocacao_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "af39f6c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.172952Z",
     "start_time": "2023-02-23T00:52:55.159425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19529128532924242"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_revocacao_cbow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80e20a",
   "metadata": {},
   "source": [
    "# Experimento 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a281ed",
   "metadata": {},
   "source": [
    "To do:\n",
    "    - Pegar mais de uma frase objetivo na seleção das probabilidades;\n",
    "    - Melhorar o pré-processamento de forma decente, se náo vai dar errado (remover tags, pontuação e stop words)\n",
    "    - Substituir o bigrama pelo trigrama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67432f02",
   "metadata": {},
   "source": [
    "x=1\n",
    "for i in (bigrams(teste)):\n",
    "    print(f'P({i[0]}|{i[1]}):')\n",
    "    print(model.score(i[1], context=i[0].split()))\n",
    "    a=model.score(i[1], context=i[0].split())\n",
    "    x=x*a\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c806f70",
   "metadata": {},
   "source": [
    "N-grams are a sparse representation of language. It will give zero probability to all the words that are not present in the training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685dd55",
   "metadata": {},
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    " Create a placeholder for model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    " Count frequency of co-occurance  \n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "  Let's transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4b103f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:55.188696Z",
     "start_time": "2023-02-23T00:52:55.174083Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_paper(corpus: str):\n",
    "    regIter = re.finditer('(<PAPER>(.*)<\\/PAPER>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = re.sub('e.g.','',txt)\n",
    "        txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "        txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "        txt = txt.strip()\n",
    "        txt = txt.lstrip()\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8baa67",
   "metadata": {},
   "source": [
    "https://github.com/ilhamksyuriadi/Language-Modeling-Bigram/blob/master/Bigram.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a69f1e",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b374694",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T02:23:44.705757Z",
     "start_time": "2023-02-13T02:23:44.685961Z"
    }
   },
   "source": [
    "model.score('is', context='language'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c58d595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-13T02:25:49.052389Z",
     "start_time": "2023-02-13T02:25:49.028076Z"
    }
   },
   "source": [
    "x=1\n",
    "for i in (bigrams(teste)):\n",
    "    print(f'P({i[0]}|{i[1]}):')\n",
    "    print(model.score(i[1], context=i[0].split()))\n",
    "    a=model.score(i[1], context=i[0].split())\n",
    "    x=x*a\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16280735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:56.019150Z",
     "start_time": "2023-02-23T00:52:56.019150Z"
    }
   },
   "outputs": [],
   "source": [
    "paper_exemplo_treino = preprocessa_paper(papers_treino[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a88cd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:56.020059Z",
     "start_time": "2023-02-23T00:52:56.020059Z"
    }
   },
   "outputs": [],
   "source": [
    "len(paper_exemplo_treino.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd879b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:56.022150Z",
     "start_time": "2023-02-23T00:52:56.022150Z"
    }
   },
   "outputs": [],
   "source": [
    "#Lista das palavras\n",
    "lista_palavras=[]\n",
    "for i in paper_exemplo_treino.split('.'):\n",
    "    lista_palavras.append(i.split(' '))\n",
    "lista_palavras\n",
    "\n",
    "#Bigrama de cada frase\n",
    "x=[]\n",
    "for frases in lista_palavras:\n",
    "    x.append(bigrams(frases))\n",
    "\n",
    "#Bigramas de cada frase\n",
    "for i in x:\n",
    "    for j in i:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b974f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:56.023054Z",
     "start_time": "2023-02-23T00:52:56.023054Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in x:\n",
    "    for j in i:\n",
    "        print(j)\n",
    "        print(model.score(j[1], context=j[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb2364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:52:56.024057Z",
     "start_time": "2023-02-23T00:52:56.024057Z"
    }
   },
   "outputs": [],
   "source": [
    "paper_exemplo_treino = preprocessa_paper(papers_treino[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "581f7c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:53:44.807520Z",
     "start_time": "2023-02-23T00:53:44.802518Z"
    }
   },
   "outputs": [],
   "source": [
    "todos_papers = list(papers_treino.values())[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba387a",
   "metadata": {},
   "source": [
    "## Bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d8b91ccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:41.716815Z",
     "start_time": "2023-02-23T01:01:41.707809Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_paper(corpus: str):\n",
    "    \"\"\"Pré-processa o paper.Não pode remover o ponto final\"\"\"\n",
    "    regIter = re.finditer('(<PAPER>(.*)<\\/PAPER>)', corpus, re.DOTALL|re.MULTILINE)\n",
    "    textos = [ t.group(2) for t in regIter]\n",
    "    lista = []\n",
    "    for texto in textos:\n",
    "        txt = re.sub('\\\\n',' ', texto)\n",
    "        txt = txt.lower()\n",
    "        txt = re.sub(r'<.*?>','', txt)\n",
    "        txt = re.sub(' +', ' ', txt)\n",
    "        txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "        txt = re.sub('e.g.','',txt)\n",
    "        txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "        txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "        txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "        txt = re.sub('i\\.e\\.', '', txt)\n",
    "        txt = re.sub('acc\\.', '', txt)\n",
    "        txt = re.sub('e\\.g\\.', '', txt)\n",
    "        txt = txt.strip()\n",
    "        txt = txt.lstrip()\n",
    "        lista.append(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ec0945e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:41.904382Z",
     "start_time": "2023-02-23T01:01:41.890261Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessa_paper2(frase: str):\n",
    "    \"\"\"Pré-processa as frases objetivo. Remove tudo inclusive o ponto final\"\"\"\n",
    "    txt = re.sub('\\\\n',' ', frase)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r'<.*?>','', txt)\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    txt = re.sub(\"\\(.*?\\)\",\"\",txt)\n",
    "    txt = re.sub('e.g.','',txt)\n",
    "    txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "    txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "    txt = re.sub('[!\"\\#\\$\\%\\&\\'()\\*\\+,:;<=>\\?@\\[\\]^_`{|}\\~]', '', txt)\n",
    "    txt = re.sub(r'[0-9]+\\.[0-9]+', '', txt)         #removendo decimal\n",
    "    txt = re.sub(r'\\d', '', txt) #removendo demais numeros\n",
    "    txt = re.sub('[^A-Za-z0-9]+', ' ', txt)\n",
    "    txt = re.sub('<s>', '', txt)\n",
    "    txt = re.sub('i\\.e.', '', txt)\n",
    "    txt = re.sub('acc\\.', '', txt)\n",
    "    txt = re.sub('e\\.g\\.', '', txt)\n",
    "    txt = re.sub('</s>', '', txt)\n",
    "    txt = txt.strip()\n",
    "    txt = txt.lstrip()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bdb75d40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:42.202104Z",
     "start_time": "2023-02-23T01:01:42.145575Z"
    }
   },
   "outputs": [],
   "source": [
    "#Pré-Processamento das frases Objetivo\n",
    "total=[]\n",
    "for paper in frase_objetivo_ft:\n",
    "    x=[]\n",
    "    for frase in paper:\n",
    "        x.append(preprocessa_paper2(frase))\n",
    "    total.append(x)\n",
    "frase_objetivo_ft_preprocessada = total.copy()\n",
    "\n",
    "\n",
    "frase_objetivo_ft_formato_ml=[]\n",
    "for i in frase_objetivo_ft_preprocessada:\n",
    "    for j in i:\n",
    "        frase_objetivo_ft_formato_ml.append(j.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "be0e0ada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:42.702031Z",
     "start_time": "2023-02-23T01:01:42.454138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 2979 items>\n"
     ]
    }
   ],
   "source": [
    "#Bigrama\n",
    "n = 2\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, frase_objetivo_ft_formato_ml)\n",
    "\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b2385d47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:42.936979Z",
     "start_time": "2023-02-23T01:01:42.888131Z"
    }
   },
   "outputs": [],
   "source": [
    "#Pre-processando os papers inteiros\n",
    "frases_papers = []\n",
    "for i in todos_papers:\n",
    "    i = preprocessa_paper(i)\n",
    "    frases_papers.append(i.split('.'))\n",
    "\n",
    "#Removendo string nula e dando strip\n",
    "frases_papers2=[]   \n",
    "for lista in frases_papers:\n",
    "    lista = [re.sub(' +', ' ',x.strip()) for x in lista if (x != '') and (x != ' ') and (x != '  ')]\n",
    "    frases_papers2.append(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0b5ebbca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:43.335947Z",
     "start_time": "2023-02-23T01:01:43.320259Z"
    }
   },
   "outputs": [],
   "source": [
    "#Criando uma lista de palavras, para cada frase, de cada paper\n",
    "lista_palavras_papers=[]\n",
    "for j in frases_papers2:\n",
    "    x=[]\n",
    "    for i in j:\n",
    "            x.append(i.lstrip().split(' '))\n",
    "    lista_palavras_papers.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "92577a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:43.773838Z",
     "start_time": "2023-02-23T01:01:43.742586Z"
    }
   },
   "outputs": [],
   "source": [
    "#Criando os bigramas da lista\n",
    "bigramas=[]\n",
    "for papers in lista_palavras_papers:\n",
    "    x=[]\n",
    "    for frases in papers:\n",
    "        x.append(bigrams(frases))\n",
    "    bigramas.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5580212d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:44.170646Z",
     "start_time": "2023-02-23T01:01:44.139074Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lista_bigramas2=[]\n",
    "for j in bigramas:\n",
    "    lista_bigramas=[]  \n",
    "    for i in j:\n",
    "        lista_bigramas.append(list(i))\n",
    "    lista_bigramas2.append(lista_bigramas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d6185174",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:44.739796Z",
     "start_time": "2023-02-23T01:01:44.527750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Probabilidade de cada frase do paper, com base nos bigramas\n",
    "lista_paper_prob=[]\n",
    "lista_paper_perp=[]\n",
    "for paper in lista_bigramas2:\n",
    "    lista_prob=[]\n",
    "    lista_perp=[]\n",
    "    for frase in paper:\n",
    "        prob=1\n",
    "        for bigrama in frase:\n",
    "            prob = prob * (model.score(bigrama[1], context=bigrama[0].split()))\n",
    "        lista_prob.append(prob)\n",
    "        try:\n",
    "            lista_perp.append(model.perplexity(frase))\n",
    "        except:\n",
    "            lista_perp.append(math.inf)\n",
    "    lista_paper_prob.append(lista_prob)\n",
    "    lista_paper_perp.append(lista_perp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dab00f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:44.891162Z",
     "start_time": "2023-02-23T01:01:44.873399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{12.25470006987771, 31.512647310163743, inf}\n",
      "{inf}\n",
      "{11.311884822457355, inf}\n",
      "{inf}\n",
      "{inf}\n"
     ]
    }
   ],
   "source": [
    "for i in lista_paper_perp:\n",
    "    print(set(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ead9f790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:46.471933Z",
     "start_time": "2023-02-23T01:01:46.450050Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_dict_probs=[]\n",
    "for paper,prob in zip(frases_papers2,lista_paper_prob):\n",
    "    res = {paper[i]: prob[i] for i in range(len(paper))}\n",
    "    lista_dict_probs.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "17fbfdbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:47.234272Z",
     "start_time": "2023-02-23T01:01:47.223695Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_key(input_dict, value):\n",
    "    return {k for k, v in input_dict.items() if v == value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5a98d487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:01:47.715525Z",
     "start_time": "2023-02-23T01:01:47.706433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAPER>\\n  <S sid=\"0\">TnT - A Statistical Part-Of-Speech Tagger</S>\\n  <ABSTRACT>\\n    <S sid=\"1\" ssid=\"1\">Trigrams\\'n\\'Tags (TnT) is an efficient statistical part-of-speech tagger.</S>\\n    <S sid=\"2\" ssid=\"2\">Contrary to claims found elsewhere in the literature, we argue that a tagger based on Markov models performs at least as well as other current approaches, including the Maximum Entropy framework.</S>\\n    <S sid=\"3\" ssid=\"3\">A recent comparison has even shown that TnT performs significantly better for the tested corpora.</S>\\n    <S sid=\"4\" ssid=\"4\">We describe the basic model of TnT, the techniques used for smoothing and for handling unknown words.</S>\\n    <S sid=\"5\" ssid=\"5\">Furthermore, we present evaluations on two corpora.</S>\\n  </ABSTRACT>\\n  <SECTION title=\"1 Introduction\" number=\"1\">\\n    <S sid=\"6\" ssid=\"1\">A large number of current language processing systems use a part-of-speech tagger for pre-processing.</S>\\n    <S sid=\"7\" ssid=\"2\">The tagger assigns a (unique or ambiguous) part-ofspeech tag to each token in the input and passes its output to the next processing level, usually a parser.</S>\\n    <S sid=\"8\" ssid=\"3\">Furthermore, there is a large interest in part-ofspeech tagging for corpus annotation projects, who create valuable linguistic resources by a combination of automatic processing and human correction.</S>\\n    <S sid=\"9\" ssid=\"4\">For both applications, a tagger with the highest possible accuracy is required.</S>\\n    <S sid=\"10\" ssid=\"5\">The debate about which paradigm solves the part-of-speech tagging problem best is not finished.</S>\\n    <S sid=\"11\" ssid=\"6\">Recent comparisons of approaches that can be trained on corpora (van Halteren et al., 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite-state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al., 1996).</S>\\n    <S sid=\"12\" ssid=\"7\">They are only surpassed by combinations of different systems, forming a &amp;quot;voting tagger&amp;quot;.</S>\\n    <S sid=\"13\" ssid=\"8\">Among the statistical approaches, the Maximum Entropy framework has a very strong position.</S>\\n    <S sid=\"14\" ssid=\"9\">Nevertheless, a recent independent comparison of 7 taggers (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words.</S>\\n    <S sid=\"15\" ssid=\"10\">This tagger, TnT, not only yielded the highest accuracy, it also was the fastest both in training and tagging.</S>\\n    <S sid=\"16\" ssid=\"11\">The tagger comparison was organized as a &amp;quot;blackbox test&amp;quot;: set the same task to every tagger and compare the outcomes.</S>\\n    <S sid=\"17\" ssid=\"12\">This paper describes the models and techniques used by TnT together with the implementation.</S>\\n    <S sid=\"18\" ssid=\"13\">The reader will be surprised how simple the underlying model is.</S>\\n    <S sid=\"19\" ssid=\"14\">The result of the tagger comparison seems to support the maxime &amp;quot;the simplest is the best&amp;quot;.</S>\\n    <S sid=\"20\" ssid=\"15\">However, in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models.</S>\\n    <S sid=\"21\" ssid=\"16\">As two examples, (Rabiner, 1989) and (Charniak et al., 1993) give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application.</S>\\n    <S sid=\"22\" ssid=\"17\">We argue that it is not only the choice of the general model that determines the result of the tagger but also the various &amp;quot;small&amp;quot; decisions on alternatives.</S>\\n    <S sid=\"23\" ssid=\"18\">The aim of this paper is to give a detailed account of the techniques used in TnT.</S>\\n    <S sid=\"24\" ssid=\"19\">Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993).</S>\\n    <S sid=\"25\" ssid=\"20\">The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996).</S>\\n    <S sid=\"26\" ssid=\"21\">For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999).</S>\\n  </SECTION>\\n  <SECTION title=\"2 Architecture\" number=\"2\">\\n    <S sid=\"27\" ssid=\"1\">TnT uses second order Markov models for part-ofspeech tagging.</S>\\n    <S sid=\"28\" ssid=\"2\">The states of the model represent tags, outputs represent the words.</S>\\n    <S sid=\"29\" ssid=\"3\">Transition probabilities depend on the states, thus pairs of tags.</S>\\n    <S sid=\"30\" ssid=\"4\">Output probabilities only depend on the most recent category.</S>\\n    <S sid=\"31\" ssid=\"5\">To be explicit, we calculate for a given sequence of words w1 of length T. t1 tr are elements of the tagset, the additional tags t_1, to, and t7-,&#177;1 are beginning-of-sequence and end-of-sequence markers.</S>\\n    <S sid=\"32\" ssid=\"6\">Using these additional tags, even if they stem from rudimentary processing of punctuation marks, slightly improves tagging results.</S>\\n    <S sid=\"33\" ssid=\"7\">This is different from formulas presented in other publications, which just stop with a &amp;quot;loose end&amp;quot; at the last word.</S>\\n    <S sid=\"34\" ssid=\"8\">If sentence boundaries are not marked in the input, TnT adds these tags if it encounters one of [.!?</S>\\n    <S sid=\"35\" ssid=\"9\">;] as a token.</S>\\n    <S sid=\"36\" ssid=\"10\">Transition and output probabilities are estimated from a tagged corpus.</S>\\n    <S sid=\"37\" ssid=\"11\">As a first step, we use the maximum likelihood probabilities P which are derived from the relative frequencies: for all t1, t2, t3 in the tagset and w3 in the lexicon.</S>\\n    <S sid=\"38\" ssid=\"12\">N is the total number of tokens in the training corpus.</S>\\n    <S sid=\"39\" ssid=\"13\">We define a maximum likelihood probability to be zero if the corresponding nominators and denominators are zero.</S>\\n    <S sid=\"40\" ssid=\"14\">As a second step, contextual frequencies are smoothed and lexical frequences are completed by handling words that are not in the lexicon (see below).</S>\\n    <S sid=\"41\" ssid=\"15\">Trigram probabilities generated from a corpus usually cannot directly be used because of the sparsedata problem.</S>\\n    <S sid=\"42\" ssid=\"16\">This means that there are not enough instances for each trigram to reliably estimate the probability.</S>\\n    <S sid=\"43\" ssid=\"17\">Furthermore, setting a probability to zero because the corresponding trigram never occured in the corpus has an undesired effect.</S>\\n    <S sid=\"44\" ssid=\"18\">It causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence, thus makes it impossible to rank different sequences containing a zero probability.</S>\\n    <S sid=\"45\" ssid=\"19\">The smoothing paradigm that delivers the best results in TnT is linear interpolation of unigrams, bigrams, and trigrams.</S>\\n    <S sid=\"46\" ssid=\"20\">Therefore, we estimate a trigram probability as follows: P are maximum likelihood estimates of the probabilities, and A1 + A2 &#177; A3 = 1, SO P again represent probability distributions.</S>\\n    <S sid=\"47\" ssid=\"21\">We use the context-independent variant of linear interpolation, i.e., the values of the As do not depend on the particular trigram.</S>\\n    <S sid=\"48\" ssid=\"22\">Contrary to intuition, this yields better results than the context-dependent variant.</S>\\n    <S sid=\"49\" ssid=\"23\">Due to sparse-data problems, one cannot estimate a different set of As for each trigram.</S>\\n    <S sid=\"50\" ssid=\"24\">Therefore, it is common practice to group trigrams by frequency and estimate tied sets of As.</S>\\n    <S sid=\"51\" ssid=\"25\">However, we are not aware of any publication that has investigated frequency groupings for linear interpolation in part-of-speech tagging.</S>\\n    <S sid=\"52\" ssid=\"26\">All groupings that we have tested yielded at most equivalent results to contextindependent linear interpolation.</S>\\n    <S sid=\"53\" ssid=\"27\">Some groupings even yielded worse results.</S>\\n    <S sid=\"54\" ssid=\"28\">The tested groupings included a) one set of As for each frequency value and b) two classes (low and high frequency) on the two ends of the scale, as well as several groupings in between and several settings for partitioning the classes.</S>\\n    <S sid=\"55\" ssid=\"29\">The values of A1, A2, and A3 are estimated by deleted interpolation.</S>\\n    <S sid=\"56\" ssid=\"30\">This technique successively removes each trigram from the training corpus and estimates best values for the As from all other ngrams in the corpus.</S>\\n    <S sid=\"57\" ssid=\"31\">Given the frequency counts for uni-, bi-, and trigrams, the weights can be very efficiently determined with a processing time linear in the number of different trigrams.</S>\\n    <S sid=\"58\" ssid=\"32\">The algorithm is given in figure 1.</S>\\n    <S sid=\"59\" ssid=\"33\">Note that subtracting 1 means taking unseen data into account.</S>\\n    <S sid=\"60\" ssid=\"34\">Without this subtraction the model would overfit the training data and would generally yield worse results.</S>\\n    <S sid=\"61\" ssid=\"35\">Currently, the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993).</S>\\n    <S sid=\"62\" ssid=\"36\">Tag probabilities are set according to the word\\'s ending.</S>\\n    <S sid=\"63\" ssid=\"37\">The suffix is a strong predictor for word classes, e.g., words in the Wall Street Journal part of the Penn Treebank ending in able are adjectives (.11) in 98% of the cases (e.g. fashionable, variable) , the rest of 2% are nouns (e.g. cable, variable).</S>\\n    <S sid=\"64\" ssid=\"38\">The probability distribution for a particular suffix is generated from all words in the training set that share the same suffix of some predefined maximum length.</S>\\n    <S sid=\"65\" ssid=\"39\">The term suffix as used here means &amp;quot;final sequence of characters of a word&amp;quot; which is not necessarily a linguistically meaningful suffix.</S>\\n    <S sid=\"66\" ssid=\"40\">Probabilities are smoothed by successive abstraction.</S>\\n    <S sid=\"67\" ssid=\"41\">This calculates the probability of a tag t given the last m letters i of an n letter word: P(t1/7&#8222;,+1,,..ln).</S>\\n    <S sid=\"68\" ssid=\"42\">The sequence of increasingly more general contexts omits more and more characters of the suffix, such that P(tlin-m+2, &#8226; &#8226; &#8226; P(tlin_m+3, ,i), , P(t) are used for smoothing.</S>\\n    <S sid=\"69\" ssid=\"43\">The recursion formula is set A = A2 = A3 = 0 foreach trigram t1,t2,t3 with f (ti,t2,t3) &gt;0 depending on the maximum of the following three values: for i = m ... 0, using the maximum likelihood estimates P from frequencies in the lexicon, weights Oi and the initialization For the Markov model, we need the inverse conditional probabilities P(1,2_1+1, ... /Tilt) which are obtained by Bayesian inversion.</S>\\n    <S sid=\"70\" ssid=\"44\">A theoretical motivated argumentation uses the standard deviation of the maximum likelihood probabilities for the weights 0, (Samuelsson, 1993).</S>\\n    <S sid=\"71\" ssid=\"45\">This leaves room for interpretation.</S>\\n    <S sid=\"72\" ssid=\"46\">We use the longest suffix that we can find in the training set (i.e., for which the frequency is greater than or equal to 1), but at most 10 characters.</S>\\n    <S sid=\"73\" ssid=\"47\">This is an empirically determined choice.</S>\\n    <S sid=\"74\" ssid=\"48\">2) We use a context-independent approach for 0&#8222; as we did for the contextual weights A.</S>\\n    <S sid=\"75\" ssid=\"49\">It turned out to be a good choice to set all 0, to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus, i.e., we set for all i = 0 ... m &#8212; 1, using a tagset of s tags and the average (11) This usually yields values in the range 0,03 ... 0.10.</S>\\n    <S sid=\"76\" ssid=\"50\">3) We use different estimates for uppercase and lowercase words, i.e., we maintain two different suffix tries depending on the capitalization of the word.</S>\\n    <S sid=\"77\" ssid=\"51\">This information improves the tagging results.</S>\\n    <S sid=\"78\" ssid=\"52\">4) Another freedom concerns the choice of the words in the lexicon that should be used for suffix handling.</S>\\n    <S sid=\"79\" ssid=\"53\">Should we use all words, or are some of them better suited than others?</S>\\n    <S sid=\"80\" ssid=\"54\">Accepting that unknown words are most probably infrequent, one can argue that using suffixes of infrequent words in the lexicon is a better approximation for unknown words than using suffixes of frequent words.</S>\\n    <S sid=\"81\" ssid=\"55\">Therefore, we restrict the procedure of suffix handling to words with a frequency smaller than or equal to some threshold value.</S>\\n    <S sid=\"82\" ssid=\"56\">Empirically, 10 turned out to be a good choice for this threshold.</S>\\n    <S sid=\"83\" ssid=\"57\">Additional information that turned out to be useful for the disambiguation process for several corpora and tagsets is capitalization information.</S>\\n    <S sid=\"84\" ssid=\"58\">Tags are usually not informative about capitalization, but probability distributions of tags around capitalized words are different from those not capitalized.</S>\\n    <S sid=\"85\" ssid=\"59\">The effect is larger for English, which only capitalizes proper names, and smaller for German, which capitalizes all nouns.</S>\\n    <S sid=\"86\" ssid=\"60\">We use flags ci that are true if wi is a capitalized word and false otherwise.</S>\\n    <S sid=\"87\" ssid=\"61\">These flags are added to the contextual probability distributions.</S>\\n    <S sid=\"88\" ssid=\"62\">Instead of and equations (3) to (5) are updated accordingly.</S>\\n    <S sid=\"89\" ssid=\"63\">This is equivalent to doubling the size of the tagset and using different tags depending on capitalization.</S>\\n    <S sid=\"90\" ssid=\"64\">The processing time of the Viterbi algorithm (Rabiner, 1989) can be reduced by introducing a beam search.</S>\\n    <S sid=\"91\" ssid=\"65\">Each state that receives a 6 value smaller than the largest 6 divided by some threshold value 0 is excluded from further processing.</S>\\n    <S sid=\"92\" ssid=\"66\">While the Viterbi algorithm is guaranteed to find the sequence of states with the highest probability, this is no longer true when beam search is added.</S>\\n    <S sid=\"93\" ssid=\"67\">Nevertheless, for practical purposes and the right choice of 0, there is virtually no difference between the algorithm with and without a beam.</S>\\n    <S sid=\"94\" ssid=\"68\">Empirically, a value of 0 = 1000 turned out to approximately double the speed of the tagger without affecting the accuracy.</S>\\n    <S sid=\"95\" ssid=\"69\">The tagger currently tags between 30,000 and 60,000 tokens per second (including file I/O) on a Pentium 500 running Linux.</S>\\n    <S sid=\"96\" ssid=\"70\">The speed mainly depends on the percentage of unknown words and on the average ambiguity rate.</S>\\n  </SECTION>\\n  <SECTION title=\"3 Evaluation\" number=\"3\">\\n    <S sid=\"97\" ssid=\"1\">We evaluate the tagger\\'s performance under several aspects.</S>\\n    <S sid=\"98\" ssid=\"2\">First of all, we determine the tagging accuracy averaged over ten iterations.</S>\\n    <S sid=\"99\" ssid=\"3\">The overall accuracy, as well as separate accuracies for known and unknown words are measured.</S>\\n    <S sid=\"100\" ssid=\"4\">Second, learning curves are presented, that indicate the performance when using training corpora of different sizes, starting with as few as 1,000 tokens and ranging to the size of the entire corpus (minus the test set).</S>\\n    <S sid=\"101\" ssid=\"5\">An important characteristic of statistical taggers is that they not only assign tags to words but also probabilities in order to rank different assignments.</S>\\n    <S sid=\"102\" ssid=\"6\">We distinguish reliable from unreliable assignments by the quotient of the best and second best assignmentsl .</S>\\n    <S sid=\"103\" ssid=\"7\">All assignments for which this quotient is larger than some threshold are regarded as reliable, the others as unreliable.</S>\\n    <S sid=\"104\" ssid=\"8\">As we will see below, accuracies for reliable assignments are much higher.</S>\\n    <S sid=\"105\" ssid=\"9\">The tests are performed on partitions of the corpora that use 90% as training set and 10% as test set, so that the test data is guaranteed to be unseen during training.</S>\\n    <S sid=\"106\" ssid=\"10\">Each result is obtained by repeating the experiment 10 times with different partitions and averaging the single outcomes.</S>\\n    <S sid=\"107\" ssid=\"11\">In all experiments, contiguous test sets are used.</S>\\n    <S sid=\"108\" ssid=\"12\">The alternative is a round-robin procedure that puts every 10th sentence into the test set.</S>\\n    <S sid=\"109\" ssid=\"13\">We argue that contiguous test sets yield more realistic results because completely unseen articles are tagged.</S>\\n    <S sid=\"110\" ssid=\"14\">Using the round-robin procedure, parts of an article are already seen, which significantly reduces the percentage of unknown words.</S>\\n    <S sid=\"111\" ssid=\"15\">Therefore, we expect even \\'By definition, this quotient is oo if there is only one possible tag for a given word. higher results when testing on every 10th sentence instead of a contiguous set of 10%.</S>\\n    <S sid=\"112\" ssid=\"16\">In the following, accuracy denotes the number of correctly assigned tags divided by the number of tokens in the corpus processed.</S>\\n    <S sid=\"113\" ssid=\"17\">The tagger is allowed to assign exactly one tag to each token.</S>\\n    <S sid=\"114\" ssid=\"18\">We distinguish the overall accuracy, taking into account all tokens in the test corpus, and separate accuracies for known and unknown tokens.</S>\\n    <S sid=\"115\" ssid=\"19\">The latter are interesting, since usually unknown tokens are much more difficult to process than known tokens, for which a list of valid tags can be found in the lexicon.</S>\\n    <S sid=\"116\" ssid=\"20\">The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frankfurter Rundschau) that are annotated with parts-ofspeech and predicate-argument structures (Skut et al., 1997).</S>\\n    <S sid=\"117\" ssid=\"21\">It was developed at the Saarland University in Saarbriicken2.</S>\\n    <S sid=\"118\" ssid=\"22\">Part of it was tagged at the IMS Stuttgart.</S>\\n    <S sid=\"119\" ssid=\"23\">This evaluation only uses the partof-speech annotation and ignores structural annotations.</S>\\n    <S sid=\"120\" ssid=\"24\">Tagging accuracies for the NEGRA corpus are shown in table 2.</S>\\n    <S sid=\"121\" ssid=\"25\">Figure 3 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data.</S>\\n    <S sid=\"122\" ssid=\"26\">Training length is the number of tokens used for training.</S>\\n    <S sid=\"123\" ssid=\"27\">Each training length was tested ten times, training and test sets were randomly chosen and disjoint, results were averaged.</S>\\n    <S sid=\"124\" ssid=\"28\">The training length is given on a logarithmic scale.</S>\\n    <S sid=\"125\" ssid=\"29\">It is remarkable that tagging accuracy for known words is very high even for very small training corpora.</S>\\n    <S sid=\"126\" ssid=\"30\">This means that we have a good chance of getting the right tag if a word is seen at least once during training.</S>\\n    <S sid=\"127\" ssid=\"31\">Average percentages of unknown tokens are shown in the bottom line of each diagram.</S>\\n    <S sid=\"128\" ssid=\"32\">We exploit the fact that the tagger not only determines tags, but also assigns probabilities.</S>\\n    <S sid=\"129\" ssid=\"33\">If there is an alternative that has a probability &amp;quot;close to&amp;quot; that of the best assignment, this alternative can be viewed as almost equally well suited.</S>\\n    <S sid=\"130\" ssid=\"34\">The notion of &amp;quot;close to&amp;quot; is expressed by the distance of probabilities, and this in turn is expressed by the quotient of probabilities.</S>\\n    <S sid=\"131\" ssid=\"35\">So, the distance of the probabilities of a best tag tbest and an alternative tag tau is expressed by p(tbest)/P(talt)7 which is some value greater or equal to 1 since the best tag assignment has the highest probability.</S>\\n    <S sid=\"132\" ssid=\"36\">Figure 4 shows the accuracy when separating assignments with quotients larger and smaller than the threshold (hence reliable and unreliable assignments).</S>\\n    <S sid=\"133\" ssid=\"37\">As expected, we find that accuracies for percentage known unknown &#8226; overall unknowns acc. acc. acc. a Table 5: Part-of-speech tagging accuracy for the Penn Treebank.</S>\\n    <S sid=\"134\" ssid=\"38\">The table shows the percentage of unknown tokens, separate accuracies and standard deviations for known and unknown tokens, as well as the overall accuracy. percentage known unknown overall unknowns acc. acc. acc. reliable assignments are much higher than for unreliable assignments.</S>\\n    <S sid=\"135\" ssid=\"39\">This distinction is, e.g., useful for annotation projects during the cleaning process, or during pre-processing, so the tagger can emit multiple tags if the best tag is classified as unreliable.</S>\\n    <S sid=\"136\" ssid=\"40\">We use the Wall Street Journal as contained in the Penn Treebank for our experiments.</S>\\n    <S sid=\"137\" ssid=\"41\">The annotation consists of four parts: 1) a context-free structure augmented with traces to mark movement and discontinuous constituents, 2) phrasal categories that are annotated as node labels, 3) a small set of grammatical functions that are annotated as extensions to the node labels, and 4) part-of-speech tags (Marcus et al., 1993).</S>\\n    <S sid=\"138\" ssid=\"42\">This evaluation only uses the part-ofspeech annotation.</S>\\n    <S sid=\"139\" ssid=\"43\">The Wall Street Journal part of the Penn Treebank consists of approx.</S>\\n    <S sid=\"140\" ssid=\"44\">50,000 sentences (1.2 million tokens).</S>\\n    <S sid=\"141\" ssid=\"45\">Tagging accuracies for the Penn Treebank are shown in table 5.</S>\\n    <S sid=\"142\" ssid=\"46\">Figure 6 shows the learning curve of the tagger, i.e., the accuracy depending on the amount of training data.</S>\\n    <S sid=\"143\" ssid=\"47\">Training length is the number of tokens used for training.</S>\\n    <S sid=\"144\" ssid=\"48\">Each training length was tested ten times.</S>\\n    <S sid=\"145\" ssid=\"49\">Training and test sets were disjoint, results are averaged.</S>\\n    <S sid=\"146\" ssid=\"50\">The training length is given on a logarithmic scale.</S>\\n    <S sid=\"147\" ssid=\"51\">As for the NEGRA corpus, tagging accuracy is very high for known tokens even with small amounts of training data.</S>\\n    <S sid=\"148\" ssid=\"52\">We exploit the fact that the tagger not only determines tags, but also assigns probabilities.</S>\\n    <S sid=\"149\" ssid=\"53\">Figure 7 shows the accuracy when separating assignments with quotients larger and smaller than the threshold (hence reliable and unreliable assignments).</S>\\n    <S sid=\"150\" ssid=\"54\">Again, we find that accuracies for reliable assignments are much higher than for unreliable assignments.</S>\\n    <S sid=\"151\" ssid=\"55\">Average part-of-speech tagging accuracy is between 96% and 97%, depending on language and tagset, which is at least on a par with state-of-the-art results found in the literature, possibly better.</S>\\n    <S sid=\"152\" ssid=\"56\">For the Penn Treebank, (Ratnaparkhi, 1996) reports an accuracy of 96.6% using the Maximum Entropy approach, our much simpler and therefore faster HMM approach delivers 96.7%.</S>\\n    <S sid=\"153\" ssid=\"57\">This comparison needs to be re-examined, since we use a ten-fold crossvalidation and averaging of results while Ratnaparkhi only makes one test run.</S>\\n    <S sid=\"154\" ssid=\"58\">The accuracy for known tokens is significantly higher than for unknown tokens.</S>\\n    <S sid=\"155\" ssid=\"59\">For the German newspaper data, results are 8.7% better when the word was seen before and therefore is in the lexicon, than when it was not seen before (97.7% vs. 89.0%).</S>\\n    <S sid=\"156\" ssid=\"60\">Accuracy for known tokens is high even with very small amounts of training data.</S>\\n    <S sid=\"157\" ssid=\"61\">As few as 1000 tokens are sufficient to achieve 95%-96% accuracy for them.</S>\\n    <S sid=\"158\" ssid=\"62\">It is important for the tagger to have seen a word at least once during training.</S>\\n    <S sid=\"159\" ssid=\"63\">Stochastic taggers assign probabilities to tags.</S>\\n    <S sid=\"160\" ssid=\"64\">We exploit the probabilities to determine reliability of assignments.</S>\\n    <S sid=\"161\" ssid=\"65\">For a subset that is determined during processing by the tagger we achieve accuracy rates of over 99%.</S>\\n    <S sid=\"162\" ssid=\"66\">The accuracy of the complement set is much lower.</S>\\n    <S sid=\"163\" ssid=\"67\">This information can, e.g., be exploited in an annotation project to give an additional treatment to the unreliable assignments, or to pass selected ambiguities to a subsequent processing step.</S>\\n  </SECTION>\\n  <SECTION title=\"4 Conclusion\" number=\"4\">\\n    <S sid=\"164\" ssid=\"1\">We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature.</S>\\n    <S sid=\"165\" ssid=\"2\">For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers.</S>\\n    <S sid=\"166\" ssid=\"3\">In our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor.</S>\\n    <S sid=\"167\" ssid=\"4\">The rather large amount of freedom was not handled in detail in previous publications: handling of start- and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words.</S>\\n    <S sid=\"168\" ssid=\"5\">Note that the decisions we made yield good results for both the German and the English Corpus.</S>\\n    <S sid=\"169\" ssid=\"6\">They do so for several other corpora as well.</S>\\n    <S sid=\"170\" ssid=\"7\">The architecture remains applicable to a large variety of languages.</S>\\n    <S sid=\"171\" ssid=\"8\">According to current tagger comparisons (van Halteren et al., 1998; Zavrel and Daelemans, 1999), and according to a comparsion of the results presented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here.</S>\\n    <S sid=\"172\" ssid=\"9\">It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both.</S>\\n    <S sid=\"173\" ssid=\"10\">TnT is freely available to universities and related organizations for research purposes (see http://www.coli.uni-sb.derthorstenAnt).</S>\\n  </SECTION>\\n  <SECTION title=\"Acknowledgements\" number=\"5\">\\n    <S sid=\"174\" ssid=\"1\">Many thanks go to Hans Uszkoreit for his support during the development of TnT.</S>\\n    <S sid=\"175\" ssid=\"2\">Most of the work on TnT was carried out while the author received a grant of the Deutsche Forschungsgemeinschaft in the Graduiertenkolleg Kognitionswissenschaft Saarbriicken.</S>\\n    <S sid=\"176\" ssid=\"3\">Large annotated corpora are the pre-requisite for developing and testing part-ofspeech taggers, and they enable the generation of high-quality language models.</S>\\n    <S sid=\"177\" ssid=\"4\">Therefore, I would like to thank all the people who took the effort to annotate the Penn Treebank, the Susanne Corpus, the Stuttgarter Referenzkorpus, the NEGRA Corpus, the Verbmobil Corpora, and several others.</S>\\n    <S sid=\"178\" ssid=\"5\">And, last but not least, I would like to thank the users of TnT who provided me with bug reports and valuable suggestions for improvements.</S>\\n  </SECTION>\\n</PAPER>\\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todos_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "986a493b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:54:02.709285Z",
     "start_time": "2023-02-23T00:54:02.693658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tnt - a statistical part-of-speech tagger trigramsntags is an efficient statistical part-of-speech tagger',\n",
       " 'contrary to claims found elsewhere in the literature we argue that a tagger based on markov models performs at least as well as other current approaches including the maximum entropy framework',\n",
       " 'a recent comparison has even shown that tnt performs significantly better for the tested corpora',\n",
       " 'we describe the basic model of tnt the techniques used for smoothing and for handling unknown words',\n",
       " 'furthermore we present evaluations on two corpora',\n",
       " 'a large number of current language processing systems use a part-of-speech tagger for pre-processing',\n",
       " 'the tagger assigns a part-ofspeech tag to each token in the input and passes its output to the next processing level usually a parser',\n",
       " 'furthermore there is a large interest in part-ofspeech tagging for corpus annotation projects who create valuable linguistic resources by a combination of automatic processing and human correction',\n",
       " 'for both applications a tagger with the highest possible accuracy is required',\n",
       " 'the debate about which paradigm solves the part-of-speech tagging problem best is not finished',\n",
       " 'recent comparisons of approaches that can be trained on corpora have shown that in most cases statistical aproaches yield better results than finite-state rule-based or memory-based taggers',\n",
       " 'they are only surpassed by combinations of different systems forming a ampquotvoting taggerampquot',\n",
       " 'among the statistical approaches the maximum entropy framework has a very strong position',\n",
       " 'nevertheless a recent independent comparison of taggers has shown that another approach even works better markov models combined with a good smoothing technique and with handling of unknown words',\n",
       " 'this tagger tnt not only yielded the highest accuracy it also was the fastest both in training and tagging',\n",
       " 'the tagger comparison was organized as a ampquotblackbox testampquot set the same task to every tagger and compare the outcomes',\n",
       " 'this paper describes the models and techniques used by tnt together with the implementation',\n",
       " 'the reader will be surprised how simple the underlying model is',\n",
       " 'the result of the tagger comparison seems to support the maxime ampquotthe simplest is the bestampquot',\n",
       " 'however in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with markov models',\n",
       " 'as two examples and givod overviews of the techniques and equations used for markov models and part-ofspeech tagging but they are not very explicit in the details that are needed for their application',\n",
       " 'we argue that it is not only the choice of thneral model that determines the result of the tagger but also the various ampquotsmallampquot decisions on alternatives',\n",
       " 'the aim of this paper is to give a detailed account of the techniques used in tnt',\n",
       " 'additionally we present results of the tagger on the negra corpus and the penn treebank',\n",
       " 'the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in',\n",
       " 'for a comparison to other taggers the reader is referred to',\n",
       " 'tnt uses second order markov models for part-ofspeech tagging',\n",
       " 'the states of the model represent tags outputs represent the words',\n",
       " 'transition probabilities depend on the states thus pairs of tags',\n",
       " 'output probabilities only depend on the most recent category',\n",
       " 'to be explicit we calculate for a given sequence of words w of lh t',\n",
       " 't tr are elements of the tagset the additional tags t to and t- are beginning-of-sequence and end-of-sequence markers',\n",
       " 'using these additional tags even if they stem from rudimentary processing of punctuation marks slightly improves tagging results',\n",
       " 'this is different from formulas presented in other publications which just stop with a ampquotloose endampquot at the last word',\n",
       " 'if sentence boundaries are not marked in the input tnt adds these tags if it encounters one of',\n",
       " 'as a token',\n",
       " 'transition and output probabilities are estimated from a tagged corpus',\n",
       " 'as a first step we use the maximum likelihood probabilities p which are derived from the relative frequencies for all t t t in the tagset and w in the lexicon',\n",
       " 'n is the total number of tokens in the training corpus',\n",
       " 'we define a maximum likelihood probability to be zero if the corresponding nominators and denominators are zero',\n",
       " 'as a second step contextual frequencies are smoothed and lexical frequences are completed by handling words that are not in the lexicon',\n",
       " 'trigram probabilities generated from a corpus usually cannot directly be used because of the sparsedata problem',\n",
       " 'this means that there are not enough instances for each trigram to reliably estimate the probability',\n",
       " 'furthermore setting a probability to zero because the corresponding trigram never occured in the corpus has an undesired effect',\n",
       " 'it causes the probability of a complete sequence to be set to zero if its use is necessary for a new text sequence thus makes it impossible to rank different sequences containing a zero probability',\n",
       " 'the smoothing paradigm that delivers the best results in tnt is linear interpolation of unigrams bigrams and trigrams',\n",
       " 'therefore we estimate a trigram probability as follows p are maximum likelihood estimates of the probabilities and a a a so p again represent probability distributions',\n",
       " 'we use the context-independent variant of linear interpolation the values of the as do not depend on the particular trigram',\n",
       " 'contrary to intuition this yields better results than the context-dependent variant',\n",
       " 'due to sparse-data problems one cannot estimate a different set of as for each trigram',\n",
       " 'therefore it is common practice to group trigrams by frequency and estimate tied sets of as',\n",
       " 'however we are not aware of any publication that has investigated frequency groupings for linear interpolation in part-of-speech tagging',\n",
       " 'all groupings that we have tested yielded at most equivalent results to contextindependent linear interpolation',\n",
       " 'somoupings even yielded worse results',\n",
       " 'the tested groupings included a one set of as for each frequency value and b two classes on the two ends of the scale as well as several groupings in between and several settings for partitioning the classes',\n",
       " 'the values of a a and a are estimated by deleted interpolation',\n",
       " 'this technique successively removes each trigram from the training corpus and estimates best values for the as from all other ngrams in the corpus',\n",
       " 'given the frequency counts for uni- bi- and trigrams the wts can be very efficiently determined with a processing time linear in the number of different trigrams',\n",
       " 'the algorithm is given in figure',\n",
       " 'note that subtracting means taking unseen data into account',\n",
       " 'without this subtraction the model would overfit the training data and would generally yield worse results',\n",
       " 'currently the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in',\n",
       " 'tag probabilities are set according to the words ending',\n",
       " 'the suffix is a strong predictor for word classes words in the wall street journal part of the penn treebank ending in able are adjectives in of the cases the rest of are nouns',\n",
       " 'the probability distribution for a particular suffix is generated from all words in the training set that share the same suffix of some predefined maximum lh',\n",
       " 'the term suffix as used here means ampquotfinal sequence of characters of a wordampquot which is not necessarily a linguistically meaningful suffix',\n",
       " 'probabilities are smoothed by successive abstraction',\n",
       " 'this calculates the probability of a tag t given the last m letters i of an n letter word p',\n",
       " 'the sequence of increasingly morneral contexts omits more and more characters of the suffix such that p p are used for smoothing',\n",
       " 'the recursion formula is set a a a foreach trigram ttt with f gt depending on the maximum of the following three values for i m',\n",
       " 'using the maximum likelihood estimates p from frequencies in the lexicon wts oi and the initialization for the markov model we need the inverse conditional probabilities p which are obtained by bayesian inversion',\n",
       " 'a theoretical motivated argumentation uses the standard deviation of the maximum likelihood probabilities for the wts',\n",
       " 'this leaves room for interpretation',\n",
       " 'we use the longest suffix that we can find in the training set but at most characters',\n",
       " 'this is an empirically determined choice',\n",
       " 'we use a context-independent approach for as we did for the contextual wts a',\n",
       " 'it turned out to be a good choice to set all to the standard deviation of the unconditioned maximum likelihood probabilities of the tags in the training corpus we set for all i',\n",
       " 'm using a tagset of s tags and the average this usually yields values in the range',\n",
       " 'we use different estimates for uppercase and lowercase words we maintain two different suffix tries depending on the capitalization of the word',\n",
       " 'this information improves the tagging results',\n",
       " 'another freedom concerns the choice of the words in the lexicon that should be used for suffix handling',\n",
       " 'should we use all words or are some of them better suited than others accepting that unknown words are most probably infrequent one can argue that using suffixes of infrequent words in the lexicon is a better approximation for unknown words than using suffixes of frequent words',\n",
       " 'therefore we restrict the procedure of suffix handling to words with a frequency smaller than or equal to some threshold value',\n",
       " 'empirically turned out to be a good choice for this threshold',\n",
       " 'additional information that turned out to be useful for the disambiguation process for several corpora and tagsets is capitalization information',\n",
       " 'tags are usually not informative about capitalization but probability distributions of tags around capitalized words are different from those not capitalized',\n",
       " 'the effect is larger for ish which only capitalizes proper names and smaller for german which capitalizes all nouns',\n",
       " 'we use flags ci that are true if wi is a capitalized word and false otherwise',\n",
       " 'these flags are added to the contextual probability distributions',\n",
       " 'instead of and equations to are updated accordingly',\n",
       " 'this is equivalent to doubling the size of the tagset and using different tags depending on capitalization',\n",
       " 'the processing time of the viterbi algorithm can be reduced by introducing a beam search',\n",
       " 'each state that receives a value smaller than the largest divided by some threshold value is excluded from further processing',\n",
       " 'while the viterbi algorithm is guaranteed to find the sequence of states with the highest probability this is no longer true when beam search is added',\n",
       " 'nevertheless for practical purposes and the right choice of there is virtually no difference between the algorithm with and without a beam',\n",
       " 'empirically a value of turned out to approximately double the speed of the tagger without affecting the accuracy',\n",
       " 'the tagger currently tags between and tokens per second on a pentium running linux',\n",
       " 'the speed mainly depends on the percentage of unknown words and on the average ambiguity rate',\n",
       " 'we evaluate the taggers performance under several aspects',\n",
       " 'first of all we determine the tagging accuracy averaged over ten iterations',\n",
       " 'the overall accuracy as well as separate accuracies for known and unknown words are measured',\n",
       " 'second learning curves are presented that indicate the performance when using training corpora of different sizes starting with as few as tokens and ranging to the size of the entire corpus',\n",
       " 'an important characteristic of statistical taggers is that they not only assign tags to words but also probabilities in order to rank different assignments',\n",
       " 'we distinguish reliable from unreliable assignments by the quotient of the best and second best assignmentsl',\n",
       " 'all assignments for which this quotient is larger than some threshold are regarded as reliable the others as unreliable',\n",
       " 'as we will see below accuracies for reliable assignments are much higher',\n",
       " 'the tests are performed on partitions of the corpora that use as training set and as test set so that the test data is guaranteed to be unseen during training',\n",
       " 'each result is obtained by repeating the experiment times with different partitions and averaging the single outcomes',\n",
       " 'in all experiments contiguous test sets are used',\n",
       " 'the alternative is a round-robin procedure that puts every th sentence into the test set',\n",
       " 'we argue that contiguous test sets yield more realistic results because completely unseen articles are tagged',\n",
       " 'using the round-robin procedure parts of an article are already seen which significantly reduces the percentage of unknown words',\n",
       " 'therefore we expect even by definition this quotient is oo if there is only one possible tag for a given word',\n",
       " 'higher results when testing on every th sentence instead of a contiguous set of',\n",
       " 'in the following accuracy denotes the number of correctly assigned tags divided by the number of tokens in the corpus processed',\n",
       " 'the tagger is allowed to assign exactly one tag to each token',\n",
       " 'we distinguish the overall accuracy taking into account all tokens in the test corpus and separate accuracies for known and unknown tokens',\n",
       " 'the latter are interesting since usually unknown tokens are much more difficult to process than known tokens for which a list of valid tags can be found in the lexicon',\n",
       " 'thrman negra corpus consists of sentences of newspaper texts that are annotated with parts-ofspeech and predicate-argument structures',\n",
       " 'it was developed at the saarland university in saarbriicken',\n",
       " 'part of it was tagged at the ims stuttgart',\n",
       " 'this evaluation only uses the partof-speech annotation and ignores structural annotations',\n",
       " 'tagging accuracies for the negra corpus are shown in table',\n",
       " 'figure shows the learning curve of the tagger the accuracy depending on the amount of training data',\n",
       " 'training lh is the number of tokens used for training',\n",
       " 'each training lh was tested ten times training and test sets were randomly chosen and disjoint results were averaged',\n",
       " 'the training lh is given on a logarithmic scale',\n",
       " 'it is remarkable that tagging accuracy for known words is very high even for very small training corpora',\n",
       " 'this means that we have a good chance of getting the right tag if a word is seen at least once during training',\n",
       " 'average percentages of unknown tokens are shown in the bottom line of each diagram',\n",
       " 'we exploit the fact that the tagger not only determines tags but also assigns probabilities',\n",
       " 'if there is an alternative that has a probability ampquotclose toampquot that of the best assignment this alternative can be viewed as almost equally well suited',\n",
       " 'the notion of ampquotclose toampquot is expressed by the distance of probabilities and this in turn is expressed by the quotient of probabilities',\n",
       " 'so the distance of the probabilities of a best tag tbest and an alternative tag tau is expressed by p/p which is some valueater or equal to since the best tag assignment has the highest probability',\n",
       " 'figure shows the accuracy when separating assignments with quotients larger and smaller than the threshold',\n",
       " 'as expected we find that accuracies for percentage known unknown overall unknowns a table part-of-speech tagging accuracy for the penn treebank',\n",
       " 'the table shows the percentage of unknown tokens separate accuracies and standard deviations for known and unknown tokens as well as the overall accuracy',\n",
       " 'percentage known unknown overall unknowns reliable assignments are much higher than for unreliable assignments',\n",
       " 'this distinction is useful for annotation projects during the cleaning process or during pre-processing so the tagger can emit multiple tags if the best tag is classified as unreliable',\n",
       " 'we use the wall street journal as contained in the penn treebank for our experiments',\n",
       " 'the annotation consists of four parts a context-free structure augmented with traces to mark movement and discontinuous constituents phrasal categories that are annotated as node labels a small set of grammatical functions that are annotated as extensions to the node labels and part-of-speech tags',\n",
       " 'this evaluation only uses the part-ofspeech annotation',\n",
       " 'the wall street journal part of the penn treebank consists of approx',\n",
       " 'sentences',\n",
       " 'tagging accuracies for the penn treebank are shown in table',\n",
       " 'figure shows the learning curve of the tagger the accuracy depending on the amount of training data',\n",
       " 'training lh is the number of tokens used for training',\n",
       " 'each training lh was tested ten times',\n",
       " 'training and test sets were disjoint results are averaged',\n",
       " 'the training lh is given on a logarithmic scale',\n",
       " 'as for the negra corpus tagging accuracy is very high for known tokens even with small amounts of training data',\n",
       " 'we exploit the fact that the tagger not only determines tags but also assigns probabilities',\n",
       " 'figure shows the accuracy when separating assignments with quotients larger and smaller than the threshold',\n",
       " 'again we find that accuracies for reliable assignments are much higher than for unreliable assignments',\n",
       " 'average part-of-speech tagging accuracy is between and depending on language and tagset which is at least on a par with state-of-the-art results found in the literature possibly better',\n",
       " 'for the penn treebank reports an accuracy of using the maximum entropy approach our much simpler and therefore faster hmm approach delivers',\n",
       " 'this comparison needs to be re-examined since we use a ten-fold crossvalidation and averaging of results while ratnaparkhi only makes one test run',\n",
       " 'the accuracy for known tokens is significantly higher than for unknown tokens',\n",
       " 'for thrman newspaper data results are better when the word was seen before and therefore is in the lexicon than when it was not seen before',\n",
       " 'accuracy for known tokens is high even with very small amounts of training data',\n",
       " 'as few as tokens are sufficient to achieve - accuracy for them',\n",
       " 'it is important for the tagger to have seen a word at least once during training',\n",
       " 'stochastic taggers assign probabilities to tags',\n",
       " 'we exploit the probabilities to determine reliability of assignments',\n",
       " 'for a subset that is determined during processing by the tagger we achieve accuracy rates of over',\n",
       " 'the accuracy of the complement set is much lower',\n",
       " 'this information can be exploited in an annotation project to give an additional treatment to the unreliable assignments or to pass selected ambiguities to a subsequent processing step',\n",
       " 'we have shown that a tagger based on markov models yields state-of-the-art results despite contrary claims found in the literature',\n",
       " 'for example the markov model tagger used in the comparison of yielded worse results than all other taggers',\n",
       " 'in our opinion a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor',\n",
       " 'the rather large amount of freedom was not handled in detail in previous publications handling of start- and end-of-sequence the exact smoothing technique how to determine the wts for context probabilities details on handling unknown words and how to determine the wts for unknown words',\n",
       " 'note that the decisions we made yield good results for both thrman and the ish corpus',\n",
       " 'they do so for several other corpora as well',\n",
       " 'the architecture remains applicable to a large variety of languages',\n",
       " 'according to current tagger comparisons and according to a comparsion of the results presented here with those in the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here',\n",
       " 'it is a very interesting future research topic to determine the advantages of either of these approaches to find the reason for their high accuracies and to find a good combination of both',\n",
       " 'tnt is freely available to universities and related organizations for research purposes',\n",
       " 'many thanks go to hans uszkoreit for his support during the development of tnt',\n",
       " 'most of the work on tnt was carried out while the author received a grant of the deutsche forschungsgemeinschaft in thaduiertenkolleg kognitionswissenschaft saarbriicken',\n",
       " 'large annotated corpora are the pre-requisite for developing and testing part-ofspeech taggers and they enable thneration of high-quality language models',\n",
       " 'therefore i would like to thank all the people who took the effort to annotate the penn treebank the susanne corpus the stuttgarter referenzkorpus the negra corpus the verbmobil corpora and several others',\n",
       " 'and last but not least i would like to thank the users of tnt who provided me with bug reports and valuable suggestions for improvements']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases_papers2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "992cae97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:54:03.225132Z",
     "start_time": "2023-02-23T00:54:03.216749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentences'}\n",
      "{'n'}\n",
      "{'and'}\n",
      "set()\n",
      "{'two-tailed'}\n"
     ]
    }
   ],
   "source": [
    "for i in lista_dict_probs:\n",
    "    print(find_key(i,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f6c484ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:54:04.660400Z",
     "start_time": "2023-02-23T00:54:04.630001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1, 1.0574060192225445e-24, 2.243465551361857e-33}\n",
      "{0.0, 1}\n",
      "{0.0, 1, 5.189919387348301e-26}\n",
      "{0.0}\n",
      "{0.0, 1}\n"
     ]
    }
   ],
   "source": [
    "for i in lista_paper_prob:\n",
    "    print(set(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "23bd718d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:34:22.646861Z",
     "start_time": "2023-02-23T01:34:22.630839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{12.25470006987771, 31.512647310163743, inf}\n",
      "{inf}\n",
      "{11.311884822457355, inf}\n",
      "{inf}\n",
      "{inf}\n"
     ]
    }
   ],
   "source": [
    "for i in lista_paper_perp:\n",
    "    print(set(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b463c581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:53:27.040682Z",
     "start_time": "2023-02-23T01:53:27.017273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora',\n",
       " 'the system was developed and tested using essay-lh responses to prompts on the test of ish as a for language']"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_objetivo_ft[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "a630dc49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:53:29.806761Z",
     "start_time": "2023-02-23T01:53:29.791198Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an unsupervised method for detecting grammatical errors we present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora',\n",
       " 'the system was developed and tested using essay-lh responses to prompts on the test of ish as a for language',\n",
       " 'the errorrecognition system alek performs with about precision and recall',\n",
       " 'a good indicator of whether a person knows the meaning of a word is the ability to use it appropriately in a sentence',\n",
       " 'much information about usage can be obtained from quite a limited context choueka and lusignan found that people can typically recognize the intended sense of a polysemous word by looking at a narrow window of one or two words around it',\n",
       " 'statistically-based computer programs have been able to do the same with a high level of accuracy',\n",
       " 'thal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word',\n",
       " 'we have developed a statistical system alek that uses statistical analysis for this purpose',\n",
       " 'a major objective of this research is to avoid the laborious and costly process of collecting errors for each word that we wish to evaluate',\n",
       " 'instead we train alek on a general corpus of ish and on edited text containing example uses of the target word',\n",
       " 'the system identifies inappropriate usage based on differences between the words local context cues in an essay and the models of context it has derived from the corpora of well-formed sentences',\n",
       " 'a requirement for alek has been that all steps in the process be automated beyond choosing the words to be tested and assessing the results',\n",
       " 'once a target word is chosen preprocessing building a model of the words appropriate usage and identifying usage errors in essays is performed without manual intervention',\n",
       " 'alek has been developed using the test of ish as a for language administered by the educational testing service',\n",
       " 'toefl is taken by for students who are applying to us undaduate and graduate-level programs',\n",
       " 'approaches to detecting errors by non-native writers typically producammars that look for specific expected error types',\n",
       " 'under this approach essays written by esl students are collected and examined for errors',\n",
       " 'parsers are then adapted to identify those error types that were found in the essay collection',\n",
       " 'we take a different approach initially viewing error detection as an extension of the word sense disambiguation problem',\n",
       " 'corpus-based wsd systems identify the intended sense of a polysemous word by collecting a set of example sentences for each of its various senses and extracting salient contextual cues from these sets to build a statistical model for each sense',\n",
       " 'they identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model yarowsky',\n",
       " 'golding showed how methods used for wsd could be adapted to detect errors resulting from common spelling confusions among sets such as there their and theyre',\n",
       " 'he extracted contexts from correct usage of each confusable word in a training corpus and then identified a new occurrence as an error when it matched the wrong context',\n",
       " 'however most grammatical errors are not the result of simple word confusions',\n",
       " 'this complicates the task of building a model of incorrect usage',\n",
       " 'one approach we considered was to proceed without such a model represent appropriate word usage in a single model and compare a novel example to that model',\n",
       " 'the most appealing part of this formulation was that we could bypass the knowl acquisition bottleneck',\n",
       " 'all occurrences of the word in a collection of edited text could be automatically assigned to a single training set representing appropriate usage',\n",
       " 'inappropriate usage would be signaled by contextual cues that do not occur in training',\n",
       " 'unfortunately this approach was not effective for error detection',\n",
       " 'an example of a word usage error is often very similar to the model of appropriate usage',\n",
       " 'an incorrect usage can contain two or three salient contextual elements as well as a single anomalous element',\n",
       " 'the problem of error detection does not entail finding similarities to appropriate usage rather it requires identifying one element among the contextual cues that simply does not fit',\n",
       " 'what kinds of anomalous elements does alek identify writers sometimes produce errors that violate basic principles of ish syntax while other mistakes show a lack of information about a specific vocabulary item',\n",
       " 'in order to detect these two types of problems alek uses a -million word general corpus of ish from the san jose mercury news and for each target word a set of example sentences from north american newspaper text',\n",
       " 'the corpora are extracted from the acl-dci corpora',\n",
       " 'in selecting the sentences for the word alek infers negative evidence from the contextual cues that do not co-occur with the target word either in the word specific corpus or in thneral ish one',\n",
       " 'it uses two kinds of contextual cues in a word window around the target word function words and part-of-speech tags',\n",
       " 'the brill tagger output is post-processed to ampquotenrichampquot some closed class categories of its tag set such as subject versus object pronoun and definite versus indefinite determiner',\n",
       " 'the enriched tags were adapted from francis and kaera',\n",
       " 'after the sentences have been preprocessed alek counts sequences of adjacent part-ofspeech tags and function words',\n",
       " 'for example the sequence a/at full-time/i',\n",
       " 'jobinn contributes one occurrence each to the bigrams atjj jjnn ajj and to the part-of-speech tag trigram atjjnn',\n",
       " 'each individual tag and function word also contributes to its own unigram count',\n",
       " 'these frequencies form the basis for the error detection measures',\n",
       " 'from thneral corpus alek computes a mutual information measure to determine which sequences of part-of-speech tags and function words are unusually rare and are therefore likely to be ungrammatical in ish',\n",
       " 'mutual information has often been used to detect combinations of words that occur more frequently than we would expect based on the assumption that the words are independent',\n",
       " 'here we use this measure for the opposite purpose to find combinations that occur less often than expected',\n",
       " 'alek also looks for sequences that are common in general but unusual in the word specific corpus',\n",
       " 'these divnces between the two corpora reflect syntactic properties that are peculiar to the target word',\n",
       " 'the system computes mutual information comparing the proportion of observed occurrences of bigrams in thneral corpus to the proportion expected based on the assumption of independence as shown below here p is the probability of the occurrence of the ab bigram estimated from its frequency in thneral corpus and p and p are the probabilities of the first and second elements of the bigram also estimated from thneral corpus',\n",
       " 'ungrammatical sequences should produce bigram probabilities that are much smaller than the product of the unigram probabilities',\n",
       " 'trigram sequences are also used but in this case the mutual information computation compares the co-occurrence of abc to a model in which a and c are assumed to be conditionally independent given b',\n",
       " 'once again a negative value is often indicative of a sequence that violates a rule of ish',\n",
       " 'alek also uses mutual information to compare the distributions of tags and function words in the word-specific corpus to the distributions that are expected based on thneral corpus',\n",
       " 'the measures for bigrams and trigrams are similar to thosven above except that the probability in the numerator is estimated from the wordspecific corpus and the probabilities in the denominator come from thneral corpus',\n",
       " 'to return to a previous example the phrase a knowl contains the tag bigram for singular determiner followed by singular noun',\n",
       " 'this sequence is much less common in the word-specific corpus for knowl than would be expected from thneral corpus unigram probabilities of at and nn',\n",
       " 'in addition to bigram and trigram measures alek compares the target words part-ofspeech tag in the word-specific corpus and in thneral corpus',\n",
       " 'specifically it looks at the conditional probability of the part-of-speech tag given the major syntactic category in both distributions by computing the following value',\n",
       " 'for example in thneral corpus about half of all noun tokens are plural but in the training set for the noun knowl the plural knowls occurs rarely if at all',\n",
       " 'the mutual information measures provide candidate errors but this approach ovnerates it finds rare but still quitammatical sequences',\n",
       " 'to reduce the number of false positives no candidate found by the mi measures is considered an error if it appears in the word-specific corpus at least two times',\n",
       " 'this increases aleks precision at the price of reduced recall',\n",
       " 'for example a knowl will not be treated as an error because it appears in the training corpus as part of the longer a knowl of sequence',\n",
       " 'alek also uses another statistical technique for finding rare and possibly ungrammatical tag and function word bigrams by computing the x statistic for the difference between the bigram proportions found in the word-specific and in thneral corpus the x measure faces the same problem of ovnerating errors',\n",
       " 'due to the large sample sizes extreme values can be obtained even though effect size may be minuscule',\n",
       " 'to reduce false positives alek requires that effect sizes be at least in the moderate-to-small range',\n",
       " 'direct evidence from the word specific corpus can also be used to control the ovneration of errors',\n",
       " 'for each candidate error alek compares the larger context in which the bigram appears to the contexts that have been analyzed in the word-specific corpus',\n",
       " 'from the wordspecific corpus alek forms templates sequences of words and tags that represent the local context of the target',\n",
       " 'if a test sentence contains a low probability bigram the local context of the target is compared to all the templates of which it is a part',\n",
       " 'exceptions to the error that is longer grammatical sequences that contain rare subsequences are found by examining conditional probabilities',\n",
       " 'to illustrate this consider the example of a knowl and a knowl of the conditional probability of of given a knowl is high as it accounts for almost all of the occurrences of a knowl in the wordspecific corpus',\n",
       " 'based on this high conditional probability the system will use the template for a knowl of to keep it from being marked as an error',\n",
       " 'other function words and tags in the position have much lower conditional probability so for example a knowl is will not be treated as an exception to the error',\n",
       " 'toefl essays araded on a point scale where demonstrates ampquotclear competenceampquot in writing on rhetorical and syntactic levels and demonstrates ampquotincompetence in writingampquot',\n",
       " 'if low probability n-grams signal grammatical errors then we would expect toefl essays that received lower scores to have more of these ngrams',\n",
       " 'to test this prediction we randomly selected from the toefl pool essays for each of the score values from to',\n",
       " 'for each score value all essays were concatenated to form a super-essay',\n",
       " 'in every super-essay for each adjacent pair and triple of tags containing a noun verb or adjective the bigram and trigram mutual information values were computed based on thneral corpus',\n",
       " 'table shows the proportions of bigrams and trigrams with mutual information less than',\n",
       " 'as predicted there is a significant negative correlation between the score and the proportion of low probability bigrams and trigrams',\n",
       " 'alek was developed using three target words that were extracted from toefl essays concentrate interest and knowl',\n",
       " 'these words were chosen because they represent different parts of speech and varying degrees of polysemy',\n",
       " 'each also occurred in at least sentences in what was then a small pool of toefl essays',\n",
       " 'before development began each occurrence of these words was manually labeled as an appropriate or inappropriate usage without taking into account grammatical errors that might have been present elsewhere in the sentence but which were not within the target words scope',\n",
       " 'critical values for the statistical measures were set during this development phase',\n",
       " 'the settings were based empirically on aleks performance so as to optimize precision and recall on the three development words',\n",
       " 'candidate errors were those local context sequences that produced a mutual information value of less than based on thneral corpus mutual information of less than for the specific/general comparisons or a x valueater than with an effect sizeater than',\n",
       " 'precision and recall for the three words are shown below',\n",
       " 'alek was tested on words',\n",
       " 'these words were randomly selected from those which met two criteria they appear in a university word list as words that a student in a us university will be expected to encounter and there were at least sentences containing the word in the toefl essay pool',\n",
       " 'to build the usage model for each target word sentences containing it were extracted from the north american news corpus',\n",
       " 'preprocessing included detecting sentence boundaries and part-of-speech tagging',\n",
       " 'as in the development system the model of general ish was based on bigram and trigram frequencies of function words and part-ofspeech tags from -million words of the san jose mercury news',\n",
       " 'for each test word all of the test sentences were marked by alek as either containing an error or not containing an error',\n",
       " 'the size of the test set for each word ranged from to with a mean of sentences',\n",
       " 'to evaluate the system for each test word we randomly extracted sentences that alek classified as containing no error and sentences which it labeled as containing an error',\n",
       " 'these sentences were presented to a linguist in a random order for blind evaluation',\n",
       " 'the linguist who had no part in aleks development marked each usage of the target word as incorrect or correct and in the case of incorrect usage indicated how far from the target one would have to look in order to recognise that there was an error',\n",
       " 'for example in the case of ampquotan periodampquot the error occurs at a distance of one word from period',\n",
       " 'when the error is an omission as in ampquotlived in victorian periodampquot the distance is where the missing word should have appeared',\n",
       " 'in this case the missing determiner is positions away from the target',\n",
       " 'when more than one error occurred the distance of the one closest to the target was marked',\n",
       " 'table lists the precision and recall for the test words',\n",
       " 'the column labelled ampquotrecallampquot is the proportion of human-judged errors in the sentence sample that were detected by alek',\n",
       " 'ampquottotal recallampquot is an estimate that extrapolates from the human judgements of the sample to the entire test set',\n",
       " 'we illustrate this with the results for pollution',\n",
       " 'the human judge marked as incorrect usage of the sample from aleks e-set and of the sample from its c-set',\n",
       " 'to estimate overall incorrect usage we computed a wted mean of these two rates where the wts reflected the proportion of sentences that were in the e-set and c-set',\n",
       " 'the e-set contained of the pollution sentences and the c-set had the remaining',\n",
       " 'with the human judgements as thld standard the estimated overall rate of incorrect usage is',\n",
       " 'aleks estimated recall is the proportion of sentences in the e-set times its precision divided by the overall estimated error rate /',\n",
       " 'the precision results vary from word to word',\n",
       " 'conclusion and pollution have precision in the low to middle s while individuals precision is',\n",
       " 'overall aleks predictions are about accurate',\n",
       " 'the recall is limited in part by the fact that the system only looks at syntactic information while many of the errors are semantic',\n",
       " 'nicholls identifies four error types an unnecessary word a missing word a word or phrase that needs replacing a word used in the wrong form',\n",
       " 'alek recognizes all of these types of errors',\n",
       " 'for closed class words alek identified whether a word was missing the wrong word was used and when an extra word was used',\n",
       " 'open class words have a fourth error category form including inappropriate compounding and verb agreement',\n",
       " 'during the development stage we found it useful to add additional error categories',\n",
       " 'since teofl graders are not supposed to take punctuation into account punctuation errors were only marked when they caused the judge to ampquotgarden pathampquot or initially misinterpret the sentence',\n",
       " 'spelling was marked either when a function word was misspelled causing part-ofspeech tagging errors or when the writers intent was unclear',\n",
       " 'the distributions of categories for hits and misses shown in table are not strikingly different',\n",
       " 'however the hits are primarily syntactic in nature while the misses are both semantic and syntactic',\n",
       " 'alek is sensitive to open-class word confusions where the part of speech differs or where the target word is confused with another word',\n",
       " 'in both cases the system recognizes that the target is in the wrong syntactic environment',\n",
       " 'misses can also be syntactic when the target word is confused with another word but the syntactic environment fails to trigger an error',\n",
       " 'in addition alek does not recognize semantic errors when the error involves the misuse of an open-class word in combination with the target',\n",
       " 'closed class words typically are either selected by or agree with a head word',\n",
       " 'so why are there so many misses especially with prepositions the problem is caused in part by polysemy when one sense of the word selects a preposition that another sense does not',\n",
       " 'when concentrate is used spatially it selects the preposition in as ampquotthe stores were concentrated in the downtown areaampquot',\n",
       " 'when it denotes mental activity it selects the preposition on as in ampquotsusan concentrated on her studiesampquot',\n",
       " 'since alek trains on all senses of concentrate it does not detect the error in ampquotsusan concentrated in her studiesampquot',\n",
       " 'another cause is that adjuncts especially temporal and locative adverbials distribute freely in the wordspecific corpora as in ampquotsusan concentrated in her room',\n",
       " 'ampquot this second problem is more tractable than the polysemy problem and would involve training the system to recognize certain types of adjuncts',\n",
       " 'false positives when alek ampquotidentifiesampquot an error where none exists fall into six major categories',\n",
       " 'the percentage of each false positive type in a random sample of false positives is shown in table',\n",
       " 'domain mismatch mismatch of the newspaper-domain word-specific corpora and essay-domain test corpus',\n",
       " 'one notable difference is that some toefl essay prompts call for the writers opinion',\n",
       " 'consequently toefl essays often contain first person references whereas newspaper articles are written in the third person',\n",
       " 'we need to supplement the word-specific corpora with material that more closely resembles the test corpus',\n",
       " 'tagger incorrect analysis by the part-of-speech tagger',\n",
       " 'when the part-of-speech tag is wrong alek often recognizes the resulting n-gram as anomalous',\n",
       " 'many of these errors are caused by training on the brown corpus instead of a corpus of essays',\n",
       " 'syntactic analysis errors resulting from using part-of-speech tags instead of supertags or a full parse which would give syntactic relations between constituents',\n",
       " 'for example alek false alarms on arguments of ditransitive verbs such as offer and flags as an error ampquotyou benefitsampquot in ampquotoffers you benefitsampquot',\n",
       " 'free distribution elements that distribute freely such as adverbs and conjunctions as well as temporal and locative adverbial phrases tend to be identified as errors when they occur in some positions',\n",
       " 'punctuation most notably omission of periods and commas',\n",
       " 'since these errors are not indicative of ones ability to use the target word they were not considered as errors unless they caused the judge to misanalyze the sentence',\n",
       " 'infrequent tags',\n",
       " 'an undesirable result of our ampquotenrichedampquot tag set is that some tags the post-determiner last occur too infrequently in the corpora to provide reliable statistics',\n",
       " 'solutions to some of these problems will clearly be more tractable than to others',\n",
       " 'comparison of these results to those of other systems is difficult because there is no generally accepted test set or performance baseline',\n",
       " 'given this limitation we compared aleks performance to a widely used grammar checker the one incorporated in microsofts word',\n",
       " 'we created files of sentences used for the three development words concentrate interest and knowl and manually corrected any errors outside the local context around the target before checking them with word',\n",
       " 'the performance for concentrate showed overall precision of and recall of',\n",
       " 'for interest precision was with recall of',\n",
       " 'in sentences containing knowl precision was and recall was',\n",
       " 'word correctly detected the ungrammaticality of knowls as well as a knowl while it avoided flagging a knowl of',\n",
       " 'in summary words precision in error detection is impressive but the lower recall values indicate that it is responding to fewer error types than does alek',\n",
       " 'in particular word is not sensitive to inappropriate selection of prepositions for these three words',\n",
       " 'of course word detects many kinds of errors that alek does not',\n",
       " 'research has been reported on grammar checkers specifically designed for an esl population',\n",
       " 'these have been developed by hand based on small training and test sets',\n",
       " 'schneider and mccoy developed a system tailored to the error productions of american sign language signers',\n",
       " 'this system was tested on sentences containing determiner and agreement errors and grammatical sentences',\n",
       " 'we calculate that their precision was with recall',\n",
       " 'park palmer and washburn adapted a categorial grammar to recognize ampquotclasses of errors that dominateampquot in the nine essays they inspected',\n",
       " 'this system was tested on t essays but precision and recall figures are not reported',\n",
       " 'the unsupervised techniques that we have presented for inferring negative evidence are effective in recognizing grammatical errors in written text',\n",
       " 'preliminary results indicate that aleks error detection is predictive of toefl scores',\n",
       " 'if alek accurately detects usage errors then it should report more errors in essays with lower scores than in those with higher scores',\n",
       " 'we have already seen in table that there is a negative correlation between essay score and two of aleks component measures thneral corpus n-grams',\n",
       " 'however the data in table were not based on specific vocabulary items and do not reflect overall system performance which includes the other measures as well',\n",
       " 'table shows the proportion of test word occurrences that were classified by alek as containing errors within two positions of the target at each of toefl score points',\n",
       " 'as predicted the correlation is negative',\n",
       " 'these data support the validity of the system as a detector of inappropriate usage even when only a limited number of words are targeted and only the immediate context of each target is examined',\n",
       " 'alek and by a human judge for comparison table also gives the estimated proportions of inappropriate usage by score point based on the human judges classification',\n",
       " 'here too there is a negative correlation rs',\n",
       " 'n p lt',\n",
       " 'two-tailed',\n",
       " 'although the system recognizes a wide range of error types as table shows it detects only about one-fifth as many errors as a human judge does',\n",
       " 'to improve recall research needs to focus on the areas identified in section and to improve precision efforts should be directed at reducing the false positives described in',\n",
       " 'alek is being developed as a diagnostic tool for students who are learning ish as a for language',\n",
       " 'however its techniques could be incorporated into a grammar checker for native speakers',\n",
       " 'we thank susanne wolff for evaluating the test sentences and robert kantor ken sheppard and anonymous reviewers for their helpful suggestions']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases_papers2[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "19098aed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T00:54:05.998782Z",
     "start_time": "2023-02-23T00:54:05.982674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista_paper_prob[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f0cee",
   "metadata": {},
   "source": [
    "Note that it's very easy to make a mistake with the input to e.g. lm.fit, lm.entropy, lm.perplexity or lm.score. In fact, when writing this script, I first initially accidentally made a language model with only characters, which did lead to scores of only 0 and perplexities/entropies of infinity. Furthermore, accidentally using (\"that\",) rather than [(\"that\",)] is a very easy mistake, and has big consequences:\n",
    "https://github.com/nltk/nltk/issues/3065"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2d03c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T20:33:35.147912Z",
     "start_time": "2023-02-22T20:33:35.135911Z"
    }
   },
   "source": [
    "#jeito certo de medir entropia\n",
    "model.entropy([(\"claims\",)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158eaa60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T20:36:43.300165Z",
     "start_time": "2023-02-22T20:36:43.269068Z"
    }
   },
   "source": [
    "#jeito certo de medir a perplexidade\n",
    "text='contrary to claims'\n",
    "model.perplexity(bigrams(text.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d5bee8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T20:30:51.466072Z",
     "start_time": "2023-02-22T20:30:51.455077Z"
    }
   },
   "source": [
    "prod=1\n",
    "for i in lista_bigramas2[0][1]:\n",
    "    print(f'P({i[1]}|{i[0]}):',model.score(i[1],context=i[0].split()))\n",
    "    prod = prod*model.score(i[1],context=i[0].split())\n",
    "    print(model.entropy(i[1]))\n",
    "    print(model.perplexity(i[1]))\n",
    "print(prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f0017",
   "metadata": {},
   "source": [
    "## Trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d1da809c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:31:41.383635Z",
     "start_time": "2023-02-23T01:31:40.189918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 2979 items>\n"
     ]
    }
   ],
   "source": [
    "#Trigrama\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, frase_objetivo_ft_formato_ml)\n",
    "\n",
    "model_tri = MLE(n)\n",
    "model_tri.fit(train_data, padded_sents)\n",
    "print(model_tri.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f093089b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:31:41.698748Z",
     "start_time": "2023-02-23T01:31:41.666585Z"
    }
   },
   "outputs": [],
   "source": [
    "#Criando uma lista de palavras, para cada frase, de cada paper\n",
    "lista_palavras_papers=[]\n",
    "for j in frases_papers2:\n",
    "    x=[]\n",
    "    for i in j:\n",
    "            x.append(i.lstrip().split(' '))\n",
    "    lista_palavras_papers.append(x)\n",
    "    \n",
    "#Criando os bigramas da lista\n",
    "trigramas=[]\n",
    "for papers in lista_palavras_papers:\n",
    "    x=[]\n",
    "    for frases in papers:\n",
    "        x.append(trigrams(frases))\n",
    "    trigramas.append(x)\n",
    "    \n",
    "lista_trigramas2=[]\n",
    "for j in trigramas:\n",
    "    lista_trigramas=[]  \n",
    "    for i in j:\n",
    "        lista_trigramas.append(list(i))\n",
    "    lista_trigramas2.append(lista_trigramas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "2165aef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:32:22.935916Z",
     "start_time": "2023-02-23T01:32:22.201073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Probabilidade de cada frase do paper, com base nos bigramas\n",
    "lista_paper_prob_tri=[]\n",
    "lista_paper_perp_tri=[]\n",
    "for paper in lista_trigramas2:\n",
    "    lista_prob_tri=[]\n",
    "    lista_perp_tri=[]\n",
    "    for frase in paper:\n",
    "        prob=1\n",
    "        for trigrama in frase:\n",
    "            prob = prob * (model_tri.score(trigrama[2], context=[trigrama[1],trigrama[0]]))\n",
    "        lista_prob_tri.append(prob)\n",
    "        try:\n",
    "            lista_perp_tri.append(model_tri.perplexity(frase))\n",
    "        except:\n",
    "            lista_perp_tri.append(math.inf)\n",
    "    lista_paper_prob_tri.append(lista_prob_tri)\n",
    "    lista_paper_perp_tri.append(lista_perp_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "89174a77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:33:41.456062Z",
     "start_time": "2023-02-23T01:33:41.441025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.9537753906637032, 1.6641416847460972, inf}\n",
      "{inf}\n",
      "{1.853007722166056, inf}\n",
      "{inf}\n",
      "{inf}\n"
     ]
    }
   ],
   "source": [
    "for i in lista_paper_perp_tri:\n",
    "    #print(set(lista_prob_tri))\n",
    "    print(set(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980f869",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "\n",
    "Aplicar o modelo de linguagem na parcela de teste tambem, para validar a perplexidade dela.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Alguns deram infinito por conta do preprocessamento: \n",
    "\n",
    "suspeita -> removação de hífen no preprocessamento e no outro nao\n",
    "título fica junto da frase, mas não fica junto no treino do modelo\n",
    "\n",
    "CAMINHO:\n",
    "DAR O SPLIT NO </S> AO INVÉS DE DAR NO . \n",
    "\n",
    "\n",
    "DEPOIS LIMPAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763625e",
   "metadata": {},
   "source": [
    "## Conclusão Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "7fe874c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:45:12.053658Z",
     "start_time": "2023-02-23T01:45:12.030382Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_contador_positivo=[]\n",
    "lista_contador_negativo=[]\n",
    "for perplexidades_bi_paper, perplexidades_tri_paper in zip(lista_paper_perp,lista_paper_perp_tri):\n",
    "    contador_positivo=0\n",
    "    contador_negativo=0\n",
    "    for p_bi, p_tri in zip(perplexidades_bi_paper, perplexidades_tri_paper):\n",
    "        if p_tri<=p_bi:\n",
    "            contador_positivo=contador_positivo+1\n",
    "        else:\n",
    "            contador_negativo=contador_negativo+1\n",
    "        #lista_contador_negativo.append(contador_negativo)\n",
    "    lista_contador_positivo.append(contador_positivo)\n",
    "    lista_contador_negativo.append(contador_negativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "a65c7310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:45:12.394402Z",
     "start_time": "2023-02-23T01:45:12.378391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_contador_negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5e309f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:45:12.944375Z",
     "start_time": "2023-02-23T01:45:12.931856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[182, 159, 160, 191, 189]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_contador_positivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed98dd",
   "metadata": {},
   "source": [
    "Conclusão: \n",
    "    Quanto menor a perplexidade, melhor. Para todas as frases dos papers, a perplexidade do trigrama é menor ou igual à perplexidade do bigrama. Logo, o melhor modelo a ser seguido é o trigrama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8463ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:47:58.864772Z",
     "start_time": "2023-02-23T01:47:58.851229Z"
    }
   },
   "source": [
    "TO DO:\n",
    "    Contar a qtd de frases cuja perplexidade diferente de infinito (e probabilidade diferente de 0). \n",
    "    Observar que para essa mesma quantidade, as perplexidades são menores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "9f7021aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T01:47:41.971950Z",
     "start_time": "2023-02-23T01:47:41.955783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_contador_menor=[]\n",
    "for perplexidades_bi_paper, perplexidades_tri_paper in zip(lista_paper_perp,lista_paper_perp_tri):\n",
    "    contador_menor=0\n",
    "    for p_bi, p_tri in zip(perplexidades_bi_paper, perplexidades_tri_paper):\n",
    "        if p_tri<p_bi:\n",
    "            contador_menor=contador_menor+1\n",
    "    lista_contador_menor.append(contador_menor)\n",
    "lista_contador_menor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f9ad7",
   "metadata": {},
   "source": [
    "# Experimento 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d88bd7",
   "metadata": {},
   "source": [
    "## aplicando o método da similaridade nos 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "814adfdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T03:13:14.682972Z",
     "start_time": "2023-02-23T03:13:12.063649Z"
    }
   },
   "outputs": [],
   "source": [
    "#Definição do Abstract\n",
    "abstract=[]\n",
    "for i in papers_teste.values():\n",
    "    abstract.append(preprocessa_abstract(i))\n",
    "    \n",
    "#Definição da Introdução\n",
    "intro=[]\n",
    "for i in papers_teste.values():\n",
    "    intro.append(preprocessa_intro(i))\n",
    "    \n",
    "#Definição da Conclusão\n",
    "conclusion=[]\n",
    "for i in papers_teste.values():\n",
    "    conclusion.append(preprocessa_conclusion(i))\n",
    "    \n",
    "#Tokens\n",
    "token_abstract   = []\n",
    "for texto in abstract:\n",
    "    token_abstract.append(texto.split())\n",
    "    \n",
    "token_intro   = []\n",
    "for texto in intro:\n",
    "    token_intro.append(texto.split())\n",
    "    \n",
    "token_conclusion   = []\n",
    "for texto in conclusion:\n",
    "    token_conclusion.append(texto.split())\n",
    "\n",
    "tokens = token_abstract+token_intro+token_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "aad5547f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T03:16:56.006134Z",
     "start_time": "2023-02-23T03:16:11.422793Z"
    }
   },
   "outputs": [],
   "source": [
    "#FastText\n",
    "frases_intro_ft,palavras_intro_ft,vetor_intro_ft                = frases_palavras_vetores(intro, vetor_frases_fasttext)\n",
    "frases_abstract_ft,palavras_abstract_ft,vetor_abstract_ft       = frases_palavras_vetores(abstract, vetor_frases_fasttext)\n",
    "frases_conclusion_ft,palavras_conclusion_ft,vetor_conclusion_ft = frases_palavras_vetores(conclusion, vetor_frases_fasttext)\n",
    "\n",
    "\n",
    "#Abtract e Introdução\n",
    "sim_ab_intro_ft,indices_ab_intro_ft,lista_idx_ab_intro_ft = aplicando_dicionario_similaridade(vetor_abstract_ft,vetor_intro_ft)\n",
    "\n",
    "#Abtract e Conclusão\n",
    "sim_ab_conc_ft,indices_ab_conc_ft,lista_idx_ab_c_ft = aplicando_dicionario_similaridade(vetor_abstract_ft,vetor_conclusion_ft)\n",
    "\n",
    "#Indices\n",
    "indices_objetivo_ft = frases_interseccao(lista_idx_ab_intro_ft,lista_idx_ab_c_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "59bdb427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T03:17:01.143362Z",
     "start_time": "2023-02-23T03:17:01.130483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [5],\n",
       " [2],\n",
       " [1, 2],\n",
       " [1],\n",
       " [2],\n",
       " [4],\n",
       " [2],\n",
       " [0, 1, 2],\n",
       " [0],\n",
       " [2, 5],\n",
       " [1],\n",
       " [0, 3],\n",
       " [0],\n",
       " [0, 5],\n",
       " [2],\n",
       " [0, 1],\n",
       " [0, 3],\n",
       " [0, 6],\n",
       " [],\n",
       " [1, 3],\n",
       " [2],\n",
       " [0, 1, 3],\n",
       " [2, 3],\n",
       " [0, 1],\n",
       " [0],\n",
       " [3, 4],\n",
       " [2],\n",
       " [1, 3],\n",
       " [0, 2],\n",
       " [0, 1],\n",
       " [0, 1, 2],\n",
       " [0, 3],\n",
       " [1],\n",
       " [0, 1],\n",
       " [1, 2],\n",
       " [1, 2],\n",
       " [2],\n",
       " [3],\n",
       " [4],\n",
       " [0],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [0, 1],\n",
       " [1, 3],\n",
       " [1],\n",
       " [1, 4],\n",
       " [0, 1],\n",
       " [0],\n",
       " [0],\n",
       " [1, 2],\n",
       " [],\n",
       " [2],\n",
       " [2],\n",
       " [3],\n",
       " [],\n",
       " [0],\n",
       " [0],\n",
       " [1, 3],\n",
       " [1, 5],\n",
       " [0, 3],\n",
       " [0],\n",
       " [0, 2],\n",
       " [0, 2],\n",
       " [0, 2],\n",
       " [1, 2],\n",
       " [1, 3],\n",
       " [0],\n",
       " [0],\n",
       " [2],\n",
       " [2, 4],\n",
       " [0],\n",
       " [0],\n",
       " [5],\n",
       " [2],\n",
       " [1],\n",
       " [0, 1],\n",
       " [],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1, 2],\n",
       " [1, 2],\n",
       " [4],\n",
       " [0, 1, 2],\n",
       " [0, 1],\n",
       " [9, 11],\n",
       " [0],\n",
       " [1, 2],\n",
       " [2, 3],\n",
       " [2],\n",
       " [0],\n",
       " [3],\n",
       " [0, 1],\n",
       " [2, 3],\n",
       " [5],\n",
       " [0, 2],\n",
       " [0, 1, 2],\n",
       " [3],\n",
       " [0, 3],\n",
       " [3],\n",
       " [2],\n",
       " [4, 5],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [2],\n",
       " [1],\n",
       " [],\n",
       " [4],\n",
       " [2, 3],\n",
       " [4, 6],\n",
       " [1, 6],\n",
       " [0],\n",
       " [0, 1],\n",
       " [0],\n",
       " [0, 2],\n",
       " [2],\n",
       " [1],\n",
       " [1, 3],\n",
       " [1, 2, 5],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [5],\n",
       " [1, 3],\n",
       " [1],\n",
       " [3, 4],\n",
       " [0, 2, 4],\n",
       " [0],\n",
       " [],\n",
       " [0, 3, 4],\n",
       " [4],\n",
       " [1],\n",
       " [1],\n",
       " [1, 3, 5],\n",
       " [3],\n",
       " [0, 4],\n",
       " [],\n",
       " [0, 1],\n",
       " [2, 3, 4],\n",
       " [3],\n",
       " [0, 1],\n",
       " [1],\n",
       " [0, 1, 2],\n",
       " [1, 2],\n",
       " [0, 3],\n",
       " [1],\n",
       " [1, 4],\n",
       " [0, 1],\n",
       " [1],\n",
       " [0],\n",
       " [0],\n",
       " [2],\n",
       " [4],\n",
       " [1, 3],\n",
       " [0, 2],\n",
       " [2, 5],\n",
       " [4],\n",
       " [3],\n",
       " [3],\n",
       " [4],\n",
       " [2, 3],\n",
       " [1, 3, 4],\n",
       " [2],\n",
       " [1, 2, 4],\n",
       " [0, 2],\n",
       " [2, 4],\n",
       " [0, 2],\n",
       " [0],\n",
       " [0],\n",
       " [2],\n",
       " [0, 3],\n",
       " [2, 3],\n",
       " [1],\n",
       " [1, 3],\n",
       " [4],\n",
       " [0, 3, 4],\n",
       " [0, 4],\n",
       " [2],\n",
       " [4],\n",
       " [0],\n",
       " [3],\n",
       " [0, 1],\n",
       " [1],\n",
       " [1],\n",
       " [4],\n",
       " [3],\n",
       " [2],\n",
       " [0],\n",
       " [7],\n",
       " [0, 3],\n",
       " [2],\n",
       " [6, 7],\n",
       " [0],\n",
       " [0, 1],\n",
       " [5],\n",
       " [],\n",
       " [6],\n",
       " [],\n",
       " [1, 2, 3],\n",
       " [0],\n",
       " [1, 2],\n",
       " [6, 7],\n",
       " [],\n",
       " [0, 3],\n",
       " [1],\n",
       " [5],\n",
       " [0],\n",
       " [2],\n",
       " [1, 2],\n",
       " [1],\n",
       " [0, 3],\n",
       " [0, 3]]"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_objetivo_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0acfd",
   "metadata": {},
   "source": [
    "#Frases\n",
    "frase_objetivo_ft = get_frases_objetivo(indices_objetivo_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5617594",
   "metadata": {},
   "source": [
    "#Frases\n",
    "frase_objetivo_ft = get_frases_objetivo(indices_objetivo_ft)\n",
    "\n",
    "joblib.dump(frase_objetivo_ft,'frase_objetivo_ft_teste')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b68c3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T03:15:23.858221Z",
     "start_time": "2023-02-23T03:15:23.858221Z"
    }
   },
   "source": [
    "frase_objetivo_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151fc2f",
   "metadata": {},
   "source": [
    "REVISAR A FUNCAO get_frases_objetivo pois ela nao muda o frase_abstract de acordo com o modelo [e no teste isso importa] -> mas acho q nao eh um problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "1946bc91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T03:20:15.591178Z",
     "start_time": "2023-02-23T03:20:15.575171Z"
    }
   },
   "outputs": [],
   "source": [
    "frase_objetivo=[]\n",
    "for idx_f,idx_ind in zip(range(len(frases_abstract_ft)),indices_objetivo_ft):\n",
    "    lista_one=[]\n",
    "    for j in idx_ind:\n",
    "        lista_one.append(frases_abstract_ft[idx_f][j])\n",
    "    frase_objetivo.append(lista_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "0e252569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T03:20:17.794242Z",
     "start_time": "2023-02-23T03:20:17.769513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated ish text, rivals the performance of supervised techniques that require time-consuming hand annotations',\n",
       "  'the algorithm is based on two powerful constraints &#; that words tend to have one sense per discourse and one sense per collocation &#; exploited in an iterative bootstrapping procedure'],\n",
       " ['we also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowl is in principle obtainable'],\n",
       " ['this work is based on the following premises:  grammars are too complex and detailed to develop manually for most interesting domains;  parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and  existing n-grain modeling techniques are inadequate for parsing models'],\n",
       " ['this study suggests that the identification of word translations should also be possible with non-parallel and even unrelated texts',\n",
       "  'the method proposed is based on the assumption that there is a correlation between the patterns of word cooccurrences in texts of different languages'],\n",
       " ['this approach integrates a diverse set of knowl sources to disambiguate word sense, including part of speech of nboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation'],\n",
       " ['this paper addresses the problem for a fairly general form of combinatory categorial grammar, by means of an efficient, correct, and easy to implement normal-form parsing tech- the parser is proved to find exone in each semantic equivalence class of allowable parses; that is, spurious ambiguity  is shown to be both safely and completely eliminated'],\n",
       " ['experimental results arven, showing that the two new algorithms have improved performance over the viterbi algorithm on many criteria, especially the ones that they optimize'],\n",
       " ['tests using wall street journal data show that the method performs at least as well as spatter , which has the best published results for a statistical parser on this task'],\n",
       " ['this paper reports on corpus-based research into the relationship between intonational variation and discourse structure',\n",
       "  'we examine the effects of speaking style  and of discourse segmentation method  on the nature of this relationship',\n",
       "  'we also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment'],\n",
       " ['we present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by jelinek and mercer , katz , and church and gale'],\n",
       " ['this approach, during training the learning program examines many unlabeled examples and selects for labeling  only those that are most informative at each stage',\n",
       "  'we describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-ofspeech tagging'],\n",
       " ['we then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement'],\n",
       " ['most previous corpus-based algorithms disambiguate a word with a classifier trained from previous usages of the same word',\n",
       "  'the algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts'],\n",
       " ['we derive the rhetorical structures of texts by means of two new, surface-form-based algorithms: one that identifies discourse usages of cue phrases and breaks sentences into clauses, and one that produces valid rhetorical structure trees for unrestricted natural language texts'],\n",
       " ['it is challng to translate names and technical terms across languages with different alphabets and sound inventories',\n",
       "  'translating such items from japanese back to ish is even more challng, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries'],\n",
       " ['combining the constraints across many adjectives, a clustering algorithm separates the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative'],\n",
       " ['this paper presents paradise , a general framework for evaluating spoken rlialogue agents',\n",
       "  \"the framework decouples task requirements from an agent's dialogue behaviors, supports comparisons among dialogue strategies, enables the calculation of performance over subdialogues and whole dialogues, specifies the relative contribution of various factors to performance, and makes it possible to compare agents performing different tasks by normalizing for task complexity\"],\n",
       " ['this paper presents a trainable rule-based algorithm for performing word segmentation',\n",
       "  'in addition, we show the transformation-based algorithm to be effective in improving the output of several existing word segmentation algorithms in three different languages'],\n",
       " ['many multilingual nlp applications need to translate words between different languages, but cannot afford the computational expense of inducing or applying a full translation model',\n",
       "  'unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over % accuracy'],\n",
       " [],\n",
       " ['computer recognition of this phenomenon is important because it helps break &amp;quot;the document boundary&amp;quot; by allowing a user to examine information about a particular entity from multiple text sources at the same time',\n",
       "  'in addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the muc-   coreference task'],\n",
       " ['the resulting database will contain  descriptions of the semantic frames underlying the meanings of the words described, and  the valence representation  of several thousand words and phrases, each accompanied by  a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between &amp;quot;frame elements&amp;quot; and their syntactic realizations'],\n",
       " ['one of the most exciting recent directions in machine learning is the discovery that the combination of multiple classifiers often results in significantly better performance than what can be achieved with a single classifier',\n",
       "  'in this paper, we first show that the errors made from three different state of the art part of speech taggers are strongly complementary',\n",
       "  'by using contextual cues to guide tagger combination, we are able to derive a new tagger that achieves performance significantly greater than any of the individual taggers'],\n",
       " ['in particular, we present a corpus-based approach for finding base nps by matching part-ofspeech tag sequences',\n",
       "  'the training phase of the algorithm is based on two successful techniques: first the base np grammar is read from a &amp;quot;treebank&amp;quot; corpus; then thammar is improved by selecting rules with high &amp;quot;benefit&amp;quot; scores'],\n",
       " ['the paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies',\n",
       "  'the model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner &#; therefore usable for automatic speech recognition'],\n",
       " ['in this paper we examine how the differences in modelling between different data driven systems performing the same nlp task can be exploited to yield a higher accuracy than the best individual system'],\n",
       " ['we then present a new evaluation methodology for the automatically constructed thesaurus',\n",
       "  'the evaluation results show that the thesaurus is significantly closer to wordnet than roget thesaurus is'],\n",
       " ['it is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task'],\n",
       " ['in this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars',\n",
       "  'additionally, the entries that arnerated potentially provide broader coverage of the category than would occur to an individual coding them by hand'],\n",
       " [\"i propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities\",\n",
       "  'the ranking criteria for the s-list based on the distinction between entities and incorporate preferences for interand intra-sentential anaphora'],\n",
       " ['distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences',\n",
       "  'our contributions are three-fold: an empirical comparison of a broad range of measures; a classification'],\n",
       " ['we present a method for extracting parts of objects from wholes',\n",
       "  'given a very large corpus our method finds part words with % accuracy for the top  words as ranked by the system',\n",
       "  'the part list could be scanned by an end-user and added to an existing ontology , or used as a part of a rough semantic lexicon'],\n",
       " ['we present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the em framework of statistical estimation',\n",
       "  'we outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries'],\n",
       " ['this work goes a step further by automatically creating not just clusters of related words, but a hierarchy of nouns and their hypernyms, akin to the hand-built hierarchy in wordnet'],\n",
       " ['this paper presents a case study of analyzing and improving intercoder reliability in discourse using statistical techniques',\n",
       "  'corrected tags are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier'],\n",
       " ['we present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus',\n",
       "  'our method is based on the hypothesis that when a phrase is non-composition, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word'],\n",
       " ['we have acquired a corpus of  and  test stories of to grade material; each story is followed by short-answer questions',\n",
       "  'we used these to construct and evaluate a baseline system that uses pattern matching  techniques augmented with additional automated linguistic processing'],\n",
       " ['we have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems'],\n",
       " ['our final results &#; % dependency accuracy &#; represent good progress towards the % accuracy of the parser on ish  text'],\n",
       " ['the current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about % of word translations identified correctly'],\n",
       " ['strand  is a languageindependent system for automatic discovery of text in parallel translation on the world wide web'],\n",
       " ['we describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of lexical'],\n",
       " ['we present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents'],\n",
       " ['tempeval- comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier'],\n",
       " ['this paper presents the description and evaluation framework of semeval- word sense induction &amp; disambiguation task, as well as the evaluation results of  participating systems',\n",
       "  'in this task, participants were required to induce the senses of  target words using a training set, and then disambiguate unseen instances of the same words using the induced senses'],\n",
       " ['the task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario',\n",
       "  'we report on the training and test data used for evaluation, the process of their creation, the participating systems , the approaches adopted and the results achieved'],\n",
       " ['we also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries'],\n",
       " ['previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate',\n",
       "  'we implement our approach using latent semantic analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system'],\n",
       " ['this paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora',\n",
       "  'previous techniques givod results, but fail to cope well with ambiguity or rare words'],\n",
       " ['this paper presents the first-ever results of applying statistical parsing models to the newly-available chinese treebank'],\n",
       " ['this paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging'],\n",
       " ['we present such a component: a fast and robust morphological generator for ish based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required',\n",
       "  'we describe how this morphological generator is used in a prototype system for automatic simplification of ish newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application'],\n",
       " [],\n",
       " ['we find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves'],\n",
       " ['we use latent semantic analysis to make modest gains in performance, but we show the significant challs encountered in trying this approach'],\n",
       " ['test results show lsa is a more accurate similarity measure than the'],\n",
       " [],\n",
       " ['we describe a procedure for arranging into a time-line the contents of news stories describing the development of some situation'],\n",
       " ['nltk, the natural language toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware'],\n",
       " ['to make the svm training with the available largest corpus &#; thnia corpus &#; tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information',\n",
       "  'experiments on thnia corpus show that our class splitting technique not only enables the training with thnia corpus but also improves the accuracy'],\n",
       " ['the model utilized is especially suited for languages with a rich morphology, such as finnish',\n",
       "  'experiments on both finnish and ish corpora show that the presented methods perform well compared to a current stateof-the-art system'],\n",
       " ['open mind word expert is an implemented active learning system for collecting word sense tagging from thneral public over the web',\n",
       "  'we expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers'],\n",
       " ['this paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora'],\n",
       " ['the use of semantic resources is comin modern but methods to extract lexical semantics have only recently begun to perform well enough for practical use',\n",
       "  'we propose an approximation based on attributes and coarseand finained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty'],\n",
       " ['we consider the problem of classifying documents not by topic, but by overall sentiment, , determining whether a review is positive or negative',\n",
       "  'however, the three machine learning methods we employed  do not perform as well on sentiment classification as on traditional topic-based categorization'],\n",
       " ['this paper describes a bootstrapping algorithm called basilisk that learns highquality semantic lexicons for multiple categories',\n",
       "  'basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts'],\n",
       " ['we explore how well phrases cohere across two languages, specifically ish and french, and examine the particular conditions under which they do not',\n",
       "  'we demonstrate that while there are cases where coherence is poor, there are many regularities which can be exploited by a statistical machine translation system'],\n",
       " ['thammar is created for use in real world applications, such that robustness and performance issues play an important role',\n",
       "  'this grammar is being developed in a multilingual context, requiring mrs structures that are easily comparable across languages'],\n",
       " ['thammar matrix is an open-source starter-kit for the development of broad- by using a type hierarchy to represent cross-linguistic generalizations and providing compatibility with other open-source tools for grammar neering, evaluation, parsing and generation, it facilitates not only quick start-up but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding'],\n",
       " ['we report on the parallel grammar  project which uses the xle parser and grammar development platform for six languages: ish,'],\n",
       " ['we propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side'],\n",
       " ['while parameter estimation for me models is conceptually straightforward, in practice me models for typical natural language tasks are very large, and may well contain many thousands of free parameters',\n",
       "  'surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices'],\n",
       " ['we explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms'],\n",
       " ['this paper presents a set of algorithms for distinguishing personal names with multiple real referents in text, based on little or no supervision'],\n",
       " ['however, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost'],\n",
       " ['the tagger uses features which can be obtained for a variety of languages and works effectively not only for ish, but also for other languages such as german and dutch'],\n",
       " ['when no gazetteer or other additional training resources are used, the combined system attains a performance of f on the ish development data; integrating name, location and person gazetteers, and named entity systems trained on additional, morneral, data reduces the f-measure error by a factor of  to % on the ish data'],\n",
       " ['we discuss two named-entity recognition models which use characters and character -grams either exclusively or as an important part of their data representation',\n",
       "  'the first model is a character-level hmm with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features'],\n",
       " [],\n",
       " ['we use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features',\n",
       "  'we also show that predicting labels from a &#;lightwt&#; parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features'],\n",
       " ['we present a system for automatically identifying propbank-style semantic roles based on the output of a statistical parser for combinatory categorial grammar',\n",
       "  'this system performs at least as well as a system based on a traditional treebank parser, and outperforms it on core argument roles'],\n",
       " ['we present a general framework for distributional similarity based on the concepts of precision and recall',\n",
       "  'different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored',\n",
       "  'we show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns'],\n",
       " ['high-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm',\n",
       "  'the learned patterns are then used to identify more subjective sentences'],\n",
       " ['results from a large collection of news stories and a human evaluation of  sentences are reported, indicating that we achieve very high performance in document classification , and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral'],\n",
       " ['in this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed',\n",
       "  'the main point of this paper is that by adding linguistic knowl to the representation , rather than relying only on , a better result is obtained as measured by keywords previously assigned by professional indexers',\n",
       "  'in more detail, ves a better precithan and by adding the tag assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied'],\n",
       " ['this paper presents the results from the acl-sighan-sponsored first international chinese word segmentation bakeoff held in  and reported in conjunction with the second sighan workshop on chinese language processing, sapporo, japan',\n",
       "  'wve the motivation for having an international segmentation contest  and we report on the results of this first international contest, analyze these results, and make some recommendations for the future'],\n",
       " ['through the first bakeoff, we could learn more about the development in chinese word segmentation and become more confident on our hhmm-based approach',\n",
       "  'the bakeoff is interesting and helpful'],\n",
       " ['this paper describes a distributional approach to the semantics of verb-particle ( we report first on a framework for implementing and evaluating such models'],\n",
       " ['we examine various measures using the nearest nbours of the phrasal verb, and in some cases the nbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set',\n",
       "  'we also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus'],\n",
       " ['we test the model over ish noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in wordnet',\n",
       "  'based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of wordnet'],\n",
       " ['however, we also show that it is possible to minimize the number of structures that require nonincremental processing by choosing an optimal parsing algorithm'],\n",
       " ['this paper presents the task definition, resources, participating systems, and comparative results for the ish lexical sample task, which was orgaas part of the evaluation exercise'],\n",
       " ['this paper introduces four different included in the summarization evaluation package and their evaluations'],\n",
       " ['data we describe a new corpus of over , handannotated dialog act tags and accompanying adjacency pair annotations for roughly  hours of speech from  naturally-occurring meetings',\n",
       "  'we provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data'],\n",
       " ['we develop a linear programming formulation for this problem and evaluate it in the context of simultaneously learning named entities and relations',\n",
       "  'our approach allows us to efficiently incorporate domain and task specific constraints at decision time, resulting in significant improvements in the accuracy and the &#;human-like&#; quality of the inferences'],\n",
       " ['second order context vectors are an indirect representation of the context based on the average of vectors that represent the words that occur in the context'],\n",
       " ['this paper reports the results of experimentsusing memory-based learning to guide a de terministic dependency parser for unrestricted natural language text',\n",
       "  'the accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parside is evaluated by parsing the held-out por tion of the treebank'],\n",
       " ['this paper describes nombank, a project that will provide argument structure for instances of common nouns in the penn treebank ii corpus',\n",
       "  'nombank is part of a larger effort to add additional layers of annotation to the penn treebank ii corpus',\n",
       "  'the university of pennsylvania&#;s propbank, nombank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text'],\n",
       " ['distributional observations are also presented as well as a discussion of possible uses for a system that can recognize speculative language'],\n",
       " ['we describe an approach to two areas of biomedical information extraction, drug development and cancer genomics',\n",
       "  'we are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process'],\n",
       " ['our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness'],\n",
       " ['we detect similarity, strh, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the web'],\n",
       " ['we focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowl base',\n",
       "  'our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the web for related syntactic entailment templates'],\n",
       " ['we describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-freammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best ish parse, korean parse, and word alignment, where these hidden structures all constrain each other',\n",
       "  'the model used for parsing is completely factored into the two parsers and the tm, allowing separate parameter estimation'],\n",
       " ['this paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited',\n",
       "  'we propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed'],\n",
       " ['a novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model'],\n",
       " ['the system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the world wide web'],\n",
       " [],\n",
       " ['this paper presents an in-depth study on such issues of processing architecture and feature representation for chinese pos tagging, within a maximum entropy framework'],\n",
       " ['the &#;in-domain&#; performance of the wsj capitalizer is % better than that of the -gram baseline, when evaluated on a test set drawn from wsj',\n",
       "  'when evaluating on the mismatched &#;out-ofdomain&#; test data, the -gram baseline is outperformed by %; the improvement brought by the adaptation technique using a very small amount of matched bn data &#; -kwds &#; is about -% relative'],\n",
       " ['the proposal consists of i) decision stumps that use subtrees as features and ii) the boosting algorithm which employs the subtree-based decision stumps as weak learners',\n",
       "  'two experiments on opinion/modality classification confirm that subtree features are important'],\n",
       " ['centrality is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence',\n",
       "  'the results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems'],\n",
       " ['if two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality? to answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the even for small test sizes of only  sentences, our methods may give us assurances that test result differences are real'],\n",
       " ['in this paper, we introduce textrank &#; a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications',\n",
       "  'in particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks'],\n",
       " ['this paper introduces an approach to sentiment analysis which uses support vector machines  to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and, where available, knowl of the topic of the text'],\n",
       " ['automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of mt systems',\n",
       "  'our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments'],\n",
       " ['in this paper, we introduce a method that combines wordto-word similarity metrics into a text-totext metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching'],\n",
       " ['to demonstrate the efficiency, scal ability and accuracy of these algorithms, we present experiments on bikel?s implementation of collins? \\t\\tlexicalized pcfg model, and on chiang?s cfg-based decoder for hierarchicalphrase-based translation'],\n",
       " ['the parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar',\n",
       "  'we show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers'],\n",
       " ['we introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective',\n",
       "  'this method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from framenet',\n",
       "  'our experimental results show that our system performs significantly better than the baseline'],\n",
       " ['we introduce spmt, a new class of statistical translation models that use syntactified target language phrases',\n",
       "  'the spmt models outperform a state of the art phrase-based baseline model by  bleu points on the nist  chinese-ish test corpus and  points on a humanbased quality metric that ranks translations on a scale from  to'],\n",
       " ['we discuss different strategies for smoothing the phrasetable in statistical mt, and give results over a range of translation settings',\n",
       "  'we show that any type of smoothing is a better idea than the relativefrequency estimates that are often used'],\n",
       " ['we test our technique on part of speech tagging and show performancins for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger'],\n",
       " ['however, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable',\n",
       "  'this approach is applied to dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over stateof-the-art'],\n",
       " ['s'],\n",
       " ['using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values',\n",
       "  'the experimental results show that the precision of polarity assignment with the automatically acquired lexicon was % on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon'],\n",
       " ['we present an approach for the joint extraction of entities and relations in the con text of opinion recognition and analysis',\n",
       "  'in spired by roth and yih , we employ an integer linear programming approach to solve the joint opinion recognition task,and show that global, constraint-based inference can significantly boost the perfor mance of both relation extraction and theextraction of opinion-related entities',\n",
       "  'the resulting system achieves f-measuresof  and  for entity and relation extrac tion, respectively, improving substantially over prior results in the area'],\n",
       " ['in this paper we approach word sense disambiguation and information extraction as a unified tagging problem'],\n",
       " [],\n",
       " ['in this paper we investigate a new problem identifying the which a document is written',\n",
       "  'can computers learn to identify which sentences strongly convey a particular perspective? we develop statistical models to capture how perspectives are expressed at the document and sentence levels, and evaluate the proposed models on articles about the israeli-palestinian conflict',\n",
       "  'the results show that the proposed models successfully learn how perspectives are reflected in word usage and can identify the perspective of a document with high accuracy'],\n",
       " ['finally, we try to draw general conclusions about multi-lingual parsing: what makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challng for any dependency parser? acknowlment many thanks to amit dubey and yuval krymolowski, the other two organizers of the shared task, for discussions, converting treebanks, writing and helping with the also to alexander yeh for additional help with the paper reviews'],\n",
       " ['the first stage based on the unlabeled dependency parsing models described by mcdonald and pereira  augmented with morphological features for a subset of the languages'],\n",
       " ['non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser'],\n",
       " ['we first propose a simplnerative, phrase-based model and verify that its estimates are inferior to thosven by surface statistics',\n",
       "  'in particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can',\n",
       "  'we also show that interpolation of the two methods can result in a modest increase in bleu score'],\n",
       " ['we evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus'],\n",
       " ['we present translation results on the shared task &#;exploiting parallel texts for statistical machine translation&#; generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories',\n",
       "  'our translation system is available open-source under thu general'],\n",
       " [],\n",
       " ['we introduce chinese whispers, a randomized graph-clustering algorithm, which is time-linear in the number of s',\n",
       "  'after a detailed definition of the algorithm and a discussion of its strhs and weaknesses, the performance of chinese whispers is measured on natural language processing  problems as diverse as language separation, acquisition of syntactic word classes and word sense disambiguation'],\n",
       " ['we demonstrate that the consistency constraints that allow flat phrasal models to scale also help itg algorithms, producing an -times faster inside-outside algorithm',\n",
       "  'we also show that the phrasal translation tables produced by the itg are superior to those of the flat joint phrasal model, producing up to a  point improvement in bleu score',\n",
       "  'finally, we explore, for the first time, the utility of a joint phrasal translation model as a word alignment method'],\n",
       " ['we show that this results in an im provement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings'],\n",
       " ['we describe a mixture-model approach to adapting a statistical machine translation system for new domains, using wts that depend on text distances to mixture components',\n",
       "  'we investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning wts; and granularity of the source unit being adapted to'],\n",
       " ['we carried out an extensive human evaluation which allowed us not only to rank the different mt systems, but also to perform higher-level analysis of the evaluation process'],\n",
       " ['an automatic metric for machine translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used it is one of several automatic metrics used in this year&#;s shared task within the acl wmt- workshop',\n",
       "  'this paper recaps the technical details underlying the metric and describes recent improvements in the metric',\n",
       "  'the latest release includes improved metric parameters and extends the metric to support evaluation of mt output in spanish, french and'],\n",
       " ['since prepositions account for a substantial proportion of all grammatical errors by esl  learners, developing an nlp application that can reliably detect these types of errors will provide an invaluable learning resource to esl students',\n",
       "  'to address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays'],\n",
       " ['thal of this task is to allow for comparison across sense-induction and discrim ination systems, and also to compare thesesystems to other supervised and knowlbased systems',\n",
       "  'we provide a comparison to the results ofthe systems participating in the lexical sam ple subtask of task'],\n",
       " ['we describe our experience in producing acoarse version of the wordnet sense inven tory and preparing the sense-tagged corpusfor the task'],\n",
       " ['in the task, annotators and systems find an alternative substitute word or phrase for a target word in context',\n",
       "  'there is a subtask which re quires identifying cases where the word isfunctioning as part of a multiword in the sen tence and detecting what that multiword is'],\n",
       " ['this paper presents the task definition, resources, participation, and comparative re sults for the web people search task, which was organized as part of the semeval- evaluation exercise',\n",
       "  'this task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name'],\n",
       " ['it avoids the pitfalls of evaluating a graph of inter-related labels by defining three sub tasks that allow pairwise eval uation of temporal relations'],\n",
       " ['this paper describes our experience in preparing the data and evaluating the results for three subtasks of semeval- task- ? lexical sample, semantic role labeling and all-words respectively'],\n",
       " ['in this paper we investigate several nonprojective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the -factored model'],\n",
       " ['we use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strh of the correlation with human judgments at both the system-level and at the sentence-level'],\n",
       " ['we also show that improving segmentation consistency using external lexicon and proper noun features yields a  bleu increase'],\n",
       " ['this paper describes two parallel implementations of giza++ that accelerate this word alignment process',\n",
       "  'results show a near-linear speedup according to the number of cpus used, and alignment quality is preserved'],\n",
       " ['this paper examines the stanford typed dependencies representation, which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding',\n",
       "  'we consider the underlying design principles of the stanford scheme from this perspective, and compare it to th and parc representations'],\n",
       " ['a severe chall in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved',\n",
       "  'experiments on the penn wsj treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy'],\n",
       " ['furthermore, we report and analyze the results and describe the approaches of the participating systems'],\n",
       " ['we present a new evaluation technique whereby system output is edited and judged for correctness'],\n",
       " ['it uses parallel and distributed computing techniques for scalability'],\n",
       " ['we propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language'],\n",
       " ['we explore these differences through the use of a new tunable mt metric: ter-plus, which extends the translation edit rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases',\n",
       "  'ter-plus was shown to be one of the top metrics in nist&#;s metrics matr  chall, having the highest average rank in terms of pearson and spearman correlation'],\n",
       " ['in this paper we present a machine learning system that finds the scope of negation in biomedical texts',\n",
       "  'to investigate the robustness of the approach, the system is tested on the three subcorpora of the bioscope corpus representing different text types',\n",
       "  'it achieves the best results to date for this task, with an error reduction of % compared to current state of the art results'],\n",
       " ['in the process of comparing several solutions to these challs we reach some surprising conclusions, as well as develop an system that achieves  on the conll- ner shared task, the best reported result for this dataset'],\n",
       " ['in this paper we present a machine learning system that finds the scope of h cues in biomedical texts',\n",
       "  'the system is based on a similar system that finds the scope of negation cues',\n",
       "  'to investigate the robustness of the approach, the system is tested on the three subcorpora of the bioscope corpus that represent different text types'],\n",
       " ['the paper presents the design and implementation of the bionlp&#; shared task, and reports the final results with analysis',\n",
       "  'the data was developed based on thnia event corpus'],\n",
       " ['in addition to questions about emotions evoked by terms, we show how the inclusion of a word choice question can discourage malicious data entry, help identify instances where the annotator may not be familiar with the target term , and help obtain annotations at sense level',\n",
       "  'we identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand'],\n",
       " ['in this paper wve an introduction to using amazon?s mechanical turk crowdsourc ing platform for the purpose of collecting data for human language technologies',\n",
       "  'researchers participated in the workshop?s shared task to create data for speech and language applications with $'],\n",
       " ['this paper presents the results of the wmt and metricsmatr shared which included a translation task, a system combination task, and an evaluation task'],\n",
       " ['in this paper we explore the computational modelling of compositionality in distributional models of semantics'],\n",
       " ['this paper presents a new learning paradigm aimed at alleviating the supervision burden'],\n",
       " ['the conll- shared task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts',\n",
       "  'the paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task'],\n",
       " ['we explore the use of a tree kernel to obviate the need for tedious feature neering',\n",
       "  'the new features  and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline'],\n",
       " ['five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in finained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects'],\n",
       " ['as its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since , and to evaluatneralization of the technology to full text papers',\n",
       "  'the results show the community has made a significant advancement in terms of both performance improvement and generalization'],\n",
       " ['this paper briefly describes the ontonotes annotation  and then describes the parameters of the shared task including the format, pre-processing information, and evaluation criteria, and presents and discusses the results achieved by the participating systems'],\n",
       " ['this paper details the coreference resolution system submitted by stanford at the conll-  shared task',\n",
       "  'we participated in both the open and closed tracks and submitted results using both predicted and gold mentions',\n",
       "  'our system was ranked first in both tracks, with a score of  in the closed track and  in the open track'],\n",
       " ['this paper presents the results of the wmt shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics',\n",
       "  'we also conducted a pilot &#;tunable metrics&#; task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality'],\n",
       " ['we include ranking and adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced tuning version shown to outperform bleu in minimum error rate training for a phrase-based urdu-ish system'],\n",
       " ['our code is thread-safe, and integrated into the moses, cdec, and joshua translation systems'],\n",
       " ['this paper presents the results of the wmt shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality'],\n",
       " ['the combination of word_align plus char_align reduces the variance  by a factor of over more importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the hansards, it has been possible to successfully deploy the programs at at&amp;t language line services, a commercial translation service, to help them with difficult terminology'],\n",
       " ['i survey some recent applications-oriented nl generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide thneration process into, the computations these modules perform, and the way the modules interact with each other',\n",
       "  \"i also compare this 'consensus architecture' among applied nlg systems with psycholinguistic knowl about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems\"],\n",
       " ['we compare this algorithm to the baum-welch algorithm, used for unsupervised training of stochastic taggers'],\n",
       " ['typically, ambiguous verb phrases of the form v p np through a model which considers values of the four head words (v, nl, paper shows that the problem is analogous to n-gram language models in speech recognition, and that one of the most common methods for language modeling, the backed-off estimate, is applicable'],\n",
       " ['the idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be'],\n",
       " ['disambiguation is performed with respect to wordnet senses, which are fairly finained; however, the method also permits the assignment of higher-level wordnet categories rather than sense labels'],\n",
       " ['for this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word'],\n",
       " ['this paper shows how to induce an n-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowl sources'],\n",
       " ['in this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive and time complexity properties when using tree-based formalism for indexing and searching huge case bases'],\n",
       " ['this paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context',\n",
       "  'the statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this obdifference'],\n",
       " ['furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems'],\n",
       " [\"this differs significantly from the results reported by bod, and is comparable to results from a duplication of pereira and schabes's  experiment on the same data\",\n",
       "  \"we show that bod's results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers\"],\n",
       " ['we present a statistical word feature, the word relation matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across languagoups'],\n",
       " ['absence of is a real problem for corpus-based approaches to sense disambiguation, one that is unlikely to be solved soon',\n",
       "  'selectional preference is traditionally connected with sense ambiguity; this paper explores how a statistical model of selectional preference, requiring neither manual annotation of selection restrictions nor supervised training, can be used in sense disambiguation'],\n",
       " ['furthermore, the parser returns several scored parses for a sentence, and this paper shows that a scheme to pick the best parse from the  highest scoring a dramatically higher accuracy of % precision and recall'],\n",
       " [],\n",
       " ['the method makes few assumptions about the data, so it can be applied to parallel data other than parallel texts, such as word spellings and pronunciations'],\n",
       " [],\n",
       " [\"the methods described in this paper, mcquitty's similarity analysis, ward's minimum&#;variance method, and the em algorithm, assign each instance of an ambiguous word to a known sense definition based solely on the values of automatically identifiable features in text\",\n",
       "  'these methods and feature sets are found to be more successful in disambiguating nouns rather than adjectives or verbs',\n",
       "  \"overall, the most accurate of these procedures is mcquitty's similarity analysis in combination with a high dimensional feature set\"],\n",
       " ['we describe experiments that show that the concepts of rhetorical analysis and nucleanty can be used effectively for deternulling the most important units in a text we show how these concepts can be implemented and we discuss results that we obtained with a discourse-based summanzation program  motivation the evaluation of automatic summanzers has always been a thorny problem most papers on summarization describe the approach that they use and give some &amp;quot;consamples of the output in very few cases, output of a summarization program with a human-made summary or evaluated with the help of human subjects, usually, the results are modest unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions the position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs , but also the adequacy of the assumptions that these programs use that way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each with few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a text  for an excellent overview) determining the salient parts is considered to be achievable because one or more of the following assumptions hold  important sentences in a text contain words that are used frequently ,  important sentences contain words that are used in the tide and section headings ,  important sentences are located at the beginning or end of paragraphs ,  important sentences are located at posilions in a text that arnre dependent these positions can be determined automatically, through training techniques ,  important sentences use words as &amp;quot;greatest&amp;quot; and &amp;quot;significant&amp;quot; or indiphrases as &amp;quot;the main aim of this paper&amp;quot; and &amp;quot;the purpose of this article&amp;quot;, while non-important senuse words as &amp;quot;impossible&amp;quot;  important sentences and concepts highest connected entities in elaborate semantic structures , and  imponant and non-important sentences are derivable from a discourse representation of the text  in determinmg the words that occur most frequently in a text or the sentences that use words that occur in the headings of sections, computers are accurate tools flowever, in determining the concepts that are semantically related or the discourse structure of a text, computers are no longer so accurate, rather, they are highly dependent on the coverage of the linguistic resources that they use and the quality of the algondims that they implement although it is plausible that elaborate cohesionand coherence-based structures can be used effectively in summarization, we believe that before building summarization programs, we should determine the extent to which these assumptions hold in this paper, we describe experiments that show that concepts of rhetorical analysis and nucleanty used effectively for determining the most important units in a text we show how these concepts were implemented and discuss results that we obtained with a discoursebased summarization program  from discourse trees to summaries &#; an empirical view'],\n",
       " ['it is compatible with the princeton wordnet but integrates principlebased modifications on the constructional and organizational level as well as on the level of lexical and conceptual relations',\n",
       "  'germanet includes a new treatment of regular polysemy, artificial concepts and of particle verbs'],\n",
       " ['given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension',\n",
       "  'the system has been evaluated in two distinct experiments which support the overall validity of the approach'],\n",
       " [],\n",
       " ['best-first probabilistic chart parsing attempts to parse efficiently by working on s that are judged &amp;quot;best&amp;quot; by some probabilistic figure of merit',\n",
       "  'we show how this can be accomplished in a particularly simple way using the common idea of binarizing the pcfg'],\n",
       " ['by working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowl sources in making its tagging decisions'],\n",
       " ['we present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves % accuracy'],\n",
       " ['marcu has characterised an important and difficult problem in text planning: given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how can one arrange this material so as to yield the best possible text? we describe experiments with a number of heuristic search methods for this task'],\n",
       " ['in an evaluation on the muc- coreference resolution corpus, the algorithm achieves an f-measure of %, placing it firmly between the worst  and best  systems in the muc- evaluation'],\n",
       " ['this paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models',\n",
       "  'the algorithm learns from unannotated text and achieves competitive performance when trained on a very short labelled name list with no other required language-specific information, tokenizers or tools'],\n",
       " ['a large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- however, we show that the use of data can reduce the requirements for supervision to just  simple &amp;quot;seed&amp;quot; rules'],\n",
       " ['three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable treebank parsing accuracy',\n",
       "  'the resulting parsers surpass the best previously published performance results for the penn treebank'],\n",
       " ['in this paper we discuss cascaded memory- based grammatical relations assignment',\n",
       "  'we studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder']]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase_objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "c9ddc8b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T03:21:22.989881Z",
     "start_time": "2023-02-23T03:21:22.975411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAPER>\\n  <S sid=\"0\">Unsupervised Word Sense Disambiguation Rivaling Supervised Methods</S>\\n  <ABSTRACT>\\n    <S sid=\"1\" ssid=\"1\">This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations.</S>\\n    <S sid=\"2\" ssid=\"2\">The algorithm is based on two powerful constraints &#8212; that words tend to have one sense per discourse and one sense per collocation &#8212; exploited in an iterative bootstrapping procedure.</S>\\n    <S sid=\"3\" ssid=\"3\">Tested accuracy exceeds 96%.</S>\\n  </ABSTRACT>\\n  <SECTION title=\"1 Introduction\" number=\"1\">\\n    <S sid=\"4\" ssid=\"1\">This paper presents an unsupervised algorithm that can accurately disambiguate word senses in a large, completely untagged corpus.1 The algorithm avoids the need for costly hand-tagged training data by exploiting two powerful properties of human language: Moreover, language is highly redundant, so that the sense of a word is effectively overdetermined by (1) and (2) above.</S>\\n    <S sid=\"5\" ssid=\"2\">The algorithm uses these properties to incrementally identify collocations for target senses of a word, given a few seed collocations \\'Note that the problem here is sense disambiguation: assigning each instance of a word to established sense definitions (such as in a dictionary).</S>\\n    <S sid=\"6\" ssid=\"3\">This differs from sense induction: using distributional similarity to partition word instances into clusters that may have no relation to standard sense partitions.</S>\\n    <S sid=\"7\" ssid=\"4\">\\'Here I use the traditional dictionary definition of collocation &#8212; &amp;quot;appearing in the same location; a juxtaposition of words&amp;quot;.</S>\\n    <S sid=\"8\" ssid=\"5\">No idiomatic or non-compositional interpretation is implied. for each sense, This procedure is robust and selfcorrecting, and exhibits many strengths of supervised approaches, including sensitivity to word-order information lost in earlier unsupervised algorithms.</S>\\n  </SECTION>\\n  <SECTION title=\"2 One Sense Per Discourse\" number=\"2\">\\n    <S sid=\"9\" ssid=\"1\">The observation that words strongly tend to exhibit only one sense in a given discourse or document was stated and quantified in Gale, Church and Yarowsky (1992).</S>\\n    <S sid=\"10\" ssid=\"2\">Yet to date, the full power of this property has not been exploited for sense disambiguation.</S>\\n    <S sid=\"11\" ssid=\"3\">The work reported here is the first to take advantage of this regularity in conjunction with separate models of local context for each word.</S>\\n    <S sid=\"12\" ssid=\"4\">Importantly, I do not use one-sense-per-discourse as a hard constraint; it affects the classification probabilistically and can be overridden when local evidence is strong.</S>\\n    <S sid=\"13\" ssid=\"5\">In this current work, the one-sense-per-discourse hypothesis was tested on a set of 37,232 examples (hand-tagged over a period of 3 years), the same data studied in the disambiguation experiments.</S>\\n    <S sid=\"14\" ssid=\"6\">For these words, the table below measures the claim\\'s accuracy (when the word occurs more than once in a discourse, how often it takes on the majority sense for the discourse) and applicability (how often the word does occur more than once in a discourse).</S>\\n    <S sid=\"15\" ssid=\"7\">Clearly, the claim holds with very high reliability for these words, and may be confidently exploited as another source of evidence in sense tagging.\\'</S>\\n  </SECTION>\\n  <SECTION title=\"3 One Sense Per Collocation\" number=\"3\">\\n    <S sid=\"16\" ssid=\"1\">The strong tendency for words to exhibit only one sense in a given collocation was observed and quantified in (Yarowsky, 1993).</S>\\n    <S sid=\"17\" ssid=\"2\">This effect varies depending on the type of collocation.</S>\\n    <S sid=\"18\" ssid=\"3\">It is strongest for immediately adjacent collocations, and weakens with distance.</S>\\n    <S sid=\"19\" ssid=\"4\">It is much stronger for words in a predicate-argument relationship than for arbitrary associations at equivalent distance.</S>\\n    <S sid=\"20\" ssid=\"5\">It is very much stronger for collocations with content words than those with function words.\\'</S>\\n    <S sid=\"21\" ssid=\"6\">In general, the high reliability of this behavior (in excess of 97% for adjacent content words, for example) makes it an extremely useful property for sense disambiguation.</S>\\n    <S sid=\"22\" ssid=\"7\">A supervised algorithm based on this property is given in (Yarowsky, 1994).</S>\\n    <S sid=\"23\" ssid=\"8\">Using a decision list control structure based on (Rivest, 1987), this algorithm integrates a wide diversity of potential evidence sources (lemmas, inflected forms, parts of speech and arbitrary word classes) in a wide diversity of positional relationships (including local and distant collocations, trigram sequences, and predicate-argument association).</S>\\n    <S sid=\"24\" ssid=\"9\">The training procedure computes the word-sense probability distributions for all such collocations, and orders them by the log-likelihood ratio Log( r4S enseAlCollocation,)\\\\ 5 Sensealeollocatton,)), with optional steps for interpolation and pruning.</S>\\n    <S sid=\"25\" ssid=\"10\">New data are classified by using the single most predictive piece of disambiguating evidence that appears in the target context.</S>\\n    <S sid=\"26\" ssid=\"11\">By not combining probabilities, this decision-list approach avoids the problematic complex modeling of statistical dependencies \\'It is interesting to speculate on the reasons for this phenomenon.</S>\\n    <S sid=\"27\" ssid=\"12\">Most of the tendency is statistical: two distinct arbitrary terms of moderate corpus frequency are quite unlikely to co-occur in the same discourse whether they are homographs or not.</S>\\n    <S sid=\"28\" ssid=\"13\">This is particularly true for content words, which exhibit a &amp;quot;bursty&amp;quot; distribution.</S>\\n    <S sid=\"29\" ssid=\"14\">However, it appears that human writers also have some active tendency to avoid mixing senses within a discourse.</S>\\n    <S sid=\"30\" ssid=\"15\">In a small study, homograph pairs were observed to co-occur roughly 5 times less often than arbitrary word pairs of comparable frequency.</S>\\n    <S sid=\"31\" ssid=\"16\">Regardless of origin, this phenomenon is strong enough to be of significant practical use as an additional probabilistic disambiguation constraint.</S>\\n    <S sid=\"32\" ssid=\"17\">4This latter effect is actually a continuous function conditional on the burstiness of the word (the tendency of a word to deviate from a constant Poisson distribution in a corpus).</S>\\n    <S sid=\"33\" ssid=\"18\">\\'As most ratios involve a 0 for some observed value, smoothing is crucial.</S>\\n    <S sid=\"34\" ssid=\"19\">The process employed here is sensitive to variables including the type of collocation (adjacent bigrams or wider context), collocational distance, type of word (content word vs. function word) and the expected amount of noise in the training data.</S>\\n    <S sid=\"35\" ssid=\"20\">Details are provided in (Yarowsky, to appear). encountered in other frameworks.</S>\\n    <S sid=\"36\" ssid=\"21\">The algorithm is especially well suited for utilizing a large set of highly non-independent evidence such as found here.</S>\\n    <S sid=\"37\" ssid=\"22\">In general, the decision-list algorithm is well suited for the task of sense disambiguation and will be used as a component of the unsupervised algorithm below.</S>\\n  </SECTION>\\n  <SECTION title=\"4 Unsupervised Learning Algorithm\" number=\"4\">\\n    <S sid=\"38\" ssid=\"1\">Words not only tend to occur in collocations that reliably indicate their sense, they tend to occur in multiple such collocations.</S>\\n    <S sid=\"39\" ssid=\"2\">This provides a mechanism for bootstrapping a sense tagger.</S>\\n    <S sid=\"40\" ssid=\"3\">If one begins with a small set of seed examples representative of two senses of a word, one can incrementally augment these seed examples with additional examples of each sense, using a combination of the one-senseper-collocation and one-sense-per-discourse tendencies.</S>\\n    <S sid=\"41\" ssid=\"4\">Although several algorithms can accomplish similar ends,6 the following approach has the advantages of simplicity and the ability to build on an existing supervised classification algorithm without modification.\\'</S>\\n    <S sid=\"42\" ssid=\"5\">As shown empirically, it also exhibits considerable effectiveness.</S>\\n    <S sid=\"43\" ssid=\"6\">The algorithm will be illustrated by the disambiguation of 7538 instances of the polysemous word plant in a previously untagged corpus.</S>\\n  </SECTION>\\n  <SECTION title=\"STEP 1:\" number=\"5\">\\n    <S sid=\"44\" ssid=\"1\">In a large corpus, identify all examples of the given polysemous word, storing their contexts as lines in an initially untagged training set.</S>\\n    <S sid=\"45\" ssid=\"2\">For example: TIndeed, any supervised classification algorithm that returns probabilities with its classifications may potentially be used here.</S>\\n    <S sid=\"46\" ssid=\"3\">These include Bayesian classifiers (Mosteller and Wallace, 1964) and some implementations of neural nets, but not Brill rules (Brill, 1993).</S>\\n    <S sid=\"47\" ssid=\"4\">For each possible sense of the word, identify a relatively small number of training examples representative of that sense.\\'</S>\\n    <S sid=\"48\" ssid=\"5\">This could be accomplished by hand tagging a subset of the training sentences.</S>\\n    <S sid=\"49\" ssid=\"6\">However, I avoid this laborious procedure by identifying a small number of seed collocations representative of each sense and then tagging all training examples containing the seed collocates with the seed\\'s sense label.</S>\\n    <S sid=\"50\" ssid=\"7\">The remainder of the examples (typically 85-98%) constitute an untagged residual.</S>\\n    <S sid=\"51\" ssid=\"8\">Several strategies for identifying seeds that require minimal or no human participation are discussed in Section 5.</S>\\n    <S sid=\"52\" ssid=\"9\">In the example below, the words life and manufacturing are used as seed collocations for the two major senses of plant (labeled A and B respectively).</S>\\n    <S sid=\"53\" ssid=\"10\">This partitions the training set into 82 examples of living plants (1%), 106 examples of manufacturing plants (1%), and 7350 residual examples (98%).</S>\\n    <S sid=\"54\" ssid=\"11\">Sense Training Examples (Keyword in Context) A A... A used to strain microscopic plant life from the ... A ... zonal distribution of plant life .</S>\\n    <S sid=\"55\" ssid=\"12\">... A close-up studies of plant life and natural ... A too rapid growth of aquatic plant life in water ... A ... the proliferation of plant and animal life ... A establishment phase of the plant virus life cycle ... A ... that divide life into plant and animal kingdom A ... many dangers to plant and animal life ... A mammals .</S>\\n    <S sid=\"56\" ssid=\"13\">Animal and plant life are delicately A beds too salty to support plant life .</S>\\n    <S sid=\"57\" ssid=\"14\">River ... heavy seas, damage , and plant life growing on ... ... ?</S>\\n    <S sid=\"58\" ssid=\"15\">... vinyl chloride monomer plant, which is ... ?</S>\\n    <S sid=\"59\" ssid=\"16\">... molecules found in plant and animal tissue ?</S>\\n    <S sid=\"60\" ssid=\"17\">... Nissan car and truck plant in Japan is ... ?</S>\\n    <S sid=\"61\" ssid=\"18\">... and Golgi apparatus of plant and animal cells ... ?</S>\\n    <S sid=\"62\" ssid=\"19\">... union responses to plant closures .</S>\\n    <S sid=\"63\" ssid=\"20\">... ?</S>\\n    <S sid=\"64\" ssid=\"21\">... ... ?</S>\\n    <S sid=\"65\" ssid=\"22\">... ... ?</S>\\n    <S sid=\"66\" ssid=\"23\">... cell types found in the plant kingdom are ... ?</S>\\n    <S sid=\"67\" ssid=\"24\">... company said the plant is still operating ... ?</S>\\n    <S sid=\"68\" ssid=\"25\">...</S>\\n    <S sid=\"69\" ssid=\"26\">Although thousands of plant and animal species ?</S>\\n    <S sid=\"70\" ssid=\"27\">... animal rather than plant tissues can be ... ?</S>\\n    <S sid=\"71\" ssid=\"28\">... computer disk drive plant located in ... polystyrene manufacturing plant at its Dow ... company manufacturing plant is in Orlando ...</S>\\n    <S sid=\"72\" ssid=\"29\">It is useful to visualize the process of seed development graphically.</S>\\n    <S sid=\"73\" ssid=\"30\">The following figure illustrates this sample initial state.</S>\\n    <S sid=\"74\" ssid=\"31\">Circled regions are the training examples that contain either an A or B seed collocate.</S>\\n    <S sid=\"75\" ssid=\"32\">The bulk of the sample points &amp;quot;?&amp;quot; constitute the untagged residual.</S>\\n    <S sid=\"76\" ssid=\"33\">8 For the purposes of exposition, I will assume a binary sense partition.</S>\\n    <S sid=\"77\" ssid=\"34\">It is straightforward to extend this to k senses using k sets of seeds.</S>\\n    <S sid=\"78\" ssid=\"35\">= Set of training examples containing the collocation &amp;quot;life&amp;quot;.</S>\\n  </SECTION>\\n  <SECTION title=\"STEP 3a:\" number=\"6\">\\n    <S sid=\"79\" ssid=\"1\">Train the supervised classification algorithm on the SENSE-A/SENSE-B seed sets.</S>\\n    <S sid=\"80\" ssid=\"2\">The decision-list algorithm used here (Yarowsky, 1994) identifies other collocations that reliably partition the seed training data, ranked by the purity of the distribution.</S>\\n    <S sid=\"81\" ssid=\"3\">Below is an abbreviated example of the decision list trained on the plant seed data.9 STEP 3h: Apply the resulting classifier to the entire sample set.</S>\\n    <S sid=\"82\" ssid=\"4\">Take those members in the residual that are tagged as SENSE-A or SENSE-B with probability above a certain threshold, and add those examples to the growing seed sets.</S>\\n    <S sid=\"83\" ssid=\"5\">Using the decision-list algorithm, these additions will contain newly-learned collocations that are reliably indicative of the previously-trained seed sets.</S>\\n    <S sid=\"84\" ssid=\"6\">The acquisition of additional partitioning collocations from cooccurrence with previously-identified ones is illustrated in the lower portion of Figure 2.</S>\\n    <S sid=\"85\" ssid=\"7\">STEP 3c: Optionally, the one-sense-per-discourse constraint is then used both to filter and augment this addition.</S>\\n    <S sid=\"86\" ssid=\"8\">The details of this process are discussed in Section 7.</S>\\n    <S sid=\"87\" ssid=\"9\">In brief, if several instances of the polysemous word in a discourse have already been assigned SENSE-A, this sense tag may be extended to all examples in the discourse, conditional on the relative numbers and the probabilities associated with the tagged examples.</S>\\n    <S sid=\"88\" ssid=\"10\">This augmentation of the training data can often form a bridge to new collocations that may not otherwise co-occur in the same nearby context with previously identified collocations.</S>\\n    <S sid=\"89\" ssid=\"11\">Such a bridge to the SENSE-A collocate &amp;quot;cell&amp;quot; is illustrated graphically in the upper half of Figure 2.</S>\\n    <S sid=\"90\" ssid=\"12\">Similarly, the one-sense-per-discourse constraint may also be used to correct erroneously labeled examples.</S>\\n    <S sid=\"91\" ssid=\"13\">For example:</S>\\n  </SECTION>\\n  <SECTION title=\"STEP 3d:\" number=\"7\">\\n    <S sid=\"92\" ssid=\"1\">Repeat Step 3 iteratively.</S>\\n    <S sid=\"93\" ssid=\"2\">The training sets (e.g.</S>\\n    <S sid=\"94\" ssid=\"3\">SENSE-A seeds plus newly added examples) will tend to grow, while the residual will tend to shrink.</S>\\n    <S sid=\"95\" ssid=\"4\">Additional details aimed at correcting and avoiding misclassifications will be discussed in Section 6.</S>\\n  </SECTION>\\n  <SECTION title=\"STEP 4:\" number=\"8\">\\n    <S sid=\"96\" ssid=\"1\">Stop.</S>\\n    <S sid=\"97\" ssid=\"2\">When the training parameters are held constant, the algorithm will converge on a stable residual set.</S>\\n    <S sid=\"98\" ssid=\"3\">Note that most training examples will exhibit multiple collocations indicative of the same sense (as illustrated in Figure 3).</S>\\n    <S sid=\"99\" ssid=\"4\">The decision list algorithm resolves any conflicts by using only the single most reliable piece of evidence, not a combination of all matching collocations.</S>\\n    <S sid=\"100\" ssid=\"5\">This circumvents many of the problems associated with non-independent evidence sources.</S>\\n    <S sid=\"101\" ssid=\"6\">STEP 5: The classification procedure learned from the final supervised training step may now be applied to new data, and used to annotate the original untagged corpus with sense tags and probabilities.</S>\\n    <S sid=\"102\" ssid=\"7\">An abbreviated sample of the final decision list for plant is given below.</S>\\n    <S sid=\"103\" ssid=\"8\">Note that the original seed words are no longer at the top of the list.</S>\\n    <S sid=\"104\" ssid=\"9\">They have been displaced by more broadly applicable collocations that better partition the newly learned classes.</S>\\n    <S sid=\"105\" ssid=\"10\">In cases where there are multiple seeds, it is even possible for an original seed for SENSE-A to become an indicator for SENSE-B if the collocate is more compatible with this second class.</S>\\n    <S sid=\"106\" ssid=\"11\">Thus the noise introduced by a few irrelevant or misleading seed words is not fatal.</S>\\n    <S sid=\"107\" ssid=\"12\">It may be corrected if the majority of the seeds forms a coherent collocation space.</S>\\n    <S sid=\"108\" ssid=\"13\">When this decision list is applied to a new test sentence, ... the loss of animal and plant species through extinction ... , the highest ranking collocation found in the target context (species) is used to classify the example as SENSE-A (a living plant).</S>\\n    <S sid=\"109\" ssid=\"14\">If available, information from other occurrences of &amp;quot;plant&amp;quot; in the discourse may override this classification, as described in Section 7.</S>\\n  </SECTION>\\n  <SECTION title=\"5 Options for Training Seeds\" number=\"9\">\\n    <S sid=\"110\" ssid=\"1\">The algorithm should begin with seed words that accurately and productively distinguish the possible senses.</S>\\n    <S sid=\"111\" ssid=\"2\">Such seed words can be selected by any of the following strategies: Extract seed words from a dictionary\\'s entry for the target sense.</S>\\n    <S sid=\"112\" ssid=\"3\">This can be done automatically, using words that occur with significantly greater frequency in the entry relative to the entire dictionary.</S>\\n    <S sid=\"113\" ssid=\"4\">Words in the entry appearing in the most reliable collocational relationships with the target word are given the most weight, based on the criteria given in Yarowsky (1993).</S>\\n    <S sid=\"114\" ssid=\"5\">Remarkably good performance may be achieved by identifying a single defining collocate for each class (e.g. bird and machine for the word crane), and using for seeds only those contexts containing one of these words.</S>\\n    <S sid=\"115\" ssid=\"6\">WordNet (Miller, 1990) is an automatic source for such defining terms.</S>\\n    <S sid=\"116\" ssid=\"7\">Words that co-occur with the target word in unusually great frequency, especially in certain collocational relationships, will tend to be reliable indicators of one of the target word\\'s senses (e.g. flock and bulldozer for &amp;quot;crane&amp;quot;).</S>\\n    <S sid=\"117\" ssid=\"8\">A human judge must decide which one, but this can be done very quickly (typically under 2 minutes for a full list of 30-60 such words).</S>\\n    <S sid=\"118\" ssid=\"9\">Co-occurrence analysis selects collocates that span the space with minimal overlap, optimizing the efforts of the human assistant.</S>\\n    <S sid=\"119\" ssid=\"10\">While not fully automatic, this approach yields rich and highly reliable seed sets with minimal work.</S>\\n  </SECTION>\\n  <SECTION title=\"6 Escaping from Initial Misclassifications\" number=\"10\">\\n    <S sid=\"120\" ssid=\"1\">Unlike many previous bootstrapping approaches, the present algorithm can escape from initial misclassification.</S>\\n    <S sid=\"121\" ssid=\"2\">Examples added to the the growing seed sets remain there only as long as the probability of the classification stays above the threshold.</S>\\n    <S sid=\"122\" ssid=\"3\">IIf their classification begins to waver because new examples have discredited the crucial collocate, they are returned to the residual and may later be classified differently.</S>\\n    <S sid=\"123\" ssid=\"4\">Thus contexts that are added to the wrong seed set because of a misleading word in a dictionary definition may be (and typically are) correctly reclassified as iterative training proceeds.</S>\\n    <S sid=\"124\" ssid=\"5\">The redundancy of language with respect to collocation makes the process primarily self-correcting.</S>\\n    <S sid=\"125\" ssid=\"6\">However, certain strong collocates may become entrenched as indicators for the wrong class.</S>\\n    <S sid=\"126\" ssid=\"7\">We discourage such behavior in the training algorithm by two techniques: 1) incrementally increasing the width of the context window after intermediate convergence (which periodically adds new feature values to shake up the system) and 2) randomly perturbing the class-inclusion threshold, similar to simulated annealing.</S>\\n  </SECTION>\\n  <SECTION title=\"7 Using the One-sense-per-discourse Property\" number=\"11\">\\n    <S sid=\"127\" ssid=\"1\">The algorithm performs well using only local collocational information, treating each token of the target word independently.</S>\\n    <S sid=\"128\" ssid=\"2\">However, accuracy can be improved by also exploiting the fact that all occurrences of a word in the discourse are likely to exhibit the same sense.</S>\\n    <S sid=\"129\" ssid=\"3\">This property may be utilized in two places, either once at the end of Step 4 after the algorithm has converged, or in Step 3c after each iteration.</S>\\n    <S sid=\"130\" ssid=\"4\">At the end of Step 4, this property is used for error correction.</S>\\n    <S sid=\"131\" ssid=\"5\">When a polysemous word such as plant occurs multiple times in a discourse, tokens that were tagged by the algorithm with low confidence using local collocation information may be overridden by the dominant tag for the discourse.</S>\\n    <S sid=\"132\" ssid=\"6\">The probability differentials necessary for such a reclassification were determined empirically in an early pilot study.</S>\\n    <S sid=\"133\" ssid=\"7\">The variables in this decision are the total number of occurrences of plant in the discourse (n), the number of occurrences assigned to the majority and minor senses for the discourse, and the cumulative scores for both (a sum of log-likelihood ratios).</S>\\n    <S sid=\"134\" ssid=\"8\">If cumulative evidence for the majority sense exceeds that of the minority by a threshold (conditional on n), the minority cases are relabeled.</S>\\n    <S sid=\"135\" ssid=\"9\">The case n = 2 does not admit much reclassification because it is unclear which sense is dominant.</S>\\n    <S sid=\"136\" ssid=\"10\">But for n &gt; 4, all but the most confident local classifications tend to be overridden by the dominant tag, because of the overwhelming strength of the one-sense-perdiscourse tendency.</S>\\n    <S sid=\"137\" ssid=\"11\">The use of this property after each iteration is similar to the final post-hoc application, but helps prevent initially mistagged collocates from gaining a foothold.</S>\\n    <S sid=\"138\" ssid=\"12\">The major difference is that in discourses where there is substantial disagreement concerning which is the dominant sense, all instances in the discourse are returned to the residual rather than merely leaving their current tags unchanged.</S>\\n    <S sid=\"139\" ssid=\"13\">This helps improve the purity of the training data.</S>\\n    <S sid=\"140\" ssid=\"14\">The fundamental limitation of this property is coverage.</S>\\n    <S sid=\"141\" ssid=\"15\">As noted in Section 2, half of the examples occur in a discourse where there are no other instances of the same word to provide corroborating evidence for a sense or to protect against misclassification.</S>\\n    <S sid=\"142\" ssid=\"16\">There is additional hope for these cases, however, as such isolated tokens tend to strongly favor a particular sense (the less &amp;quot;bursty&amp;quot; one).</S>\\n    <S sid=\"143\" ssid=\"17\">We have yet to use this additional information.</S>\\n  </SECTION>\\n  <SECTION title=\"8 Evaluation\" number=\"12\">\\n    <S sid=\"144\" ssid=\"1\">The words used in this evaluation were randomly selected from those previously studied in the literature.</S>\\n    <S sid=\"145\" ssid=\"2\">They include words where sense differences are realized as differences in French translation (drug drogue/medicament, and duty .&amp;quot;-P devoir/droit), a verb (poach) and words used in Schiitze\\'s 1992 disambiguation experiments (tank, space, motion, plant).1&#176; The data were extracted from a 460 million word corpus containing news articles, scientific abstracts, spoken transcripts, and novels, and almost certainly constitute the largest training/testing sets used in the sense-disambiguation literature.</S>\\n    <S sid=\"146\" ssid=\"3\">Columns 6-8 illustrate differences in seed training options.</S>\\n    <S sid=\"147\" ssid=\"4\">Using only two words as seeds does surprisingly well (90.6 %).</S>\\n    <S sid=\"148\" ssid=\"5\">This approach is least successful for senses with a complex concept space, which cannot be adequately represented by single words.</S>\\n    <S sid=\"149\" ssid=\"6\">Using the salient words of a dictionary definition as seeds increases the coverage of the concept space, improving accuracy (94.8%).</S>\\n    <S sid=\"150\" ssid=\"7\">However, spurious words in example sentences can be a source of noise.</S>\\n    <S sid=\"151\" ssid=\"8\">Quick hand tagging of a list of algorithmically-identified salient collocates appears to be worth the effort, due to the increased accuracy (95.5%) and minimal cost.</S>\\n    <S sid=\"152\" ssid=\"9\">Columns 9 and 10 illustrate the effect of adding the probabilistic one-sense-per-discourse constraint to collocation-based models using dictionary entries as training seeds.</S>\\n    <S sid=\"153\" ssid=\"10\">Column 9 shows its effectiveness as a post-hoc constraint.</S>\\n    <S sid=\"154\" ssid=\"11\">Although apparently small in absolute terms, on average this represents a 27% reduction in error rate.11 When applied at each iteration, this process reduces the training noise, yielding the optimal observed accuracy in column 10.</S>\\n    <S sid=\"155\" ssid=\"12\">Comparative performance: Column 5 shows the relative performance of supervised training using the decision list algorithm, applied to the same data and not using any discourse information.</S>\\n    <S sid=\"156\" ssid=\"13\">Unsupervised training using the additional one-sense-per-discourse constraint frequently exceeds this value.</S>\\n    <S sid=\"157\" ssid=\"14\">Column 11 shows the performance of Schiitze\\'s unsupervised algorithm applied to some of these words, trained on a New York Times News Service corpus.</S>\\n    <S sid=\"158\" ssid=\"15\">Our algorithm exceeds this accuracy on each word, with an average relative performance of 97% vs. 92%.12</S>\\n  </SECTION>\\n  <SECTION title=\"9 Comparison with Previous Work\" number=\"13\">\\n    <S sid=\"159\" ssid=\"1\">This algorithm exhibits a fundamental advantage over supervised learning algorithms (including Black (1988), Hearst (1991), Gale et al. (1992), Yarowsky (1993, 1994), Leacock et al.</S>\\n    <S sid=\"160\" ssid=\"2\">(1993), Bruce and Wiebe (1994), and Lehman (1994)), as it does not require costly hand-tagged training sets.</S>\\n    <S sid=\"161\" ssid=\"3\">It thrives on raw, unannotated monolingual corpora &#8212; the more the merrier.</S>\\n    <S sid=\"162\" ssid=\"4\">Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences.</S>\\n    <S sid=\"163\" ssid=\"5\">The use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approaches, including Lesk (1986), Guthrie et al. (1991), Veronis and Ide (1990), and Slator (1991).</S>\\n    <S sid=\"164\" ssid=\"6\">Although these earlier approaches have used often sophisticated measures of overlap with dictionary definitions, they have not realized the potential for combining the relatively limited seed information in such definitions with the nearly unlimited co-occurrence information extractable from text corpora.</S>\\n    <S sid=\"165\" ssid=\"7\">Other unsupervised methods have shown great promise.</S>\\n    <S sid=\"166\" ssid=\"8\">Dagan and Itai (1994) have proposed a method using co-occurrence statistics in independent monolingual corpora of two languages to guide lexical choice in machine translation.</S>\\n    <S sid=\"167\" ssid=\"9\">Translation of a Hebrew verb-object pair such as lahtom (sign or seal) and hoze (contract or treaty) is determined using the most probable combination of words in an English monolingual corpus.</S>\\n    <S sid=\"168\" ssid=\"10\">This work shows &amp;quot;The maximum possible error rate reduction is 50.1%, or the mean applicability discussed in Section 2.</S>\\n    <S sid=\"169\" ssid=\"11\">\\'This difference is even more striking given that Schiitze\\'s data exhibit a higher baseline probability (65% vs. 55%) for these words, and hence constitute an easier task. that leveraging bilingual lexicons and monolingual language models can overcome the need for aligned bilingual corpora.</S>\\n    <S sid=\"170\" ssid=\"12\">Hearst (1991) proposed an early application of bootstrapping to augment training sets for a supervised sense tagger.</S>\\n    <S sid=\"171\" ssid=\"13\">She trained her fully supervised algorithm on hand-labelled sentences, applied the result to new data and added the most confidently tagged examples to the training set.</S>\\n    <S sid=\"172\" ssid=\"14\">Regrettably, this algorithm was only described in two sentences and was not developed further.</S>\\n    <S sid=\"173\" ssid=\"15\">Our current work differs by eliminating the need for handlabelled training data entirely and by the joint use of collocation and discourse constraints to accomplish this.</S>\\n    <S sid=\"174\" ssid=\"16\">Schiitze (1992) has pioneered work in the hierarchical clustering of word senses.</S>\\n    <S sid=\"175\" ssid=\"17\">In his disambiguation experiments, Schiitze used post-hoc alignment of clusters to word senses.</S>\\n    <S sid=\"176\" ssid=\"18\">Because the toplevel cluster partitions based purely on distributional information do not necessarily align with standard sense distinctions, he generated up to 10 sense clusters and manually assigned each to a fixed sense label (based on the hand-inspection of 10-20 sentences per cluster).</S>\\n    <S sid=\"177\" ssid=\"19\">In contrast, our algorithm uses automatically acquired seeds to tie the sense partitions to the desired standard at the beginning, where it can be most useful as an anchor and guide.</S>\\n    <S sid=\"178\" ssid=\"20\">In addition, Schiitze performs his classifications by treating documents as a large unordered bag of words.</S>\\n    <S sid=\"179\" ssid=\"21\">By doing so he loses many important distinctions, such as collocational distance, word sequence and the existence of predicate-argument relationships between words.</S>\\n    <S sid=\"180\" ssid=\"22\">In contrast, our algorithm models these properties carefully, adding considerable discriminating power lost in other relatively impoverished models of language.</S>\\n  </SECTION>\\n  <SECTION title=\"10 Conclusion\" number=\"14\">\\n    <S sid=\"181\" ssid=\"1\">In essence, our algorithm works by harnessing several powerful, empirically-observed properties of language, namely the strong tendency for words to exhibit only one sense per collocation and per discourse.</S>\\n    <S sid=\"182\" ssid=\"2\">It attempts to derive maximal leverage from these properties by modeling a rich diversity of collocational relationships.</S>\\n    <S sid=\"183\" ssid=\"3\">It thus uses more discriminating information than available to algorithms treating documents as bags of words, ignoring relative position and sequence.</S>\\n    <S sid=\"184\" ssid=\"4\">Indeed, one of the strengths of this work is that it is sensitive to a wider range of language detail than typically captured in statistical sense-disambiguation algorithms.</S>\\n    <S sid=\"185\" ssid=\"5\">Also, for an unsupervised algorithm it works surprisingly well, directly outperforming Schiitze\\'s unsupervised algorithm 96.7 % to 92.2 %, on a test of the same 4 words.</S>\\n    <S sid=\"186\" ssid=\"6\">More impressively, it achieves nearly the same performance as the supervised algorithm given identical training contexts (95.5 % vs. 96.1 %) , and in some cases actually achieves superior performance when using the one-sense-perdiscourse constraint (96.5 % vs. 96.1%).</S>\\n    <S sid=\"187\" ssid=\"7\">This would indicate that the cost of a large sense-tagged training corpus may not be necessary to achieve accurate word-sense disambiguation.</S>\\n  </SECTION>\\n</PAPER>\\n'"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_teste[746]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee5de3",
   "metadata": {},
   "source": [
    "### ROUGE-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d48a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd2e38f0",
   "metadata": {},
   "source": [
    "## Aplicando o modelo de linguagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70d255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272.094px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
